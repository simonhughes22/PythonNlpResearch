{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE - Use Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pymongo\n",
    "import dill\n",
    "import os\n",
    "\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from window_based_tagger_config import get_config\n",
    "from FindFiles import find_files\n",
    "from DirUtils import dir_exists\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Settings import Settings\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = mcnemar([[1,2],[3,1]], exact=True)\n",
    "# result.statistic, result.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/Predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_str(s, files, exclude=False):   \n",
    "    return  [f for f in files if (s in f) != exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988\n",
      "76\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(FOLDER)\n",
    "print(len(all_files))\n",
    "files = list(all_files)\n",
    "# files = filter_by_str(\"_TAGGING_\", files, exclude=True)\n",
    "files = filter_by_str(\"_HYPER_PARAM2_\", files, exclude=True) # this is the essay parser\n",
    "files = filter_by_str(\"_CR_\", files) \n",
    "# files = filter_by_str(\"_RNN_\", files)\n",
    "files = filter_by_str(\"_VD_\", files)\n",
    "print(len(files))\n",
    "files = filter_by_str(\"2019\", files, exclude=True)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FINAL_RUN_VD_PREDS__.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FINAL_RUN_VD_YS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_VD_PREDS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_VD_YS_.dill',\n",
       " 'TEST_CR_CB_STACKED_VD_PREDS_.dill',\n",
       " 'TEST_CR_CB_STACKED_VD_YS_.dill',\n",
       " 'TEST_CR_CB_TAGGING_VD_MOST_COMMON_TAG_RNN_PREDS_.dill',\n",
       " 'TEST_CR_CB_TAGGING_VD_MOST_COMMON_TAG_RNN_YS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_FINAL_RUN_VD_PREDS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_FINAL_RUN_VD_YS_.dill',\n",
       " 'TEST_CR_SC_STACKED_VD_PREDS_.dill',\n",
       " 'TEST_CR_SC_STACKED_VD_YS_.dill',\n",
       " 'TEST_CR_SC_TAGGING_VD_MOST_COMMON_TAG_RNN_PREDS_.dill',\n",
       " 'TEST_CR_SC_TAGGING_VD_MOST_COMMON_TAG_RNN_YS_.dill']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_files = filter_by_str(\"_SC_\", files)\n",
    "cb_files = filter_by_str(\"_CB_\", files)\n",
    "len(cb_files), len(sc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "filters = OrderedDict({\n",
    "    \"STACKED_CLASSIFIER\":\"STACKED\",\n",
    "    \"RNN\": \"RNN\",\n",
    "    \"PARSER\": \"PARSER_TEMPLATED_FINAL_RUN\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def load_predictions(input_files):\n",
    "    algo2preds = dict()\n",
    "    pred_files = filter_by_str(\"_PREDS\", input_files)\n",
    "    for algo_name, fltr in filters.items():\n",
    "        f_files = filter_by_str(fltr, pred_files)\n",
    "        assert len(f_files) == 1, (algo_name,f_files)\n",
    "        fname = FOLDER + f_files[0]\n",
    "        with open(fname, \"rb+\") as f:\n",
    "            algo2preds[algo_name] = dill.load(f)\n",
    "    return algo2preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_preds = load_predictions(cb_files)\n",
    "sc_preds = load_predictions(sc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FOLDER + \"TEST_CR_CB_STACKED_VD_YS_.dill\", \"rb+\") as f:\n",
    "    cb_ysbytag = dill.load(f)\n",
    "    \n",
    "with open(FOLDER + \"TEST_CR_SC_STACKED_VD_YS_.dill\", \"rb+\") as f:\n",
    "    sc_ysbytag = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(predsbytag, k_filter=None):\n",
    "    if k_filter is None:\n",
    "        k_filter = set(predsbytag.keys())\n",
    "    all_p = []\n",
    "    \n",
    "    for k, vals in sorted(predsbytag.items(), key = lambda tpl: tpl[0]):\n",
    "        if k not in k_filter:\n",
    "            continue\n",
    "        all_p.extend(vals)\n",
    "    return all_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_value_binomial_test(ysbytag, predsbytaga, predsbytagb, alternative='two-sided'):\n",
    "#     assert len(ysbytag.keys()) == len(predsbytaga.keys()), (len(ysbytag.keys()),len(predsbytaga.keys())) \n",
    "#     assert len(ysbytag.keys()) == len(predsbytagb.keys()), (len(ysbytag.keys()),len(predsbytagb.keys())) \n",
    "\n",
    "    first = list(ysbytag.keys())[0]\n",
    "    assert len(ysbytag[first]) == len(predsbytaga[first])\n",
    "    assert len(ysbytag[first]) == len(predsbytagb[first])\n",
    "\n",
    "    lbls = set(ysbytag.keys())\n",
    "    lbls = lbls.intersection(predsbytaga.keys())\n",
    "    lbls = lbls.intersection(predsbytagb.keys())\n",
    "    \n",
    "    ys = get_all_preds(ysbytag, k_filter=lbls)\n",
    "    aas = get_all_preds(predsbytaga, k_filter=lbls)\n",
    "    bbs = get_all_preds(predsbytagb, k_filter=lbls)\n",
    "\n",
    "    assert len(ys) == len(aas) == len(bbs)\n",
    "\n",
    "    successes = defaultdict(int)\n",
    "    both_correct, both_wrong, a_correct_only, b_correct_only = 0,0,0,0\n",
    "    for y,a,b in zip(ys,aas,bbs):\n",
    "        if a == b:\n",
    "            if a == y:\n",
    "                both_correct +=1\n",
    "            else:\n",
    "                both_wrong += 1        \n",
    "        else: # a != b\n",
    "            if a == y:\n",
    "                successes[\"a\"] += 1\n",
    "                a_correct_only += 1\n",
    "            else:\n",
    "                successes[\"b\"] += 1\n",
    "                b_correct_only +=1\n",
    "\n",
    "\n",
    "    mcn_result = mcnemar([[both_correct, a_correct_only],[b_correct_only, both_wrong]], exact=True)\n",
    "#     p_value = stats.binom_test(successes[\"a\"], sum(successes.values()), p=0.5, alternative=alternative)\n",
    "    return mcn_result.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get Predicted Tags from Labelled Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(ysbytag, algo2preds, alternative=\"two-sided\", stats_only=False):\n",
    "    algo2metrics = {}\n",
    "    for algo, preds in algo2preds.items():\n",
    "        mean_metrics = ResultsProcessor.compute_mean_metrics(ysbytag, preds)\n",
    "        algo2metrics[algo] = mean_metrics[__MICRO_F1__]\n",
    "\n",
    "    matrix = dict()\n",
    "    for algo_name_a, predsbytaga in algo2preds.items():\n",
    "        for algo_name_b, predsbytagb in algo2preds.items():\n",
    "            if algo_name_a == algo_name_b:\n",
    "                continue\n",
    "            f1_a = algo2metrics[algo_name_a][\"f1_score\"]\n",
    "            f1_b = algo2metrics[algo_name_b][\"f1_score\"]\n",
    "            pval = compute_p_value_binomial_test(ysbytag, predsbytaga, predsbytagb, alternative)\n",
    "            if stats_only:\n",
    "                print(f\"{algo_name_a.ljust(20)} \\t{algo_name_b.ljust(20)}\\t {pval}\")\n",
    "            else:\n",
    "                print(f\"{algo_name_a.ljust(20)} {f1_a:.4f}\\t{algo_name_b.ljust(20)} {f1_b:.4f}\\t {pval}\")\n",
    "            matrix[(algo_name_a, algo_name_b)] = pval\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKED_CLASSIFIER   0.7081\tRNN                  0.6730\t 1.6427816964126646e-05\n",
      "STACKED_CLASSIFIER   0.7081\tPARSER               0.7264\t 0.6931012792713278\n",
      "\n",
      "RNN                  0.6730\tSTACKED_CLASSIFIER   0.7081\t 1.6427816964126646e-05\n",
      "RNN                  0.6730\tPARSER               0.7264\t 0.0003808264095775129\n",
      "\n",
      "PARSER               0.7264\tSTACKED_CLASSIFIER   0.7081\t 0.6931012792713278\n",
      "PARSER               0.7264\tRNN                  0.6730\t 0.0003808264095775129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perceptron and CRF differ somewhat from prior run\n",
    "print_comparison(cb_ysbytag, cb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKED_CLASSIFIER   0.7674\tRNN                  0.7851\t 0.4932963607808494\n",
      "STACKED_CLASSIFIER   0.7674\tPARSER               0.7908\t 0.11321794589246366\n",
      "\n",
      "RNN                  0.7851\tSTACKED_CLASSIFIER   0.7674\t 0.4932963607808494\n",
      "RNN                  0.7851\tPARSER               0.7908\t 0.4148572810310714\n",
      "\n",
      "PARSER               0.7908\tSTACKED_CLASSIFIER   0.7674\t 0.11321794589246366\n",
      "PARSER               0.7908\tRNN                  0.7851\t 0.4148572810310714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN only differs\n",
    "print_comparison(sc_ysbytag, sc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKED_CLASSIFIER   \tRNN                 \t 1.6427816964126646e-05\n",
      "STACKED_CLASSIFIER   \tPARSER              \t 0.6931012792713278\n",
      "\n",
      "RNN                  \tSTACKED_CLASSIFIER  \t 1.6427816964126646e-05\n",
      "RNN                  \tPARSER              \t 0.0003808264095775129\n",
      "\n",
      "PARSER               \tSTACKED_CLASSIFIER  \t 0.6931012792713278\n",
      "PARSER               \tRNN                 \t 0.0003808264095775129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_comparison(cb_ysbytag, cb_preds, stats_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STACKED_CLASSIFIER   \tRNN                 \t 0.4932963607808494\n",
      "STACKED_CLASSIFIER   \tPARSER              \t 0.11321794589246366\n",
      "\n",
      "RNN                  \tSTACKED_CLASSIFIER  \t 0.4932963607808494\n",
      "RNN                  \tPARSER              \t 0.4148572810310714\n",
      "\n",
      "PARSER               \tSTACKED_CLASSIFIER  \t 0.11321794589246366\n",
      "PARSER               \tRNN                 \t 0.4148572810310714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_comparison(sc_ysbytag, sc_preds, stats_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
