{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "from window_based_tagger_config import get_config\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shared code from the results folder\n",
    "import sys\n",
    "sys.path.append(\"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Results\")\n",
    "\n",
    "from results_common import get_essays, validate_essays, tally_essay_attributes\n",
    "from process_essays_coref import processed_essays_predict_most_recent_tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "settings = Settings()\n",
    "\n",
    "DATASET = \"CoralBleaching\"  # SkinCancer\n",
    "\n",
    "root_folder = settings.data_directory +  DATASET + \"/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "\n",
    "# Get Test Data In Order to Get Test CRELS\n",
    "# load the test essays to make sure we compute metrics over the test CR labels\n",
    "test_config = get_config(test_folder)\n",
    "\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "berkeley_coref_predictions_folder = root_folder + \"CoReference/Berkeley/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Optimal Parameters (from Hyper Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berkeley best for CB\n",
    "coref_predictions_folder = berkeley_coref_predictions_folder\n",
    "coref_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/CRel'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder for the output (input to the CRel parser model)\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "output_folder = coref_root + \"CRel\"\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-Ref folder: /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/\n",
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/training_processed.dill\n",
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/test_processed.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Co-Ref folder:\", coref_predictions_folder)\n",
    "\n",
    "coref_train_essays = get_essays(coref_predictions_folder, \"Training\")\n",
    "coref_test_essays = get_essays(coref_predictions_folder, \"Test\")\n",
    "\n",
    "len(coref_train_essays), len(coref_test_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = rnn_predictions_folder + \"essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "    \n",
    "len(pred_tagged_essays_train), len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "reg_tally = defaultdict(int)\n",
    "crel_tally = defaultdict(int)\n",
    "crel_ana_tally = defaultdict(int)\n",
    "for e in pred_tagged_essays_train + pred_tagged_essays_test:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                t_lower = t.lower()\n",
    "                if \"rhet\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "                    continue\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if \"->\" in t and (\"ana\" not in t_lower and \n",
    "                                  \"other\" not in t_lower and \n",
    "                                  \"rhet\" not in t_lower and \n",
    "                                  \"change\" not in t_lower):\n",
    "                    crel_tally[t] += 1\n",
    "                if \"->\" in t and ANAPHORA in t:\n",
    "                    crel_ana_tally[t] += 1\n",
    "                    \n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "crel_tags = sorted(crel_tally.keys())\n",
    "\n",
    "cc_crel_tags_filter = set(reg_tags + crel_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_the_same(essay_sets):\n",
    "    unique_fnames = [] # list of sets of str (fnames)\n",
    "    for essay_collection in essay_sets:\n",
    "        names = set()\n",
    "        for e in essay_collection:\n",
    "            names.add(e.name)\n",
    "        unique_fnames.append(names)\n",
    "    for a in unique_fnames:\n",
    "        print(len(a))\n",
    "        for b in unique_fnames:\n",
    "            assert len(a) == len(b), \"lens don't match\"\n",
    "            assert a == b, \"don't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essays_2_hash_map(essays):\n",
    "    lu = {}\n",
    "    for e in essays:\n",
    "        lu[e.name] = e\n",
    "    return lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks the number of words and sentences are the same for 2 sets of essays\n",
    "def validate_tagged_essays(essays_a, essays_b, tags_filter):\n",
    "    # make sure obj is not the same\n",
    "    assert essays_a != essays_b\n",
    "    print(\"Validating\", len(essays_a), \"essays\")\n",
    "    assert len(essays_a) == len(essays_b), \"Lens don't match\"\n",
    "    \n",
    "    a_hmap = essays_2_hash_map(essays_a)\n",
    "    b_hmap = essays_2_hash_map(essays_b)\n",
    "    \n",
    "    # same essays?\n",
    "    assert a_hmap.keys() == b_hmap.keys()\n",
    "    intersect = set(a_hmap.keys()).intersection(b_hmap.keys())\n",
    "    assert len(intersect) == len(a_hmap.keys())\n",
    "    assert len(a_hmap.keys()) > 1    \n",
    "    assert len(a_hmap.keys()) == len(b_hmap.keys())\n",
    "    \n",
    "    word_misses = 0\n",
    "    \n",
    "    for key, a_essay in a_hmap.items():\n",
    "        b_essay = b_hmap[key]\n",
    "        # assert NOT the same obj ref\n",
    "        assert a_essay != b_essay\n",
    "        assert len(a_essay.sentences) == len(b_essay.sentences)\n",
    "        assert len(a_essay.sentences) > 0\n",
    "        assert len(b_essay.sentences) > 0\n",
    "        for i in range(len(a_essay.sentences)):\n",
    "            a_sent = a_essay.sentences[i]\n",
    "            b_sent = b_essay.sentences[i]\n",
    "            # the same lists?\n",
    "            #assert a_sent == b_sent\n",
    "            assert len(a_sent) == len(b_sent)\n",
    "            if not len(a_sent) == len(b_sent):\n",
    "                print(key, \"\\tsent-ix:\", i, \"lens\", len(a_sent), len(b_sent))\n",
    "            for wd_ix, (a_wd, a_tags) in enumerate(a_sent):\n",
    "                b_wd, b_tags = b_sent[wd_ix]\n",
    "                if a_wd != b_wd:\n",
    "                    word_misses+=1\n",
    "                assert a_wd   == b_wd,   \\\n",
    "                    \"Words don't match: '{a}' - '{b}', Esssay: {essay} Sent Ix: {i}\".format(\n",
    "                            a=a_wd, b=b_wd, essay=key, i=i)\n",
    "                \n",
    "                # SH - Make conditional, as untagged essays contain new anaphora tags\n",
    "                filtered_a_tags = tags_filter.intersection(a_tags)\n",
    "                filtered_b_tags = tags_filter.intersection(b_tags)\n",
    "\n",
    "                assert filtered_a_tags == filtered_b_tags, \\\n",
    "                    \"Tags don't match: '{a}' - '{b}', Esssay: {essay} Sent Ix: {i}\".format(\n",
    "                        a=str(filtered_a_tags), b=str(filtered_b_tags), essay=key, i=i)                \n",
    "                        \n",
    "    if word_misses:\n",
    "        print(\"Word miss-matches: \", word_misses)\n",
    "    print(\"Validation Passed\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Non Anaphora Labels for Comparison (Should Match Across Essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902\n",
      "902\n"
     ]
    }
   ],
   "source": [
    "names_the_same([coref_train_essays, pred_tagged_essays_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "names_the_same([coref_test_essays, pred_tagged_essays_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 902 essays\n",
      "Validation Passed\n"
     ]
    }
   ],
   "source": [
    "validate_tagged_essays(essays_a=coref_train_essays, essays_b=pred_tagged_essays_train,\n",
    "                       tags_filter=cc_crel_tags_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 226 essays\n",
      "Validation Passed\n"
     ]
    }
   ],
   "source": [
    "validate_tagged_essays(essays_a=coref_test_essays, essays_b=pred_tagged_essays_test,\n",
    "                       tags_filter=cc_crel_tags_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Predictions from Crel Essays with CoRef Data from CoRef Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the coref essays (used for predictions), and copy over the prediction tags from the \n",
    "# crel essays. We do this as we also need the Anaphora labels from the CoRef data\n",
    "def combine_essays(crel_essays, coref_essays):\n",
    "    \n",
    "    crel_hmap = essays_2_hash_map(crel_essays)\n",
    "    coref_hmap = essays_2_hash_map(coref_essays)\n",
    "    \n",
    "    new_essays = []\n",
    "    for key, crel_essay in crel_hmap.items():\n",
    "        coref_essay = coref_hmap[key]\n",
    "        # clone from coref essay so we grab the anaphora labels (not present in those essays)\n",
    "        new_essay = coref_essay.clone()\n",
    "        \n",
    "        # copy coref data from coref essay\n",
    "        new_essay.ana_tagged_sentences    = coref_essay.ana_tagged_sentences\n",
    "        new_essay.pred_corefids           = coref_essay.pred_corefids\n",
    "        new_essay.pred_ner_tags_sentences = coref_essay.pred_ner_tags_sentences\n",
    "        new_essay.pred_pos_tags_sentences = coref_essay.pred_pos_tags_sentences\n",
    "        \n",
    "        # BUT copy predictions from the crel essay\n",
    "        new_essay.pred_tagged_sentences = crel_essay.pred_tagged_sentences\n",
    "        new_essays.append(new_essay)\n",
    "    return new_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_essays = combine_essays(crel_essays=pred_tagged_essays_train, coref_essays=coref_train_essays)\n",
    "merged_test_essays  = combine_essays(crel_essays=pred_tagged_essays_test, coref_essays=coref_test_essays)\n",
    "len(merged_train_essays), len(merged_test_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Anaphora Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[13]',\n",
       " 'Anaphor:[14]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[5b]',\n",
       " 'Anaphor:[6]',\n",
       " 'Anaphor:[7]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana_tally = defaultdict(int)\n",
    "all_merged_essays = merged_train_essays + merged_test_essays\n",
    "for e in all_merged_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:                \n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"Anaphor:[\" in t and \"rhetorical\" not in t and \"->\" not in t:\n",
    "                        ana_tally[t] += 1\n",
    "\n",
    "all_ana_tags = sorted(ana_tally.keys())\n",
    "assert len(all_ana_tags) == len(reg_tags), \"Number of anaphora tags doesn't match the number of regular tags\"\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Causer:1->Result:Anaphor': 142,\n",
       "             'Causer:1->Result:Anaphor[3]': 6,\n",
       "             'Causer:1->Result:Anaphor[50]': 136,\n",
       "             'Causer:11->Result:Anaphor': 90,\n",
       "             'Causer:11->Result:Anaphor[14]': 54,\n",
       "             'Causer:11->Result:Anaphor[1]': 7,\n",
       "             'Causer:11->Result:Anaphor[50]': 26,\n",
       "             'Causer:13->Result:Anaphor': 52,\n",
       "             'Causer:13->Result:Anaphor[14]': 11,\n",
       "             'Causer:13->Result:Anaphor[50]': 41,\n",
       "             'Causer:2->Result:Anaphor': 17,\n",
       "             'Causer:2->Result:Anaphor[50]': 17,\n",
       "             'Causer:3->Result:Anaphor': 99,\n",
       "             'Causer:3->Result:Anaphor[4]': 8,\n",
       "             'Causer:3->Result:Anaphor[50]': 82,\n",
       "             'Causer:3->Result:Anaphor[7]': 9,\n",
       "             'Causer:4->Result:Anaphor': 52,\n",
       "             'Causer:4->Result:Anaphor[50]': 7,\n",
       "             'Causer:4->Result:Anaphor[5]': 45,\n",
       "             'Causer:5->Result:Anaphor': 4,\n",
       "             'Causer:5->Result:Anaphor[50]': 4,\n",
       "             'Causer:5b->Result:Anaphor': 12,\n",
       "             'Causer:5b->Result:Anaphor[14]': 12,\n",
       "             'Causer:6->Result:Anaphor': 28,\n",
       "             'Causer:6->Result:Anaphor[50]': 9,\n",
       "             'Causer:6->Result:Anaphor[7]': 19,\n",
       "             'Causer:7->Result:Anaphor': 125,\n",
       "             'Causer:7->Result:Anaphor[50]': 116,\n",
       "             'Causer:7->Result:Anaphor[6]': 9,\n",
       "             'Causer:Anaphor->Result:11': 9,\n",
       "             'Causer:Anaphor->Result:12': 54,\n",
       "             'Causer:Anaphor->Result:13': 43,\n",
       "             'Causer:Anaphor->Result:14': 281,\n",
       "             'Causer:Anaphor->Result:2': 159,\n",
       "             'Causer:Anaphor->Result:3': 138,\n",
       "             'Causer:Anaphor->Result:4': 30,\n",
       "             'Causer:Anaphor->Result:5': 18,\n",
       "             'Causer:Anaphor->Result:50': 712,\n",
       "             'Causer:Anaphor->Result:5b': 127,\n",
       "             'Causer:Anaphor->Result:6': 87,\n",
       "             'Causer:Anaphor->Result:7': 149,\n",
       "             'Causer:Anaphor->Result:Anaphor': 5,\n",
       "             'Causer:Anaphor->Result:Anaphor[50]': 5,\n",
       "             'Causer:Anaphor[11]->Result:12': 54,\n",
       "             'Causer:Anaphor[11]->Result:13': 5,\n",
       "             'Causer:Anaphor[11]->Result:50': 7,\n",
       "             'Causer:Anaphor[12]->Result:13': 28,\n",
       "             'Causer:Anaphor[12]->Result:14': 35,\n",
       "             'Causer:Anaphor[12]->Result:50': 26,\n",
       "             'Causer:Anaphor[13]->Result:14': 147,\n",
       "             'Causer:Anaphor[13]->Result:50': 84,\n",
       "             'Causer:Anaphor[14]->Result:11': 9,\n",
       "             'Causer:Anaphor[14]->Result:50': 114,\n",
       "             'Causer:Anaphor[14]->Result:6': 15,\n",
       "             'Causer:Anaphor[1]->Result:2': 159,\n",
       "             'Causer:Anaphor[1]->Result:3': 91,\n",
       "             'Causer:Anaphor[1]->Result:4': 17,\n",
       "             'Causer:Anaphor[1]->Result:50': 63,\n",
       "             'Causer:Anaphor[1]->Result:6': 18,\n",
       "             'Causer:Anaphor[2]->Result:3': 38,\n",
       "             'Causer:Anaphor[2]->Result:4': 8,\n",
       "             'Causer:Anaphor[2]->Result:50': 9,\n",
       "             'Causer:Anaphor[2]->Result:6': 8,\n",
       "             'Causer:Anaphor[2]->Result:7': 3,\n",
       "             'Causer:Anaphor[3]->Result:14': 7,\n",
       "             'Causer:Anaphor[3]->Result:4': 5,\n",
       "             'Causer:Anaphor[3]->Result:5': 18,\n",
       "             'Causer:Anaphor[3]->Result:50': 139,\n",
       "             'Causer:Anaphor[3]->Result:6': 16,\n",
       "             'Causer:Anaphor[3]->Result:7': 35,\n",
       "             'Causer:Anaphor[3]->Result:Anaphor': 5,\n",
       "             'Causer:Anaphor[4]->Result:13': 5,\n",
       "             'Causer:Anaphor[4]->Result:14': 59,\n",
       "             'Causer:Anaphor[4]->Result:3': 9,\n",
       "             'Causer:Anaphor[4]->Result:50': 45,\n",
       "             'Causer:Anaphor[4]->Result:5b': 7,\n",
       "             'Causer:Anaphor[4]->Result:6': 10,\n",
       "             'Causer:Anaphor[4]->Result:7': 23,\n",
       "             'Causer:Anaphor[5]->Result:50': 16,\n",
       "             'Causer:Anaphor[5]->Result:5b': 89,\n",
       "             'Causer:Anaphor[5]->Result:7': 6,\n",
       "             'Causer:Anaphor[5b]->Result:50': 40,\n",
       "             'Causer:Anaphor[6]->Result:14': 18,\n",
       "             'Causer:Anaphor[6]->Result:50': 46,\n",
       "             'Causer:Anaphor[6]->Result:7': 82,\n",
       "             'Causer:Anaphor[7]->Result:14': 15,\n",
       "             'Causer:Anaphor[7]->Result:50': 232,\n",
       "             'Causer:Anaphor[7]->Result:5b': 31,\n",
       "             'Causer:Anaphor[7]->Result:6': 8})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crel_ana_tally = defaultdict(int)\n",
    "\n",
    "for e in all_merged_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                t_lower = t.lower()\n",
    "                if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "                    continue\n",
    "                if \"->\" in t and ANAPHORA in t:\n",
    "                    crel_ana_tally[t] += 1\n",
    "crel_ana_tally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Would Ana Resolution work with the CRel Predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999732</td>\n",
       "      <td>1783158.0</td>\n",
       "      <td>0.246057</td>\n",
       "      <td>344.0</td>\n",
       "      <td>0.268966</td>\n",
       "      <td>0.226744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  data_points  f1_score  num_codes  precision    recall\n",
       "0  0.999732    1783158.0  0.246057      344.0   0.268966  0.226744"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from process_essays_coref import get_coref_processed_essays\n",
    "from metrics import get_metrics_raw\n",
    "\n",
    "format_ana_tags = True # use this as true to eval performance, but then change to False for the actual exercise    \n",
    "processed_train_essays_ana = processed_essays_predict_most_recent_tag(\n",
    "                            essays=merged_train_essays, \n",
    "                            format_ana_tags=format_ana_tags)\n",
    "\n",
    "metrics = get_metrics_raw(processed_train_essays_ana, expected_tags=all_ana_tags,  micro_only=True)    \n",
    "pd.DataFrame([metrics[\"MICRO_F1\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Final Set of Essays\n",
    "- Add in new predicted Anaphora tags as additional regular codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_ana_tags = False # Set to false so Anaphora codes are merged in with the regular codes\n",
    "processed_train_essays_full = get_coref_processed_essays(\n",
    "                            essays=merged_train_essays, \n",
    "                            format_ana_tags=format_ana_tags)\n",
    "\n",
    "processed_test_essays_full = get_coref_processed_essays(\n",
    "                            essays=merged_test_essays, \n",
    "                            format_ana_tags=format_ana_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate there are Differences in the New Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() {'50'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'7'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'12', '13'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'12', '13'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'3'} Anaphor\n",
      "set() {'3'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'6'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'3'} Anaphor\n",
      "set() {'7'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'11'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'1'} Anaphor\n"
     ]
    }
   ],
   "source": [
    "EMPTY = 'Empty'\n",
    "for a,b in zip(merged_train_essays, processed_train_essays_full):\n",
    "    assert len(a.sentences)  == len(b.sentences)\n",
    "    assert a.name == b.name\n",
    "    \n",
    "    assert len(a.pred_tagged_sentences) == len(b.pred_tagged_sentences)\n",
    "    \n",
    "    sent_ix =- 1\n",
    "    for atag_sent, btag_sent in zip(a.pred_tagged_sentences, b.pred_tagged_sentences):        \n",
    "        sent_ix += 1\n",
    "        word_ix = -1\n",
    "        for atags, btags in zip(atag_sent, btag_sent):\n",
    "            word_ix+=1\n",
    "            atags = set([atags])\n",
    "            if EMPTY in atags:\n",
    "                atags.remove(EMPTY)\n",
    "            if atags != btags:\n",
    "                print(atags, btags, b.ana_tagged_sentences[sent_ix][word_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in Additional Crel Codes (from Anaphora Codes mapped to Regular Crel Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Causer:11->Result:1',\n",
       " 'Causer:14->Result:11',\n",
       " 'Causer:2->Result:4',\n",
       " 'Causer:2->Result:7',\n",
       " 'Causer:7->Result:6'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "def get_anaphora_crel_codes(tags):\n",
    "    \n",
    "    additional_codes = set()\n",
    "    prefix = \"Anaphor[\"\n",
    "    \n",
    "    for k in tags:        \n",
    "        if prefix in k: # has an Anaphor tag with an indentified code\n",
    "            k_lower = k.lower()\n",
    "            if \"rhetorical\" in k_lower or \"other\" in k_lower or \"change\" in k_lower:\n",
    "                continue\n",
    "            k_fixed = k.replace(prefix, \"\").replace(\"]\",\"\")\n",
    "            if ANAPHORA not in k_fixed:\n",
    "                l,r = k_fixed.split(\"->\")\n",
    "                l_code = l.replace(\"Causer:\",\"\")\n",
    "                r_code = r.replace(\"Result:\",\"\")\n",
    "                assert is_a_regular_code(l_code), l_code\n",
    "                assert is_a_regular_code(r_code), r_code\n",
    "                additional_codes.add(k_fixed)\n",
    "    return additional_codes\n",
    "\n",
    "add_crel_codes = get_anaphora_crel_codes(crel_ana_tally.keys())\n",
    "# did we add any new unique tags?\n",
    "add_crel_codes - set(crel_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_set_to_str(predicted_tags):\n",
    "    # Crel parser expects tags as strings not sets\n",
    "    new_predicted_tags = []\n",
    "    for sent in predicted_tags:\n",
    "        new_sent = []\n",
    "        for tags in sent:\n",
    "            if len(tags) == 0:\n",
    "                new_sent.append(EMPTY)\n",
    "            elif len(tags) == 1:\n",
    "                first = list(tags)[0]\n",
    "                new_sent.append(first)\n",
    "            else: # more than 1, pick the most common\n",
    "                most_common = sorted(tags, key = lambda t: -reg_tally[t])[0]\n",
    "                new_sent.append(most_common)\n",
    "        assert len(sent) == len(new_sent)\n",
    "        new_predicted_tags.append(new_sent)\n",
    "    \n",
    "    assert len(new_predicted_tags) == len(predicted_tags)\n",
    "    return new_predicted_tags\n",
    "\n",
    "def add_crel_ana_codes_as_regular_relations(essays):\n",
    "    new_essays = []\n",
    "    for e in essays:\n",
    "        new_essay = e.clone()\n",
    "        new_essays.append(new_essay)\n",
    "        \n",
    "        new_essay.ana_tagged_sentences    = e.ana_tagged_sentences\n",
    "        new_essay.pred_corefids           = e.pred_corefids\n",
    "        new_essay.pred_ner_tags_sentences = e.pred_ner_tags_sentences\n",
    "        new_essay.pred_pos_tags_sentences = e.pred_pos_tags_sentences\n",
    "        new_essay.pred_tagged_sentences   = flatten_set_to_str(e.pred_tagged_sentences)\n",
    "        \n",
    "        new_essay.sentences = []\n",
    "        for sent in e.sentences:\n",
    "            new_sent = []\n",
    "            new_essay.sentences.append(new_sent)\n",
    "            for wd, tags in sent:\n",
    "                new_tags = set(tags)\n",
    "                addnl_crel_tags = get_anaphora_crel_codes(tags)\n",
    "                if len(addnl_crel_tags) > 0:\n",
    "                    new_tags.update(addnl_crel_tags)\n",
    "                new_sent.append((wd, new_tags))\n",
    "            assert len(new_sent) == len(sent)\n",
    "        assert len(new_essay.sentences) == len(e.sentences)\n",
    "    return new_essays      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_essays_full_with_codes = add_crel_ana_codes_as_regular_relations(processed_train_essays_full)\n",
    "processed_test_essays_full_with_codes  = add_crel_ana_codes_as_regular_relations(processed_test_essays_full)\n",
    "\n",
    "len(processed_train_essays_full_with_codes), len(processed_test_essays_full_with_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the new essays are different - this should blow up (it does)\n",
    "# validate_tagged_essays(processed_train_essays_full, processed_train_essays_full_with_codes, set(crel_tags))\n",
    "# validate_tagged_essays(processed_test_essays_full, processed_test_essays_full_with_codes, set(crel_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-SEARN/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_predictions_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "p = pathlib.Path(output_folder)\n",
    "p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "fname = output_folder + \"/training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(fname, \"wb+\") as f:\n",
    "    dill.dump(processed_train_essays_full_with_codes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "fname = output_folder + \"/test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(fname, \"wb+\") as f:\n",
    "    dill.dump(processed_test_essays_full_with_codes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
