{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "CoRef Data:  /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Results\")\n",
    "\n",
    "from results_common import get_essays, validate_essays, tally_essay_attributes\n",
    "from process_essays_coref import get_coref_processed_essays, processed_essays_predict_most_recent_tag\n",
    "from metrics import get_metrics_raw\n",
    "\n",
    "# progress bar widget\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET = \"CoralBleaching\" # CoralBleaching | SkinCancer\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "berkeley_coref_predictions_folder = root_folder + \"CoReference/Berkeley/\"\n",
    "# Which algorithm?\n",
    "coref_predictions_folder = berkeley_coref_predictions_folder\n",
    "print(\"CoRef Data: \", stanford_coref_predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/training_processed.dill\n"
     ]
    }
   ],
   "source": [
    "training_essays = get_essays(coref_predictions_folder, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/test_processed.dill\n"
     ]
    }
   ],
   "source": [
    "test_essays = get_essays(coref_predictions_folder, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = training_essays + test_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Anaphor Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[13]',\n",
       " 'Anaphor:[14]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[5b]',\n",
       " 'Anaphor:[6]',\n",
       " 'Anaphor:[7]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "cc_tally = defaultdict(int)\n",
    "cr_tally = defaultdict(int)\n",
    "reg_tally = defaultdict(int)\n",
    "for e in all_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"->\" in t:\n",
    "                        cr_tally[t] += 1\n",
    "                    elif \"Anaphor:[\" in t:\n",
    "                        cc_tally[t] += 1\n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "all_ana_tags = sorted(cc_tally.keys())\n",
    "assert len(reg_tags) == len(all_ana_tags)\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_if_none(val):\n",
    "    return \"-\" if (val is None or not val or str(val).lower() == \"none\") else val\n",
    "\n",
    "def replace_if_blank(val, replace):\n",
    "    if val == \"\" or val == \"-\":\n",
    "        return replace\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With Anaphora Prediction Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_essays_coref import *\n",
    "def processed_essays_predict_most_recent_tag(essays, format_ana_tags=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Uses the most recently predicted concept code as the predicted tag\n",
    "    \n",
    "            essays:                   List[Essay] objects - merged tagged essays\n",
    "    \"\"\"\n",
    "\n",
    "    ana_tagged_essays = []\n",
    "    for eix, e in enumerate(essays):\n",
    "\n",
    "        fix_coref_ids(e)\n",
    "        \n",
    "        # following are flattened so they span sentences\n",
    "        seq_pred_tags  = [] # all predicted tags\n",
    "        seq_is_ana_tag = [] # is ana tag\n",
    "        seq_ix = -1\n",
    "        \n",
    "        ana_tagged_e = Essay(e.name, e.sentences)\n",
    "        ana_tagged_e.pred_tagged_sentences = []\n",
    "        ana_tagged_e.pred_pos_tags_sentences = list(e.pred_pos_tags_sentences)\n",
    "        ana_tagged_e.pred_ner_tags_sentences = list(e.pred_pos_tags_sentences)\n",
    "        ana_tagged_e.ana_tagged_sentences    = list(e.ana_tagged_sentences)\n",
    "        ana_tagged_e.pred_corefids           = list(e.pred_corefids)\n",
    "        ana_tagged_essays.append(ana_tagged_e)\n",
    "\n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            ana_tagged_sent = []\n",
    "            ana_tagged_e.pred_tagged_sentences.append(ana_tagged_sent)\n",
    "\n",
    "            sent = e.sentences[sent_ix]\n",
    "\n",
    "            # SENTENCE LEVEL TAGS / PREDICTIONS\n",
    "            ana_tags = e.ana_tagged_sentences[sent_ix]\n",
    "            coref_ids = e.pred_corefids[sent_ix]\n",
    "            # ner_tags = e.pred_ner_tags_sentences[sent_ix]\n",
    "            pos_tags = e.pred_pos_tags_sentences[sent_ix]\n",
    "            ptags = e.pred_tagged_sentences[sent_ix]\n",
    "\n",
    "            for wd_ix in range(len(sent)):\n",
    "                seq_ix +=1\n",
    "                \n",
    "                pos_tag = pos_tags[wd_ix]  # POS tag\n",
    "\n",
    "                word, _ = sent[wd_ix]  # ignore actual tags\n",
    "                pred_cc_tag = ptags[wd_ix]  # predict cc tag\n",
    "                seq_pred_tags.append(pred_cc_tag)\n",
    "\n",
    "                is_ana_tag = ana_tags[wd_ix] == ANAPHORA\n",
    "                seq_is_ana_tag.append(is_ana_tag)\n",
    "                \n",
    "                wd_coref_ids = coref_ids[wd_ix]  # Set[str]\n",
    "\n",
    "                # note we are changing this to a set rather than a single string\n",
    "                wd_ptags = set()\n",
    "                # initialize predicted tags, inc. cc tag\n",
    "                # DON'T run continue until after this point\n",
    "                ana_tagged_sent.append(wd_ptags)\n",
    "\n",
    "                # add predicted concept code tag (filtered out by evaluation code, which filters to specific tags)\n",
    "                if pred_cc_tag != EMPTY:\n",
    "                    wd_ptags.add(pred_cc_tag)\n",
    "                # else here because we don't want to assign additional cc tags if there are already ones\n",
    "                elif is_ana_tag and pred_cc_tag == EMPTY: # and current tag is EMPTY\n",
    "                    code = find_previous_predicted_tag(seq_ix, seq_pred_tags, seq_is_ana_tag)  \n",
    "                    if code is None:\n",
    "                    \tcode = EMPTY              \n",
    "                    if format_ana_tags:\n",
    "                        code = \"{anaphora}:[{code}]\".format(anaphora=ANAPHORA, code=code)\n",
    "                    wd_ptags.add(code)\n",
    "\n",
    "    # validation check\n",
    "    #   check essay and sent lengths align\n",
    "    for e in ana_tagged_essays:\n",
    "        assert len(e.sentences) == len(e.pred_tagged_sentences)\n",
    "        for ix in range(len(e.sentences)):\n",
    "            assert len(e.sentences[ix]) == len(e.pred_tagged_sentences[ix])\n",
    "\n",
    "    return ana_tagged_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(essays, expected_tags, format_ana_tags=True):\n",
    "    rows_ana = []\n",
    "    proc_essays = processed_essays_predict_most_recent_tag(essays=essays, format_ana_tags=format_ana_tags)\n",
    "\n",
    "    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)\n",
    "    row = metrics[\"MICRO_F1\"]\n",
    "    rows_ana.append(row)\n",
    "\n",
    "    df_results = pd.DataFrame(rows_ana)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999738</td>\n",
       "      <td>1783158.0</td>\n",
       "      <td>0.262243</td>\n",
       "      <td>344.0</td>\n",
       "      <td>0.287197</td>\n",
       "      <td>0.241279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  data_points  f1_score  num_codes  precision    recall\n",
       "0  0.999738    1783158.0  0.262243      344.0   0.287197  0.241279"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = grid_search(essays=training_essays, expected_tags=all_ana_tags)\n",
    "df_train # 0.262 0.287, 0.241"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999885</td>\n",
       "      <td>399087.0</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.282051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  data_points  f1_score  num_codes  precision    recall\n",
       "0  0.999885     399087.0  0.323529       39.0    0.37931  0.282051"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = grid_search(essays=test_essays, expected_tags=all_ana_tags)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_tags_by_word(essays_a):\n",
    "    diff_by_sent = defaultdict(list)\n",
    "    i = -1\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.pred_tagged_sentences:\n",
    "            i += 1\n",
    "            for tags in asent:\n",
    "                for t in tags:\n",
    "                    if t.startswith(\"Ana\"):\n",
    "                        diff_by_sent[i].append(t)\n",
    "    return diff_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {28: ['Anaphor:[50]'],\n",
       "             41: ['Anaphor:[50]'],\n",
       "             241: ['Anaphor:[50]'],\n",
       "             410: ['Anaphor:[1]'],\n",
       "             455: ['Anaphor:[4]'],\n",
       "             545: ['Anaphor:[13]'],\n",
       "             615: ['Anaphor:[13]'],\n",
       "             657: ['Anaphor:[50]'],\n",
       "             695: ['Anaphor:[3]'],\n",
       "             808: ['Anaphor:[2]'],\n",
       "             886: ['Anaphor:[50]'],\n",
       "             950: ['Anaphor:[50]'],\n",
       "             967: ['Anaphor:[6]'],\n",
       "             1009: ['Anaphor:[50]'],\n",
       "             1045: ['Anaphor:[1]'],\n",
       "             1071: ['Anaphor:[1]', 'Anaphor:[1]'],\n",
       "             1110: ['Anaphor:[14]'],\n",
       "             1111: ['Anaphor:[11]'],\n",
       "             1112: ['Anaphor:[13]'],\n",
       "             1121: ['Anaphor:[1]'],\n",
       "             1123: ['Anaphor:[4]'],\n",
       "             1154: ['Anaphor:[5]'],\n",
       "             1213: ['Anaphor:[4]'],\n",
       "             1229: ['Anaphor:[1]'],\n",
       "             1551: ['Anaphor:[1]'],\n",
       "             1596: ['Anaphor:[11]'],\n",
       "             1694: ['Anaphor:[1]'],\n",
       "             1744: ['Anaphor:[4]']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_essays = processed_essays_predict_most_recent_tag(essays=test_essays, format_ana_tags=True)\n",
    "tally = get_predicted_tags_by_word(pred_test_essays)\n",
    "tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i, tags in tally.items():\n",
    "    for t in tags:\n",
    "        count +=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
