{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Results\")\n",
    "\n",
    "from results_common import get_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient()\n",
    "db = client.metrics\n",
    "\n",
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "# NOTE: These predictions are generated from the \"./notebooks/SEARN/Keras \n",
    "#  - Train Tagger and Save CV Predictions For Word Tags.ipynb\" notebook\n",
    "# used as inputs to parsing model\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "results_processor = ResultsProcessor(dbname=\"metrics_coref_causal2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Causal Relations\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-e3e39e85dc45>\n",
      "/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Causal Relations\n"
     ]
    }
   ],
   "source": [
    "import inspect, os\n",
    "print(inspect.getfile(inspect.currentframe())) # script filename (usually with path)\n",
    "print(os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))) # script directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doesn't matter with parser for this\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "\n",
    "orig_pred_tagged_essays_train = get_essays(stanford_coref_predictions_folder, \"Training\")\n",
    "orig_pred_tagged_essays_test = get_essays(stanford_coref_predictions_folder, \"Test\")\n",
    "\n",
    "# Load original CREL data - without anaphor codes or predictions\n",
    "# NOTE that these files differ for the SC data\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "train_fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    orig_pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = rnn_predictions_folder + \"essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    orig_pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(orig_pred_tagged_essays_train), len(orig_pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'50'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = orig_pred_tagged_essays_train[0]\n",
    "set(e.pred_tagged_sentences[0]) - set(pred_tagged_essays_train[0].pred_tagged_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def get_different_predicted_tags_by_sent(essays_a, essays_b):\n",
    "    assert len(essays_a) == len(essays_b)\n",
    "    diff_by_sent = []\n",
    "    for ea, eb in zip(essays_a, essays_b):\n",
    "        assert len(ea.sentences) == len(eb.sentences)\n",
    "        assert len(ea.pred_tagged_sentences) == len(eb.pred_tagged_sentences)\n",
    "        for asent, bsent in zip(ea.pred_tagged_sentences, eb.pred_tagged_sentences):\n",
    "            unique_atags = set(asent)\n",
    "            if EMPTY in unique_atags:\n",
    "                unique_atags.remove(EMPTY)\n",
    "            unique_btags = set(bsent)\n",
    "            if EMPTY in unique_btags:\n",
    "                unique_btags.remove(EMPTY)\n",
    "            diff_by_sent.append(unique_btags - unique_atags)\n",
    "    return diff_by_sent\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_different_crel_tags_by_sent(essays_a, essays_b):\n",
    "    assert len(essays_a) == len(essays_b)\n",
    "    diff_by_sent = []\n",
    "    for ea, eb in zip(essays_a, essays_b):\n",
    "        assert len(ea.sentences) == len(eb.sentences)\n",
    "        for asent, bsent in zip(ea.sentences, eb.sentences):\n",
    "            all_atags, all_btags = set(), set()\n",
    "            for (awd, atags), (bwd, btags) in zip(asent, bsent):\n",
    "                assert awd == bwd\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "                all_btags.update(to_is_valid_crel(btags))\n",
    "            \n",
    "            diff_by_sent.append(all_btags - all_atags)\n",
    "    return diff_by_sent\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "# cv_folds  = [(pred_tagged_essays_train, pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]\n",
    "cv_folds = cross_validation(pred_tagged_essays_train, CV_FOLDS)\n",
    "\n",
    "# cv_folds  = [(pred_tagged_essays_train, pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        collection_prefix: str,\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        extractor_fn_names_lst: List[str],\n",
    "        cost_function_name: str,\n",
    "        beta: float,\n",
    "        ngrams: int,\n",
    "        stemmed: bool,\n",
    "        max_epochs: int,\n",
    "        down_sample_rate=1.0) -> float:\n",
    "\n",
    "    if down_sample_rate < 1.0:\n",
    "        new_folds = []  # type: List[Tuple[Any, Any]]\n",
    "        for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "            essays_TD = essays_TD[:int(down_sample_rate * len(essays_TD))]\n",
    "            essays_VD = essays_VD[:int(down_sample_rate * len(essays_VD))]\n",
    "            new_folds.append((essays_TD, essays_VD))\n",
    "        folds = new_folds  # type: List[Tuple[Any, Any]]\n",
    "\n",
    "    serial_results = [\n",
    "        model_train_predict(essays_TD, essays_VD, extractor_fn_names_lst, cost_function_name, ngrams, stemmed, beta, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    cv_td_preds_by_sent = []\n",
    "    cv_vd_preds_by_sent = []\n",
    "    for (num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "        \n",
    "        cv_td_preds_by_sent.append(td_preds_by_sent)\n",
    "        cv_vd_preds_by_sent.append(vd_preds_by_sent)\n",
    "\n",
    "    # Mongo settings recording\n",
    "    avg_feats = np.mean(number_of_feats)\n",
    "    sent_algo = \"Shift_Reduce_Parser_LR\"\n",
    "\n",
    "    parameters = dict(config)\n",
    "    parameters[\"extractors\"] = list(extractor_fn_names_lst)\n",
    "    parameters[\"num_extractors\"] = len(extractor_fn_names_lst)\n",
    "    parameters[\"cost_function\"] = cost_function_name\n",
    "    parameters[\"beta\"] = beta\n",
    "    parameters[\"max_epochs\"] = max_epochs\n",
    "    parameters[\"no_stacking\"] = True\n",
    "    parameters[\"algorithm\"] = str(BASE_LEARNER_FACT())\n",
    "    parameters[\"ngrams\"] = str(ngrams)\n",
    "    parameters[\"num_feats_MEAN\"] = avg_feats\n",
    "    parameters[\"num_feats_per_fold\"] = number_of_feats\n",
    "    parameters[\"min_feat_freq\"] = MIN_FEAT_FREQ\n",
    "    parameters[\"stemmed\"] = stemmed\n",
    "\n",
    "    print(\"Mean num feats: {avg_feats:.2f}\".format(avg_feats=avg_feats))\n",
    "\n",
    "    TD = collection_prefix + \"_TD\"\n",
    "    VD = collection_prefix + \"_VD\"\n",
    "    if down_sample_rate < 1.0:\n",
    "        print(\"Down sampling at rate: {rate:.5f}, storing temp results\".format(rate=down_sample_rate))\n",
    "        parameters[\"down_sample\"] = down_sample_rate\n",
    "        CB_SENT_TD, CB_SENT_VD = \"__tmp_\" + TD, \"__tmp_\" + TD\n",
    "    else:\n",
    "        CB_SENT_TD, CB_SENT_VD = TD, VD\n",
    "\n",
    "    sent_td_objectid = results_processor.persist_results(CB_SENT_TD, cv_sent_td_ys_by_tag,\n",
    "                                                         cv_sent_td_predictions_by_tag, parameters, sent_algo)\n",
    "    sent_vd_objectid = results_processor.persist_results(CB_SENT_VD, cv_sent_vd_ys_by_tag,\n",
    "                                                         cv_sent_vd_predictions_by_tag, parameters, sent_algo)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    metric = results_processor.get_metric(CB_SENT_VD, sent_vd_objectid, __MICRO_F1__)\n",
    "    return metric, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_sent(tagged_essays, model):\n",
    "    predict_by_sent = []\n",
    "    for essay_ix, essay in enumerate(tagged_essays):\n",
    "        for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "            predicted_tags = essay.pred_tagged_sentences[sent_ix]\n",
    "            pred_relations = model.predict_sentence(taggged_sentence, predicted_tags)\n",
    "            # Store predictions for evaluation\n",
    "            predict_by_sent.append(pred_relations)\n",
    "    return predict_by_sent\n",
    "\n",
    "def model_train_predict(essays_TD, essays_VD, extractor_names, cost_function_name, ngrams, stemmed, beta, max_epochs):\n",
    "    extractors = get_functions_by_name(extractor_names, all_extractor_fns)\n",
    "    # get single cost function\n",
    "    cost_fn = get_functions_by_name([cost_function_name], all_cost_functions)[0]\n",
    "    assert cost_fn is not None, \"Cost function look up failed\"\n",
    "    # Ensure all extractors located\n",
    "    assert len(extractors) == len(extractor_names), \"number of extractor functions does not match the number of names\"\n",
    "\n",
    "    template_feature_extractor = NonLocalTemplateFeatureExtractor(extractors=extractors)\n",
    "    if stemmed:\n",
    "        ngram_extractor = NgramExtractorStemmed(max_ngram_len=ngrams)\n",
    "    else:\n",
    "        ngram_extractor = NgramExtractor(max_ngram_len=ngrams)\n",
    "    parse_model = SearnModelTemplateFeatures(feature_extractor=template_feature_extractor,\n",
    "                                             cost_function=cost_fn,\n",
    "                                             min_feature_freq=MIN_FEAT_FREQ,\n",
    "                                             ngram_extractor=ngram_extractor, cr_tags=cr_tags,\n",
    "                                             base_learner_fact=BASE_LEARNER_FACT,\n",
    "                                             beta=beta,\n",
    "                                             # log_fn=lambda s: print(s))\n",
    "                                             log_fn=lambda s: None)\n",
    "\n",
    "    parse_model.train(essays_TD, max_epochs=max_epochs)\n",
    "\n",
    "    num_feats = template_feature_extractor.num_features()\n",
    "\n",
    "    sent_td_ys_bycode = parse_model.get_label_data(essays_TD)\n",
    "    sent_vd_ys_bycode = parse_model.get_label_data(essays_VD)\n",
    "\n",
    "    sent_td_pred_ys_bycode = parse_model.predict(essays_TD)\n",
    "    sent_vd_pred_ys_bycode = parse_model.predict(essays_VD)\n",
    "\n",
    "    td_preds_by_sent = predict_by_sent(essays_TD, parse_model)\n",
    "    vd_preds_by_sent = predict_by_sent(essays_VD, parse_model)\n",
    "    \n",
    "    return num_feats, sent_td_ys_bycode, sent_vd_ys_bycode, sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_WIDTH = 80\n",
    "\n",
    "# other settings\n",
    "DOWN_SAMPLE_RATE = 1.0  # For faster smoke testing the algorithm\n",
    "BASE_LEARNER_FACT = None\n",
    "COLLECTION_PREFIX = \"CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_MOST_RECENT_CODE\"\n",
    "\n",
    "# some of the other extractors aren't functional if the system isn't able to do a basic parse\n",
    "# so the base extractors are the MVP for getting to a basic parser, then additional 'meta' parse\n",
    "# features from all_extractors can be included\n",
    "base_extractors = [\n",
    "    single_words,\n",
    "    word_pairs,\n",
    "    three_words,\n",
    "    between_word_features\n",
    "]\n",
    "\n",
    "all_extractor_fns = base_extractors + [\n",
    "    word_distance,\n",
    "    valency,\n",
    "    unigrams,\n",
    "    third_order,\n",
    "    label_set,\n",
    "    size_features\n",
    "]\n",
    "\n",
    "all_cost_functions = [\n",
    "    micro_f1_cost,\n",
    "    micro_f1_cost_squared,\n",
    "    micro_f1_cost_plusone,\n",
    "    micro_f1_cost_plusepsilon,\n",
    "    binary_cost,\n",
    "    inverse_micro_f1_cost,\n",
    "    uniform_cost\n",
    "]\n",
    "\n",
    "all_extractor_fn_names = get_function_names(all_extractor_fns)\n",
    "base_extractor_fn_names = get_function_names(base_extractors)\n",
    "all_cost_fn_names = get_function_names(all_cost_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = 1\n",
    "stemmed = True\n",
    "cost_function_name = micro_f1_cost_plusepsilon.__name__\n",
    "dual = True\n",
    "fit_intercept = True\n",
    "beta = 0.5\n",
    "max_epochs = 2\n",
    "C = 0.5\n",
    "penalty = \"l2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean num feats: 27541.40\n"
     ]
    }
   ],
   "source": [
    "BASE_LEARNER_FACT = lambda : LogisticRegression(dual=dual, C=C, penalty=penalty, fit_intercept=fit_intercept)\n",
    "best_extractor_names = ['single_words', 'between_word_features', 'label_set',\n",
    "                                    'three_words', 'third_order', 'unigrams'] # type: List[str]\n",
    "result = evaluate_model(\n",
    "    collection_prefix=COLLECTION_PREFIX,\n",
    "    folds=cv_folds,\n",
    "    extractor_fn_names_lst=best_extractor_names,\n",
    "    cost_function_name=cost_function_name,\n",
    "    ngrams=ngrams,\n",
    "    beta=beta,\n",
    "    stemmed=stemmed,\n",
    "    down_sample_rate=DOWN_SAMPLE_RATE,\n",
    "    max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, \\\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Anaphora CRel Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_metrics_from_dict(act_ys_bycode, pred_ys_bycode, expected_tags):\n",
    "    assert len(act_ys_bycode.keys()) == len(pred_ys_bycode.keys()) == len(expected_tags), \"Miss-matched codes\"\n",
    "    first_tag = list(expected_tags)[0]\n",
    "    last_tag  = list(expected_tags)[-1]\n",
    "    assert len(act_ys_bycode[first_tag]) == len(pred_ys_bycode[first_tag]), \"Different numbers of words\"\n",
    "    assert len(act_ys_bycode[last_tag])  == len(pred_ys_bycode[last_tag]),  \"Different numbers of words\"\n",
    "\n",
    "    metrics = ResultsProcessor.compute_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "    return metrics_to_df(mean_metrics)\n",
    "\n",
    "def compute_metrics_from_essays(tagged_esssays, expected_tags):\n",
    "    act_ys_bycode  = get_wd_level_lbs(  tagged_esssays, expected_tags)\n",
    "    pred_ys_bycode = get_wd_level_preds(tagged_esssays, expected_tags)\n",
    "    return get_metrics_from_dict(act_ys_bycode, pred_ys_bycode, expected_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_folds = cross_validation(pred_tagged_essays_train, CV_FOLDS)\n",
    "orig_cv_folds = cross_validation(orig_pred_tagged_essays_train, CV_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_metrics_by_code(td_preds_by_sent, diffs_train, diffs_crel_train, ys_by_code_train, preds_by_code_train):\n",
    "    for pred_crels, pred_ccodes, crels in zip(td_preds_by_sent, diffs_train, diffs_crel_train):    \n",
    "        for crel in cr_tags:\n",
    "            ys_by_code_train[crel].append( 1 if crel in crels else 0)\n",
    "\n",
    "        # filter predictions to only those that were produced as a result of the ana tags\n",
    "        ana_preds = set()\n",
    "        for crel in pred_crels:\n",
    "            l,r = crel.split(\"->\")\n",
    "            lcode = l.split(\":\")[-1].strip()\n",
    "            rcode = r.split(\":\")[-1].strip()\n",
    "            if lcode in pred_ccodes or rcode in pred_ccodes:\n",
    "                ana_preds.add(crel)\n",
    "\n",
    "        for crel in cr_tags:\n",
    "            preds_by_code_train[crel].append( 1 if crel in ana_preds else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Concept Codes - correspond to Ana tags\n",
    "preds_by_code_train = defaultdict(list)\n",
    "ys_by_code_train    = defaultdict(list)\n",
    "\n",
    "preds_by_code_test = defaultdict(list)\n",
    "ys_by_code_test    = defaultdict(list)\n",
    "\n",
    "for (orig_td, orig_vd), (td, vd), td_preds_by_sent, vd_preds_by_sent in zip(orig_cv_folds, cv_folds, cv_td_preds_by_sent, cv_vd_preds_by_sent):\n",
    "    \n",
    "    diffs_train = get_different_predicted_tags_by_sent(essays_a=orig_td, essays_b=td)\n",
    "    diffs_test  = get_different_predicted_tags_by_sent(essays_a=orig_vd, essays_b=vd)\n",
    "\n",
    "    # Actual Ana CRels\n",
    "    diffs_crel_train = get_different_crel_tags_by_sent(essays_a=orig_td, essays_b=td)\n",
    "    diffs_crel_test  = get_different_crel_tags_by_sent(essays_a=orig_vd, essays_b=vd)\n",
    "    \n",
    "    build_metrics_by_code(td_preds_by_sent, diffs_train, diffs_crel_train, ys_by_code_train, preds_by_code_train)\n",
    "    build_metrics_by_code(vd_preds_by_sent, diffs_test,  diffs_crel_test,  ys_by_code_test,  preds_by_code_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.999681</td>\n",
       "      <td>0.062378</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  precision    recall\n",
       "95  0.999681  0.062378   0.484848  0.033333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = get_metrics_from_dict(ys_by_code_train, preds_by_code_train, cr_tags)\n",
    "df_train[df_train.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.999681</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  precision    recall\n",
       "95  0.999681  0.062257   0.470588  0.033333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = get_metrics_from_dict(ys_by_code_test, preds_by_code_test, cr_tags)\n",
    "df_test[df_test.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_test_folds  = [(orig_pred_tagged_essays_train, orig_pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]\n",
    "test_folds     = [(pred_tagged_essays_train, pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean num feats: 30804.00\n"
     ]
    }
   ],
   "source": [
    "result_test = evaluate_model(\n",
    "    collection_prefix=COLLECTION_PREFIX,\n",
    "    folds=test_folds,\n",
    "    extractor_fn_names_lst=best_extractor_names,\n",
    "    cost_function_name=cost_function_name,\n",
    "    ngrams=ngrams,\n",
    "    beta=beta,\n",
    "    stemmed=stemmed,\n",
    "    down_sample_rate=DOWN_SAMPLE_RATE,\n",
    "    max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, \\\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent = result_test\n",
    "    \n",
    "# Predicted Concept Codes - correspond to Ana tags\n",
    "preds_by_code_train = defaultdict(list)\n",
    "ys_by_code_train    = defaultdict(list)\n",
    "\n",
    "preds_by_code_test = defaultdict(list)\n",
    "ys_by_code_test    = defaultdict(list)\n",
    "\n",
    "for (orig_td, orig_vd), (td, vd), td_preds_by_sent, vd_preds_by_sent in zip(orig_test_folds, test_folds, cv_td_preds_by_sent, cv_vd_preds_by_sent):\n",
    "    \n",
    "    diffs_train = get_different_predicted_tags_by_sent(essays_a=orig_td, essays_b=td)\n",
    "    diffs_test  = get_different_predicted_tags_by_sent(essays_a=orig_vd, essays_b=vd)\n",
    "\n",
    "    # Actual Ana CRels\n",
    "    diffs_crel_train = get_different_crel_tags_by_sent(essays_a=orig_td, essays_b=td)\n",
    "    diffs_crel_test  = get_different_crel_tags_by_sent(essays_a=orig_vd, essays_b=vd)\n",
    "    \n",
    "    build_metrics_by_code(td_preds_by_sent, diffs_train, diffs_crel_train, ys_by_code_train, preds_by_code_train)\n",
    "    build_metrics_by_code(vd_preds_by_sent, diffs_test,  diffs_crel_test,  ys_by_code_test,  preds_by_code_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  precision    recall\n",
       "95  0.999682    0.0625        0.5  0.033333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train2 = get_metrics_from_dict(ys_by_code_train, preds_by_code_train, cr_tags)\n",
    "df_train2[df_train2.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  precision  recall\n",
       "95  0.999794       0.0        0.0     0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2 = get_metrics_from_dict(ys_by_code_test, preds_by_code_test, cr_tags)\n",
    "df_test2[df_test2.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"precision\", \"recall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'3'}, {'50'}, {'1'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d for d in diffs_test if d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_vd_preds_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_different_predicted_tags_by_word(essays_a, essays_b):\n",
    "    assert len(essays_a) == len(essays_b)\n",
    "    diff_by_sent = defaultdict(list)\n",
    "    i = -1\n",
    "    for ea, eb in zip(essays_a, essays_b):\n",
    "        assert len(ea.sentences) == len(eb.sentences)\n",
    "        assert len(ea.pred_tagged_sentences) == len(eb.pred_tagged_sentences)\n",
    "        for asent, bsent in zip(ea.pred_tagged_sentences, eb.pred_tagged_sentences):\n",
    "            i += 1\n",
    "            assert type(asent) == type(bsent)\n",
    "            unique_atags = set(asent)\n",
    "            if EMPTY in unique_atags:\n",
    "                unique_atags.remove(EMPTY)\n",
    "            unique_btags = set(bsent)\n",
    "            if EMPTY in unique_btags:\n",
    "                unique_btags.remove(EMPTY)\n",
    "            diff_wd = unique_btags - unique_atags\n",
    "            if diff_wd:\n",
    "                diff_by_sent[i].append(diff_wd)\n",
    "    return diff_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_different_predicted_tags_by_word2(essays_a, essays_b):\n",
    "    assert len(essays_a) == len(essays_b)\n",
    "    diff_by_sent = defaultdict(list)\n",
    "    i = -1\n",
    "    for ea, eb in zip(essays_a, essays_b):\n",
    "        assert len(ea.sentences) == len(eb.sentences)\n",
    "        assert len(ea.pred_tagged_sentences) == len(eb.pred_tagged_sentences)\n",
    "        for asent, bsent in zip(ea.pred_tagged_sentences, eb.pred_tagged_sentences):\n",
    "            i += 1\n",
    "            assert type(asent) == type(bsent)        \n",
    "            assert len(asent) == len(bsent)        \n",
    "            for atag, btag in zip(asent, bsent):\n",
    "                if atag != btag:\n",
    "                    diff_by_sent[i].append(btag)\n",
    "    return diff_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_sents = get_different_predicted_tags_by_word(orig_pred_tagged_essays_test, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {545: [{'3'}], 657: [{'50'}], 1229: [{'1'}]})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_sents2 = get_different_predicted_tags_by_word2(orig_pred_tagged_essays_test, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {545: ['3'],\n",
       "             657: ['50'],\n",
       "             1110: ['14'],\n",
       "             1112: ['13'],\n",
       "             1229: ['1'],\n",
       "             1835: ['5']})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_sents2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for sent_ix, diff in diff_sents.items():\n",
    "    count += len(diff)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_by_sent = get_different_predicted_tags_by_sent(orig_pred_tagged_essays_test, pred_tagged_essays_test)\n",
    "len([d for d in diff_by_sent if len(d) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_crel_test  = get_different_crel_tags_by_sent(essays_a=orig_pred_tagged_essays_test, essays_b=pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for d in diffs_crel_test:\n",
    "    if d:\n",
    "        count += len(d)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
