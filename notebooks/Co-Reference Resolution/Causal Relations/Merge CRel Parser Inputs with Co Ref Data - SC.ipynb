{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes For SC\n",
    "- Change the parameters for anaphora resolution\n",
    "- Use Stanford parser\n",
    "- Update the files used for the taggged essays (different RNN params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "from window_based_tagger_config import get_config\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shared code from the results folder\n",
    "import sys\n",
    "sys.path.append(\"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Results\")\n",
    "\n",
    "from results_common import get_essays, validate_essays, tally_essay_attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "settings = Settings()\n",
    "\n",
    "DATASET = \"SkinCancer\"  # SkinCancer\n",
    "\n",
    "root_folder = settings.data_directory +  DATASET + \"/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "rnn_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "\n",
    "# Get Test Data In Order to Get Test CRELS\n",
    "# load the test essays to make sure we compute metrics over the test CR labels\n",
    "test_config = get_config(test_folder)\n",
    "\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "berkeley_coref_predictions_folder = root_folder + \"CoReference/Berkeley/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Optimal Parameters (from Hyper Parameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Berkeley best for CB\n",
    "coref_predictions_folder = stanford_coref_predictions_folder\n",
    "\n",
    "filter_to_predicted_tags = True\n",
    "\n",
    "nearest_ref_only = True\n",
    "pos_ana_key =     \"None\"\n",
    "pos_ch_key  =     \"None\"\n",
    "max_ana_phrase_len = None\n",
    "max_cref_phrase_len = 10\n",
    "\n",
    "coref_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/CRel'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder for the output (input to the CRel parser model)\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "output_folder = coref_root + \"CRel\"\n",
    "output_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-Ref folder: /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/\n",
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/training_processed.dill\n",
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/test_processed.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Co-Ref folder:\", coref_predictions_folder)\n",
    "\n",
    "coref_train_essays = get_essays(coref_predictions_folder, \"Training\")\n",
    "coref_test_essays = get_essays(coref_predictions_folder, \"Test\")\n",
    "\n",
    "len(coref_train_essays), len(coref_test_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/Bi-LSTM-4-SEARN/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = rnn_predictions_folder + \"essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = rnn_predictions_folder + \"essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "    \n",
    "len(pred_tagged_essays_train), len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "reg_tally = defaultdict(int)\n",
    "crel_tally = defaultdict(int)\n",
    "crel_ana_tally = defaultdict(int)\n",
    "for e in pred_tagged_essays_train + pred_tagged_essays_test:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                t_lower = t.lower()\n",
    "                if \"rhet\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "                    continue\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if \"->\" in t and (\"ana\" not in t_lower and \n",
    "                                  \"other\" not in t_lower and \n",
    "                                  \"rhet\" not in t_lower and \n",
    "                                  \"change\" not in t_lower):\n",
    "                    crel_tally[t] += 1\n",
    "                if \"->\" in t and ANAPHORA in t:\n",
    "                    crel_ana_tally[t] += 1\n",
    "                    \n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "crel_tags = sorted(crel_tally.keys())\n",
    "\n",
    "cc_crel_tags_filter = set(reg_tags + crel_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_the_same(essay_sets):\n",
    "    unique_fnames = [] # list of sets of str (fnames)\n",
    "    for essay_collection in essay_sets:\n",
    "        names = set()\n",
    "        for e in essay_collection:\n",
    "            names.add(e.name)\n",
    "        unique_fnames.append(names)\n",
    "    for a in unique_fnames:\n",
    "        print(len(a))\n",
    "        for b in unique_fnames:\n",
    "            assert len(a) == len(b), \"lens don't match\"\n",
    "            assert a == b, \"don't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essays_2_hash_map(essays):\n",
    "    lu = {}\n",
    "    for e in essays:\n",
    "        lu[e.name] = e\n",
    "    return lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks the number of words and sentences are the same for 2 sets of essays\n",
    "def validate_tagged_essays(essays_a, essays_b, tags_filter):\n",
    "    # make sure obj is not the same\n",
    "    assert essays_a != essays_b\n",
    "    print(\"Validating\", len(essays_a), \"essays\")\n",
    "    assert len(essays_a) == len(essays_b), \"Lens don't match\"\n",
    "    \n",
    "    a_hmap = essays_2_hash_map(essays_a)\n",
    "    b_hmap = essays_2_hash_map(essays_b)\n",
    "    \n",
    "    # same essays?\n",
    "    assert a_hmap.keys() == b_hmap.keys()\n",
    "    intersect = set(a_hmap.keys()).intersection(b_hmap.keys())\n",
    "    assert len(intersect) == len(a_hmap.keys())\n",
    "    assert len(a_hmap.keys()) > 1    \n",
    "    assert len(a_hmap.keys()) == len(b_hmap.keys())\n",
    "    \n",
    "    word_misses = 0\n",
    "    \n",
    "    for key, a_essay in a_hmap.items():\n",
    "        b_essay = b_hmap[key]\n",
    "        # assert NOT the same obj ref\n",
    "        assert a_essay != b_essay\n",
    "        assert len(a_essay.sentences) == len(b_essay.sentences)\n",
    "        assert len(a_essay.sentences) > 0\n",
    "        assert len(b_essay.sentences) > 0\n",
    "        for i in range(len(a_essay.sentences)):\n",
    "            a_sent = a_essay.sentences[i]\n",
    "            b_sent = b_essay.sentences[i]\n",
    "            # the same lists?\n",
    "            #assert a_sent == b_sent\n",
    "            assert len(a_sent) == len(b_sent)\n",
    "            if not len(a_sent) == len(b_sent):\n",
    "                print(key, \"\\tsent-ix:\", i, \"lens\", len(a_sent), len(b_sent))\n",
    "            for wd_ix, (a_wd, a_tags) in enumerate(a_sent):\n",
    "                b_wd, b_tags = b_sent[wd_ix]\n",
    "                if a_wd != b_wd:\n",
    "                    word_misses+=1\n",
    "                assert a_wd   == b_wd,   \\\n",
    "                    \"Words don't match: '{a}' - '{b}', Esssay: {essay} Sent Ix: {i}\".format(\n",
    "                            a=a_wd, b=b_wd, essay=key, i=i)\n",
    "                \n",
    "                # SH - Make conditional, as untagged essays contain new anaphora tags\n",
    "                filtered_a_tags = tags_filter.intersection(a_tags)\n",
    "                filtered_b_tags = tags_filter.intersection(b_tags)\n",
    "\n",
    "                assert filtered_a_tags == filtered_b_tags, \\\n",
    "                    \"Tags don't match: '{a}' - '{b}', Esssay: {essay} Sent Ix: {i}\".format(\n",
    "                        a=str(filtered_a_tags), b=str(filtered_b_tags), essay=key, i=i)                \n",
    "                        \n",
    "    if word_misses:\n",
    "        print(\"Word miss-matches: \", word_misses)\n",
    "    print(\"Validation Passed\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Non Anaphora Labels for Comparison (Should Match Across Essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "870\n"
     ]
    }
   ],
   "source": [
    "names_the_same([coref_train_essays, pred_tagged_essays_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "names_the_same([coref_test_essays, pred_tagged_essays_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 870 essays\n",
      "Validation Passed\n"
     ]
    }
   ],
   "source": [
    "validate_tagged_essays(essays_a=coref_train_essays, essays_b=pred_tagged_essays_train,\n",
    "                       tags_filter=cc_crel_tags_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 218 essays\n",
      "Validation Passed\n"
     ]
    }
   ],
   "source": [
    "validate_tagged_essays(essays_a=coref_test_essays, essays_b=pred_tagged_essays_test,\n",
    "                       tags_filter=cc_crel_tags_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Predictions from Crel Essays with CoRef Data from CoRef Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the coref essays (used for predictions), and copy over the prediction tags from the \n",
    "# crel essays. We do this as we also need the Anaphora labels from the CoRef data\n",
    "def combine_essays(crel_essays, coref_essays):\n",
    "    \n",
    "    crel_hmap = essays_2_hash_map(crel_essays)\n",
    "    coref_hmap = essays_2_hash_map(coref_essays)\n",
    "    \n",
    "    new_essays = []\n",
    "    for key, crel_essay in crel_hmap.items():\n",
    "        coref_essay = coref_hmap[key]\n",
    "        # clone from coref essay so we grab the anaphora labels (not present in those essays)\n",
    "        new_essay = coref_essay.clone()\n",
    "        \n",
    "        # copy coref data from coref essay\n",
    "        new_essay.ana_tagged_sentences    = coref_essay.ana_tagged_sentences\n",
    "        new_essay.pred_corefids           = coref_essay.pred_corefids\n",
    "        new_essay.pred_ner_tags_sentences = coref_essay.pred_ner_tags_sentences\n",
    "        new_essay.pred_pos_tags_sentences = coref_essay.pred_pos_tags_sentences\n",
    "        \n",
    "        # BUT copy predictions from the crel essay\n",
    "        new_essay.pred_tagged_sentences = crel_essay.pred_tagged_sentences\n",
    "        new_essays.append(new_essay)\n",
    "    return new_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_essays = combine_essays(crel_essays=pred_tagged_essays_train, coref_essays=coref_train_essays)\n",
    "merged_test_essays  = combine_essays(crel_essays=pred_tagged_essays_test,  coref_essays=coref_test_essays)\n",
    "len(merged_train_essays), len(merged_test_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Anaphora Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[6]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana_tally = defaultdict(int)\n",
    "all_merged_essays = merged_train_essays + merged_test_essays\n",
    "for e in all_merged_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:                \n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"Anaphor:[\" in t and \"rhetorical\" not in t and \"->\" not in t:\n",
    "                        ana_tally[t] += 1\n",
    "\n",
    "all_ana_tags = sorted(ana_tally.keys())\n",
    "assert len(all_ana_tags) == len(reg_tags), \"Number of anaphora tags doesn't match the number of regular tags\"\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Causer:1->Result:Anaphor': 394,\n",
       "             'Causer:1->Result:Anaphor[2]': 6,\n",
       "             'Causer:1->Result:Anaphor[3]': 18,\n",
       "             'Causer:1->Result:Anaphor[50]': 370,\n",
       "             'Causer:11->Result:Anaphor': 61,\n",
       "             'Causer:11->Result:Anaphor[12]': 3,\n",
       "             'Causer:11->Result:Anaphor[3]': 5,\n",
       "             'Causer:11->Result:Anaphor[50]': 53,\n",
       "             'Causer:12->Result:Anaphor': 44,\n",
       "             'Causer:12->Result:Anaphor[2]': 3,\n",
       "             'Causer:12->Result:Anaphor[3]': 15,\n",
       "             'Causer:12->Result:Anaphor[50]': 26,\n",
       "             'Causer:2->Result:Anaphor': 602,\n",
       "             'Causer:2->Result:Anaphor[1]': 8,\n",
       "             'Causer:2->Result:Anaphor[3]': 29,\n",
       "             'Causer:2->Result:Anaphor[4]': 36,\n",
       "             'Causer:2->Result:Anaphor[50]': 483,\n",
       "             'Causer:2->Result:Anaphor[5]': 43,\n",
       "             'Causer:3->Result:Anaphor': 321,\n",
       "             'Causer:3->Result:Anaphor[4]': 21,\n",
       "             'Causer:3->Result:Anaphor[50]': 297,\n",
       "             'Causer:3->Result:Anaphor[6]': 3,\n",
       "             'Causer:4->Result:Anaphor': 208,\n",
       "             'Causer:4->Result:Anaphor[50]': 121,\n",
       "             'Causer:4->Result:Anaphor[5]': 80,\n",
       "             'Causer:5->Result:Anaphor': 687,\n",
       "             'Causer:5->Result:Anaphor[50]': 659,\n",
       "             'Causer:5->Result:Anaphor[6]': 28,\n",
       "             'Causer:6->Result:Anaphor': 78,\n",
       "             'Causer:6->Result:Anaphor[50]': 67,\n",
       "             'Causer:6->Result:Anaphor[5]': 11,\n",
       "             'Causer:Anaphor->Result:1': 6,\n",
       "             'Causer:Anaphor->Result:11': 4,\n",
       "             'Causer:Anaphor->Result:12': 198,\n",
       "             'Causer:Anaphor->Result:2': 170,\n",
       "             'Causer:Anaphor->Result:3': 111,\n",
       "             'Causer:Anaphor->Result:4': 189,\n",
       "             'Causer:Anaphor->Result:5': 120,\n",
       "             'Causer:Anaphor->Result:50': 645,\n",
       "             'Causer:Anaphor->Result:6': 441,\n",
       "             'Causer:Anaphor->Result:Anaphor': 17,\n",
       "             'Causer:Anaphor->Result:Anaphor[50]': 7,\n",
       "             'Causer:Anaphor->Result:Anaphor[5]': 10,\n",
       "             'Causer:Anaphor[11]->Result:12': 192,\n",
       "             'Causer:Anaphor[11]->Result:50': 12,\n",
       "             'Causer:Anaphor[1]->Result:2': 170,\n",
       "             'Causer:Anaphor[1]->Result:3': 89,\n",
       "             'Causer:Anaphor[1]->Result:4': 14,\n",
       "             'Causer:Anaphor[1]->Result:50': 58,\n",
       "             'Causer:Anaphor[2]->Result:3': 22,\n",
       "             'Causer:Anaphor[2]->Result:4': 5,\n",
       "             'Causer:Anaphor[2]->Result:5': 58,\n",
       "             'Causer:Anaphor[2]->Result:50': 119,\n",
       "             'Causer:Anaphor[2]->Result:6': 8,\n",
       "             'Causer:Anaphor[3]->Result:12': 6,\n",
       "             'Causer:Anaphor[3]->Result:4': 136,\n",
       "             'Causer:Anaphor[3]->Result:5': 22,\n",
       "             'Causer:Anaphor[3]->Result:50': 98,\n",
       "             'Causer:Anaphor[3]->Result:6': 20,\n",
       "             'Causer:Anaphor[3]->Result:Anaphor': 10,\n",
       "             'Causer:Anaphor[4]->Result:5': 14,\n",
       "             'Causer:Anaphor[4]->Result:50': 44,\n",
       "             'Causer:Anaphor[4]->Result:6': 59,\n",
       "             'Causer:Anaphor[50]->Result:1': 6,\n",
       "             'Causer:Anaphor[50]->Result:5': 20,\n",
       "             'Causer:Anaphor[5]->Result:11': 4,\n",
       "             'Causer:Anaphor[5]->Result:4': 34,\n",
       "             'Causer:Anaphor[5]->Result:5': 6,\n",
       "             'Causer:Anaphor[5]->Result:50': 190,\n",
       "             'Causer:Anaphor[5]->Result:6': 354,\n",
       "             'Causer:Anaphor[5]->Result:Anaphor': 7,\n",
       "             'Causer:Anaphor[6]->Result:50': 132})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crel_ana_tally = defaultdict(int)\n",
    "\n",
    "for e in all_merged_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                t_lower = t.lower()\n",
    "                if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "                    continue\n",
    "                if \"->\" in t and ANAPHORA in t:\n",
    "                    crel_ana_tally[t] += 1\n",
    "crel_ana_tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tally = tally_essay_attributes(all_merged_essays, attribute_name=\"pred_pos_tags_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NN', 'NNP', 'NNPS', 'NNS'},\n",
       " {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'},\n",
       " {'PRP', 'PRP$', 'WP', 'WP$'},\n",
       " {'DT', 'PDT', 'WDT'},\n",
       " {'DT', 'PDT', 'PRP', 'PRP$', 'WDT', 'WP', 'WP$'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_nouns = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"NN\"])\n",
    "pos_verbs = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"VB\"])\n",
    "pos_pronouns = {\"PRP\",\"PRP$\", \"WP\", \"WP$\"}\n",
    "pos_determiners = {\"DT\",\"WDT\",\"PDT\"} # the, a, which, that, etc\n",
    "pos_pron_dt = pos_pronouns | pos_determiners\n",
    "# for meaning of pen treebank tags - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_nouns, pos_verbs, pos_pronouns, pos_determiners, pos_pron_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pos_filter = {\n",
    "            \"None\": None,\n",
    "            \"PRN\": pos_pronouns,\n",
    "            \"DT\": pos_determiners,\n",
    "            \"PRN+DT\": pos_pron_dt\n",
    "}\n",
    "\n",
    "dict_pos_ch_filter = {\n",
    "    \"None\": None,\n",
    "    \"NN\": pos_nouns,\n",
    "    \"VB\": pos_verbs,\n",
    "    \"NN+VB\": pos_nouns | pos_verbs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Well Would Ana Resolution work with the CRel Predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999652</td>\n",
       "      <td>1309239.0</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>473.0</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.067653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  data_points  f1_score  num_codes  precision    recall\n",
       "0  0.999652    1309239.0  0.123077      473.0   0.680851  0.067653"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from process_essays_coref import get_coref_processed_essays\n",
    "from metrics import get_metrics_raw\n",
    "\n",
    "format_ana_tags = True # use this as true to eval performance, but then change to False for the actual exercise\n",
    "filter_to_predicted_tags = True # filter based on the anaphora predictions from the other RNN\n",
    "\n",
    "pos_ana_filter = dict_pos_filter[pos_ana_key]\n",
    "pos_ch_filter  = dict_pos_ch_filter[pos_ch_key]\n",
    "    \n",
    "processed_train_essays_ana = get_coref_processed_essays(\n",
    "                            essays=merged_train_essays, \n",
    "                            format_ana_tags=format_ana_tags, \n",
    "                            ner_ch_filter=None, look_back_only=True,\n",
    "                            filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                            max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                            pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter, \n",
    "                            nearest_ref_only=nearest_ref_only)\n",
    "\n",
    "metrics = get_metrics_raw(processed_train_essays_ana, expected_tags=all_ana_tags,  micro_only=True)    \n",
    "pd.DataFrame([metrics[\"MICRO_F1\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Final Set of Essays\n",
    "- Add in new predicted Anaphora tags as additional regular codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_ana_tags = False # Set to false so Anaphora codes are merged in with the regular codes\n",
    "processed_train_essays_full = get_coref_processed_essays(\n",
    "                            essays=merged_train_essays, \n",
    "                            format_ana_tags=format_ana_tags, \n",
    "                            ner_ch_filter=None, look_back_only=True,\n",
    "                            filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                            max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                            pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter, \n",
    "                            nearest_ref_only=nearest_ref_only)\n",
    "\n",
    "processed_test_essays_full = get_coref_processed_essays(\n",
    "                            essays=merged_test_essays, \n",
    "                            format_ana_tags=format_ana_tags, \n",
    "                            ner_ch_filter=None, look_back_only=True,\n",
    "                            filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                            max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                            pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter, \n",
    "                            nearest_ref_only=nearest_ref_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate there are Differences in the New Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() {'50'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'7'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'3'} Anaphor\n",
      "set() {'3'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'6'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'7'} Anaphor\n",
      "set() {'50'} Anaphor\n",
      "set() {'11'} Anaphor\n",
      "set() {'14'} Anaphor\n",
      "set() {'1'} Anaphor\n",
      "set() {'13'} Anaphor\n",
      "set() {'1'} Anaphor\n"
     ]
    }
   ],
   "source": [
    "EMPTY = 'Empty'\n",
    "for a,b in zip(merged_train_essays, processed_train_essays_full):\n",
    "    assert len(a.sentences)  == len(b.sentences)\n",
    "    assert a.name == b.name\n",
    "    \n",
    "    assert len(a.pred_tagged_sentences) == len(b.pred_tagged_sentences)\n",
    "    \n",
    "    sent_ix =- 1\n",
    "    for atag_sent, btag_sent in zip(a.pred_tagged_sentences, b.pred_tagged_sentences):        \n",
    "        sent_ix += 1\n",
    "        word_ix = -1\n",
    "        for atags, btags in zip(atag_sent, btag_sent):\n",
    "            word_ix+=1\n",
    "            atags = set([atags])\n",
    "            if EMPTY in atags:\n",
    "                atags.remove(EMPTY)\n",
    "            if atags != btags:\n",
    "                print(atags, btags, b.ana_tagged_sentences[sent_ix][word_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in Additional Crel Codes (from Anaphora Codes mapped to Regular Crel Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Causer:11->Result:1',\n",
       " 'Causer:14->Result:11',\n",
       " 'Causer:2->Result:4',\n",
       " 'Causer:2->Result:7',\n",
       " 'Causer:7->Result:6'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "def get_anaphora_crel_codes(tags):\n",
    "    \n",
    "    additional_codes = set()\n",
    "    prefix = \"Anaphor[\"\n",
    "    \n",
    "    for k in tags:        \n",
    "        if prefix in k: # has an Anaphor tag with an indentified code\n",
    "            k_lower = k.lower()\n",
    "            if \"rhetorical\" in k_lower or \"other\" in k_lower or \"change\" in k_lower:\n",
    "                continue\n",
    "            k_fixed = k.replace(prefix, \"\").replace(\"]\",\"\")\n",
    "            if ANAPHORA not in k_fixed:\n",
    "                l,r = k_fixed.split(\"->\")\n",
    "                l_code = l.replace(\"Causer:\",\"\")\n",
    "                r_code = r.replace(\"Result:\",\"\")\n",
    "                assert is_a_regular_code(l_code), l_code\n",
    "                assert is_a_regular_code(r_code), r_code\n",
    "                additional_codes.add(k_fixed)\n",
    "    return additional_codes\n",
    "\n",
    "add_crel_codes = get_anaphora_crel_codes(crel_ana_tally.keys())\n",
    "# did we add any new unique tags?\n",
    "add_crel_codes - set(crel_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_crel_ana_codes_as_regular_relations(essays):\n",
    "    new_essays = []\n",
    "    for e in essays:\n",
    "        new_essay = e.clone()\n",
    "        new_essays.append(new_essay)\n",
    "        \n",
    "        new_essay.ana_tagged_sentences    = e.ana_tagged_sentences\n",
    "        new_essay.pred_corefids           = e.pred_corefids\n",
    "        new_essay.pred_ner_tags_sentences = e.pred_ner_tags_sentences\n",
    "        new_essay.pred_pos_tags_sentences = e.pred_pos_tags_sentences\n",
    "        new_essay.pred_tagged_sentences   = e.pred_tagged_sentences\n",
    "        \n",
    "        new_essay.sentences = []\n",
    "        for sent in e.sentences:\n",
    "            new_sent = []\n",
    "            new_essay.sentences.append(new_sent)\n",
    "            for wd, tags in sent:\n",
    "                new_tags = set(tags)\n",
    "                addnl_crel_tags = get_anaphora_crel_codes(tags)\n",
    "                if len(addnl_crel_tags) > 0:\n",
    "                    new_tags.update(addnl_crel_tags)\n",
    "                new_sent.append((wd, new_tags))\n",
    "            assert len(new_sent) == len(sent)\n",
    "        assert len(new_essay.sentences) == len(e.sentences)\n",
    "    return new_essays      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_essays_full_with_codes = add_crel_ana_codes_as_regular_relations(processed_train_essays_full)\n",
    "processed_test_essays_full_with_codes  = add_crel_ana_codes_as_regular_relations(processed_test_essays_full)\n",
    "\n",
    "len(processed_train_essays_full_with_codes), len(processed_test_essays_full_with_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the new essays are different - this should blow up (it does)\n",
    "# validate_tagged_essays(processed_train_essays_full, processed_train_essays_full_with_codes, set(crel_tags))\n",
    "# validate_tagged_essays(processed_test_essays_full, processed_test_essays_full_with_codes, set(crel_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-SEARN/'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_predictions_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "p = pathlib.Path(output_folder)\n",
    "p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "fname = output_folder + \"/training_crel_anatagged_essays.dill\"\n",
    "with open(fname, \"wb+\") as f:\n",
    "    dill.dump(processed_train_essays_full_with_codes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "fname = output_folder + \"/test_crel_anatagged_essays.dill\"\n",
    "with open(fname, \"wb+\") as f:\n",
    "    dill.dump(processed_test_essays_full_with_codes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
