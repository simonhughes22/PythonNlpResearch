{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Status and Work Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Approach and Thoughts\n",
    "* My initial approach focused on training a fast word based tagging model. A full set of feature extraction and hyper parameter tuning was conducted\n",
    "  * However the results were un satisfactory (see the non RNN  so I switched to an RNN model)\n",
    "  * Also note that the mongo collections have been renamed to differentiate this fix - metrics_coref_word_tagger are the original word-based tagging model work, which are valid\n",
    "* Switching to the RNN model, I made some initital mistakes:\n",
    " 1. I didn't have it choose the Anaphora tag but instead chose the most common tag (which isn't always the anaphora tag)\n",
    " 2. There seems to be some issues with the way the initial RNN's trained, e.g. compare the numbers under the Anaphora.data_points. This is correct in the word tagging model and the later RNN tagger (see \"_fixed\") but not in the initial RNN tagger work\n",
    "* To remedy this I switched to newer code, copied from different notebooks, and the results of that are seen under the \"coref_new_fixed\" collection\n",
    "* I also moved to having the models spit out tagged data points, and then re-computing the metrics directly from that data. This then allows us to interrogate those predictions at a later date as needed, not just the raw numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tagger Work (Notes)\n",
    "- Initially I trained a word tagging model as it's much faster, and had similar accuracy to the RNN (but not quite as good). This allowed faster iteration, however the results weren't great.\n",
    "- I did this in two phases:\n",
    " 1. Did initial feat sel and hyper parameter tuning to determine the optimal feats (win size, etc) and parameters for training a word tagging model to tag anaphora tags\n",
    "   - mongo - metrics_coref_word_tagger\n",
    "   - py scripts\n",
    "     - windowbasedtagger_most_common_tag_multiclass_feat_seln.py\n",
    "     - windowbasedtagger_most_common_tag_multiclass_hyper_param_tuning.py\n",
    " 2. Using results from 1, I then did a sort of feat selection on which co-ref tags to use to replace the concept codes\n",
    "   - **<span style='color:red'>I think this is invalid as BrattEssay was not updated yet - re-use code though?</span>**\n",
    "   - mongo - metrics_coref_word_tagger_coref_feats\n",
    "   - py script - windowbasedtagger_most_common_tag_multiclass_hyper_param_tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Tagger Work (Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initially made some mistakes (invalid logic - had wrong number of data points), and trained multi class RNN on most common tag\n",
    " - Stored in mongo collection called **metrics_coref_broken**, now deleted (see below)\n",
    "- Then i figured out issues with the predicted tagged Concept Codes not being from the best model, and corrected via logic in \"CB - CC Tagger MULTICLASS - Train Save CV Word Predictions - NO EXPLICIT.ipynb\"\n",
    "- With the fixed anaphora training logic, the data is now stored in **metrics_coref_rnn**\n",
    " - This contains two types of collection based on two steps of work:\n",
    "   1. Ran the initial RNN tagger model without hyper parameter tuning, stored predictions for calculating metrics directly from \n",
    "     - NB \"CB - Anaphora Tagger BINARY - FIXED - Train Save CV Word Predictions -NO EXPLICIT.ipynb\"\n",
    "     - Mongo - metrics_coref_rnn.CB_TAGGING_TD_RNN_BINARY_FIXED and similar\n",
    "   2. Decided I needed to do hyper parameter tuning on the RNN model:\n",
    "     - NB - \"CB - Anaphora Tagger BINARY - FIXED - Hyper Parameter Tuning.ipynb\"\n",
    "     - Mongo - metrics_coref_rnn.CB_TAGGING_TD_RNN_BINARY_HYPERPARAM_TUNING and similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo Collection Naming / History \n",
    "\n",
    "### RNN Work\n",
    "-  9/15/2018 Deleted - metrics_coref_broken\n",
    "  - Initial RNN tagger work with non-optimal model / broken code (num data points incorrect, did multi-class) \n",
    "- 9/15/2018 - Consolidated - metrics_coref_new_fixed and metrics_coref_rnn_fixed into metrics_coref_rnn\n",
    " - within this, the CB_TAGGING_TD_RNN_BINARY_FIXED and similar colls reflect the initial output from the CB - Anaphora Tagger BINARY - FIXED - Train Save CV Word Predictions -NO EXPLICIT.ipynb notebook (does not hyper param tune, does dump prediction files)\n",
    "  - I then decided to do hyper parameter tuning (but not to persist preds to disk...) - coll named CB_TAGGING_TD_RNN_BINARY_HYPERPARAM_TUNING and similar, refers to work done under CB - Anaphora Tagger BINARY - FIXED - Hyper Parameter Tuning.ipynb\n",
    "   \n",
    "   \n",
    "## Word Tagging Work\n",
    "- metrics coref_new renamed to metrics_coref_old\n",
    "- metrics_coref_old renamed (was originally metrics_coref_new) to be the metrics_coref_word_tagger_coref_feats mongo coll to better reflect what it's used for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Bratt Essay Parsing Logic to Resolve Anaphora Tags\n",
    "* Next I adjusted the BrattEssay file to resolve the anaphora tags with their antecedents when provided\n",
    "* These get resolved as Anaphora:[{code}] where {code} is one of the 13 or 9 concept codes, e.g. Anaphora:[50]\n",
    "* Analysis of how anaphora tags are initially tagged -  see 'Examine at How Anaphora Tags are Tagged to Inform Essay Parser Changes' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* Hyper parameter tune the fixed, binary RNNs\n",
    " - Done for CB, but no results in mongo for SC (do i need to sync to old mac?)\n",
    "* Validate the bratt parser logic\n",
    "    * See \"Test Bratt Essay Changes to include Anaphora Tags\"\n",
    "* Load the predictions and reconcile with the co-ref parser output and CC tag predictions\n",
    "  - Initial work in \n",
    "   - \"Match Predicted Anaphora Tags to CoRef Output.ipynb\"\n",
    "   - and script \"MatchCoRefTagsToEssays_new.py\"\n",
    "* Evaluate 2 things:\n",
    "    1. Taking the predicted tags, look for intersections in co-ref output, and evaluate accuracy of the resolved anaphora concept codes\n",
    "    2. Use the stanford co-reference parser alone to implement this logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "- Are the Co-references only present for concept codes? \n",
    " - No (and see ans to next qu)\n",
    " - Also, some anaphora tags have no antecedent\n",
    "- Are they only present for concept codes that form part of a causal relation?\n",
    " - No, not all anaphora tags have causal relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Measure of Success - Considerations\n",
    "  * Accuracy at detecting anaphora tags\n",
    "  * Accuracy at detecting anaphora tags and correctly resolving the associated concept(s)\n",
    "      1. Using the ML model's predictions to filter\n",
    "      2. Using the stanford output alone\n",
    "  * Impact on other metrics when incorporated into a single solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
