{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Status and Work Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Approach and Thoughts\n",
    "* My initial approach focused on training a fast word based tagging model. A full set of feature extraction and hyper parameter tuning was conducted\n",
    "  * However the results were un satisfactory (see the non RNN  so I switched to an RNN model)\n",
    "  * Also note that the mongo collections have been renamed to differentiate this fix - metrics_coref_word_tagger are the original word-based tagging model work, which are valid\n",
    "* Switching to the RNN model, I made some initital mistakes:\n",
    " 1. I didn't have it choose the Anaphora tag but instead chose the most common tag (which isn't always the anaphora tag)\n",
    " 2. There seems to be some issues with the way the initial RNN's trained, e.g. compare the numbers under the Anaphora.data_points. This is correct in the word tagging model and the later RNN tagger (see \"_fixed\") but not in the initial RNN tagger work\n",
    "* To remedy this I switched to newer code, copied from different notebooks, and the results of that are seen under the \"coref_new_fixed\" collection\n",
    "* I also moved to having the models spit out tagged data points, and then re-computing the metrics directly from that data. This then allows us to interrogate those predictions at a later date as needed, not just the raw numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tagger Work (Notes)\n",
    "- Initially I trained a word tagging model as it's much faster, and had similar accuracy to the RNN (but not quite as good). This allowed faster iteration, however the results weren't great.\n",
    "- I did this in two phases:\n",
    " 1. Did initial feat sel and hyper parameter tuning to determine the optimal feats (win size, etc) and parameters for training a word tagging model to tag anaphora tags\n",
    "   - mongo - metrics_coref_word_tagger\n",
    "   - py scripts\n",
    "     - windowbasedtagger_most_common_tag_multiclass_feat_seln.py\n",
    "     - windowbasedtagger_most_common_tag_multiclass_hyper_param_tuning.py\n",
    " 2. Using results from 1, I then did a sort of feat selection on which co-ref tags to use to replace the concept codes\n",
    "   - **<span style='color:red'>I think this is invalid as BrattEssay was not updated yet - re-use code though?</span>**\n",
    "   - mongo - metrics_coref_word_tagger_coref_feats\n",
    "   - py script - windowbasedtagger_most_common_tag_multiclass_hyper_param_tuning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN CC Tagger Work (Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Noticed that the CC tag predictions were not based on those from the optimal model\n",
    " - See \"CC Tagger Multiclass...\" Notebooks - logic re-ran to fix this\n",
    "- **NOTE:** - These predictions are now stored in **metrics_codes** mongo coll with the prefix **'STORE\\_ RESULTS\\_'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Ana Tagger Work (Notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initially made some mistakes (invalid logic - had wrong number of data points), and trained multi class RNN on most common tag\n",
    " - Stored in mongo collection called **metrics_coref_broken**, now deleted (see below)\n",
    "- Then i figured out issues with the predicted tagged Concept Codes not being from the best model, and corrected via logic in \"CB - CC Tagger MULTICLASS - Train Save CV Word Predictions - NO EXPLICIT.ipynb\"\n",
    "- With the fixed anaphora training logic, \n",
    " - the data is now stored in **metrics_coref_rnn** \n",
    " - This contains two types of collection based on two steps of work:\n",
    "   1. Ran the initial RNN tagger model without hyper parameter tuning, stored predictions for calculating metrics directly from \n",
    "     - NB \"CB - Anaphora Tagger BINARY - FIXED - Train Save CV Word Predictions -NO EXPLICIT.ipynb\"\n",
    "     - Mongo - metrics_coref_rnn.CB_TAGGING_TD_RNN_BINARY_FIXED and similar\n",
    "     - Predictions - stored in Bi-LSTM-4-Anaphora_Tags-Binary-Fixed folder - metrics match mongo 100%\n",
    "   2. Decided I needed to do hyper parameter tuning on the RNN model:\n",
    "     - NB - \"CB - Anaphora Tagger BINARY - FIXED - Hyper Parameter Tuning.ipynb\"\n",
    "     - Mongo - metrics_coref_rnn.CB_TAGGING_TD_RNN_BINARY_HYPERPARAM_TUNING and similar\n",
    "  - The predictions are stored in \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/\"\n",
    "    - The predictions in \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/\" are worse and represent the broken predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo Collection Naming / History \n",
    "\n",
    "### RNN Work\n",
    "-  9/15/2018 Deleted - metrics_coref_broken\n",
    "  - Initial RNN tagger work with non-optimal model / broken code (num data points incorrect, did multi-class) \n",
    "- 9/15/2018 - Consolidated - metrics_coref_new_fixed and metrics_coref_rnn_fixed into metrics_coref_rnn\n",
    " - within this, the CB_TAGGING_TD_RNN_BINARY_FIXED and similar colls reflect the initial output from the CB - Anaphora Tagger BINARY - FIXED - Train Save CV Word Predictions -NO EXPLICIT.ipynb notebook (does not hyper param tune, does dump prediction files)\n",
    "  - I then decided to do hyper parameter tuning (but not to persist preds to disk...) - coll named CB_TAGGING_TD_RNN_BINARY_HYPERPARAM_TUNING and similar, refers to work done under CB - Anaphora Tagger BINARY - FIXED - Hyper Parameter Tuning.ipynb\n",
    "   \n",
    "   \n",
    "## Word Tagging Work\n",
    "- metrics coref_new renamed to metrics_coref_old\n",
    "- metrics_coref_old renamed (was originally metrics_coref_new) to be the metrics_coref_word_tagger_coref_feats mongo coll to better reflect what it's used for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Bratt Essay Parsing Logic to Resolve Anaphora Tags\n",
    "* Next I adjusted the BrattEssay file to resolve the anaphora tags with their antecedents when provided\n",
    "* These get resolved as Anaphora:[{code}] where {code} is one of the 13 or 9 concept codes, e.g. Anaphora:[50]\n",
    "* Analysis of how anaphora tags are initially tagged -  see 'Examine at How Anaphora Tags are Tagged to Inform Essay Parser Changes' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code to do this was re-written in order to persist the predictions to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">For some reason the Skin Cancer Metrics in Mongo do not match those coming from the database. CB matches OK</span>\n",
    "- Subsequently I decided to re-run the SC train and test runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging CoRef Files with Annotated Essays\n",
    "\n",
    "### Notes on CoRef Datastructure\n",
    "- Dictionary of esssays, keyed by name\n",
    "- Each essay is a list of sentences\n",
    "- Each sentence is a list of words\n",
    "- Words are mapped to a tag dict\n",
    "  - tag dict - contains\n",
    "    - NER tag (most are O - none)\n",
    "    - POS tag\n",
    "    - If a Co-Reference such as an anaphor (mostly pronouns)\n",
    "      - COREF_PHRASE - phrase referred to by coref\n",
    "      - COREF_REF - Id of referenced phrase\n",
    "    - else if it is a phrase that is referenced:\n",
    "      - COREF_ID - id of the co-reference, referenced in the COREF_REF tag\n",
    "      \n",
    "### Notes of CoRef Output from Stanford\n",
    "- Co-references can be in either order - the canonical reference can be **before** or **after** the mention, so it's really just a grouping of phrases that mean the same thing.\n",
    " - e.g. essay EBA1415_SEAL_34_CB_ES-04796 in '/Users/simon.hughes/Google Drive/PhD/Data/CoralBleaching/Thesis_Dataset/CoReference/Training'\n",
    " - Mention COREF_REF = 5 comes before the coreference COREF_ID = 5, whereas for COREF_ID = 4, the coreference (id) comes before the mention  (coref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* ~~Hyper parameter tune the fixed, binary RNNs~~\n",
    " * ~~Validate the bratt parser logic~~\n",
    "   * ~~See \"Test Bratt Essay Changes to include Anaphora Tags\"~~\n",
    " * ~~Match the cc tags with the anaphora tags~~\n",
    "   * ~~See CB/SC - Load AND Eval CC and Ana Tagged Essays - Then MERGE Essays.ipynb~~\n",
    "* Load the predictions and reconcile with the co-ref parser output and CC tag predictions\n",
    "  - ~~I found an issue with the way I am processing the parser output - I am using a dictionary but there can be more than one coreference tag per word, so the last one is overwriting the others when multiple are present~~\n",
    "  - ~~Switched to the neural parser for the CoRefs (as was unsure of whether it is the default one)~~\n",
    "  - Initial work in \n",
    "   - \"Match Predicted Anaphora Tags to CoRef Output.ipynb\"\n",
    "   - and script \"MatchCoRefTagsToEssays_old.py\"\n",
    "   - and script \"MatchCoRefTagsToEssays_new.py\"\n",
    "   - See also - \"CB - Load AND Evaluate CC and Anaphora Tagged Essays\" for logic that merges the different predictions (aside from the coref tagged predictions)\n",
    "* Evaluate 2 things:\n",
    "    1. Taking the predicted tags, look for intersections in co-ref output, and evaluate accuracy of the resolved anaphora concept codes\n",
    "    2. Use the stanford co-reference parser alone to implement this logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">For Next Week</span>\n",
    "- Test the coref matching logic that i built end of day Sunday\n",
    "- Match the POS and NER codes up also so I can play with filters on those ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "- Are the Co-references only present for concept codes? \n",
    " - No (and see ans to next qu)\n",
    " - Also, some anaphora tags have no antecedent\n",
    "- Are they only present for concept codes that form part of a causal relation?\n",
    " - No, not all anaphora tags have causal relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Measure of Success - Considerations\n",
    "  * Accuracy at detecting anaphora tags\n",
    "  * Accuracy at detecting anaphora tags and correctly resolving the associated concept(s)\n",
    "      1. Using the ML model's predictions to filter\n",
    "      2. Using the stanford output alone\n",
    "  * Impact on other metrics when incorporated into a single solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining Tasks / Project Plan - (Chapter 6)\n",
    "- Match co-ref tags to anaphora tags\n",
    "  - Merge the two essay sets\n",
    "  - Cross-reference anaphora tags\n",
    "  - TARGET **1 day** - Sep/30\n",
    "- Compute Word Tagging Accuracy Metrics\n",
    " - Anaphora cross-referenced labels (using predicted anaphora tags)\n",
    " - **1/2 day** - Oct/7\n",
    "- Compute Caual Relation Tagging Accuracy\n",
    " - **1/2 day** - Oct/7\n",
    "- Compute Accuracy of using co-reference detection directly\n",
    " - **Unsure - can we use existing work?**\n",
    " - **~1-2 days** - high risk\n",
    " - Oct/14 ?\n",
    "- **BREAK** In Galena weeked of Oct 20/21\n",
    " - Should I take extra Monday off the following week (before folks arrive?)\n",
    "- Write up results\n",
    " - **2-3 days** initial\n",
    " - Oct/28, Nov 2, Nov 9\n",
    " - **2 days** with revisions\n",
    " - End of Nov\n",
    "- TARGET DATE - **End of Nov**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
