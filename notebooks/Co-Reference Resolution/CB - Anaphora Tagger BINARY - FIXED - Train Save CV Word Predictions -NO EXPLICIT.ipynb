{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py\n",
    "and also on this code: https://github.com/simonhughes22/PythonNlpResearch/blob/master/notebooks/SEARN/CB%20-%20Keras%20-%20Train%20Tagger%20and%20Save%20CV%20Predictions%20For%20Word%20Tags-NO%20EXPLICIT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "* The reason this nbook exists is that the tags used in the SEARN model and thus also the CoRef model do not seem to have been generated by the best model (they are well below the best reported results)\n",
    "* I am attempting to rectify this here, and if it works, can potentially re-run SEARN experiments also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Change to use 128 hidden units not 256 (was actually optimal settings)\n",
    "* Then Validate accuracy\n",
    "* Is that doesn't work, drop max len and ignore test data for the purposes of the CV run (on training data), then re-institute for training test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check mongo is running\n",
    "def is_mongo_runnning():\n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(serverSelectionTimeoutMS=100)\n",
    "    db = client.metrics_codes\n",
    "    coll = db.get_collection(\"CB_TAGGING_TD_AVG_PERCEPTRON_MOST_COMMON_TAG\")\n",
    "    l = list(coll.find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_mongo_runnning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - To Get this working:\n",
    "\n",
    "* Install CUDA and associated libraries, setup path\n",
    "* Install bleeding edge theano (from src)\n",
    "* Make sure the THEANO_FLAGS are set correctly via the environment var, or via the ~/.theanorc file\n",
    "* Install and compile bleeding edge Keras (from src)\n",
    "* `export KERAS_BACKEND=theano`\n",
    "* `export KERAS_IMAGE_DIM_ORDERING='th'`\n",
    "* `sh <project_root>/shell_scipts/setup_environment.sh` to install additional dependencies\n",
    "* **DO NOT SET UNROLL=True** when creating RNN's - causes max recursion issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "* You may need to clean the theano cache. To do so thoroughly, run this command from the shell:\n",
    " * `theano-cache purge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: CUDA driver version is insufficient for CUDA runtime version)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "#from keras.layers import TimeDistributed\n",
    "from keras.layers import TimeDistributedDense\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from DirUtils import dir_exists\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "DATASET = \"CoralBleaching\"\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "training_pickled = settings.data_directory + DATASET + \"/Thesis_Dataset/training.pl\"\n",
    "\n",
    "models_folder = root_folder + \"Models/Bi-LSTM-4-Anaphora-Binary-Fixed/\"\n",
    "predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/\"\n",
    "cv_folder = root_folder + \"CV_Data_Pickled_Anaphora_BINARY-Fixed/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor(dbname=\"metrics_coref_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Folders are Valid and Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Models/Bi-LSTM-4-Anaphora-Binary-Fixed/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CV_Data_Pickled_Anaphora_BINARY-Fixed/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dir_if_missing(folder):\n",
    "    if not dir_exists(folder):\n",
    "        print(\"Dir missing, creating\")\n",
    "        os.makedirs(folder)\n",
    "    assert dir_exists(folder)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_dir_if_missing(predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_dir_if_missing(models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_dir_if_missing(cv_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Essays"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = get_config(training_folder)\n",
    "tagged_essays_tmp = load_process_essays(**config)\n",
    "\n",
    "with open(training_pickled, \"wb+\") as f:\n",
    "    pickle.dump(tagged_essays_tmp, f)\n",
    "del tagged_essays_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 files found\n",
      "226 essays processed\n"
     ]
    }
   ],
   "source": [
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2018-09-16 20:59:03.188914\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "# for essay in tagged_essays_test:\n",
    "#     for sentence in essay.sentences:\n",
    "#         for word, tags in sentence:\n",
    "#             unique_words.add(word)\n",
    "#             for tag in tags:\n",
    "#                 tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if t == \"Anaphor\"))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "assert \"Anaphor\" in regular_tags\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(regular_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor', 'Empty']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Essays into Training Data (Word Ids)\n",
    "\n",
    "* Computes `xs`, `ys`, `ys_bytag` and `seq_lens`\n",
    "* `ys_bytag` includes **all tags** and does **not** focus only on the most common tag\n",
    "* `ys` only includes the most common tag (so we can use cross entropy)\n",
    "* `seq_lens` is without the start and end tags included (so we have to map back and forth to maintain mappings)\n",
    "* `ys_bytag` also excludes the START and END tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Sequence Length, Generate All Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix2tag = {}\n",
    "for ix, t in enumerate(vtags):\n",
    "    ix2tag[ix] = t\n",
    "\n",
    "generator = idGen(seed=1)  # important as we zero pad sequences\n",
    "\n",
    "maxlen = 0\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word)  # starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "\n",
    "def ids2tags(ids):\n",
    "    return [generator.get_key(j) for j in ids]\n",
    "\n",
    "def lbls2tags(ixs):\n",
    "    return [ix2tag[ix] for ix in ixs]\n",
    "\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays, max_seq_len=None):\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = maxlen\n",
    "    \n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_seq = []\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word)  # starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)\n",
    "\n",
    "                # Make sure to include Causer:<num> and Result:<num> tags for the Anaphora labels\n",
    "                tags = set([t.replace(\"Causer:\",\"\").replace(\"Result:\",\"\") for t in tags])\n",
    "               \n",
    "                # remove unwanted tags\n",
    "                tags = vtags.intersection(tags)\n",
    "                # retain all tags for evaluation (not just most common)\n",
    "                # SKIP the START and END tags\n",
    "                if word != START and word != END:\n",
    "                    for t in (vtags - set([EMPTY_TAG])):\n",
    "                        if t in tags:\n",
    "                            ys_bytag[t].append(1)\n",
    "                        else:\n",
    "                            ys_bytag[t].append(0)\n",
    "\n",
    "                # encode ys with most common tag only\n",
    "                if \"explicit\" in tags:\n",
    "                    tags.remove(\"explicit\")\n",
    "\n",
    "                if len(tags) > 1:\n",
    "                    most_common = max(tags, key=lambda t: tag_freq[t])\n",
    "                    tags = set([most_common])\n",
    "                if len(tags) == 0:\n",
    "                    tags.add(EMPTY_TAG)\n",
    "                    \n",
    "                assert len(tags) == 1, \"Wrong number of tags:\" + str(tags)\n",
    "\n",
    "                one_hot = []\n",
    "                for t in vtags:\n",
    "                    if t in tags:\n",
    "                        one_hot.append(1)\n",
    "                    else:\n",
    "                        one_hot.append(0)\n",
    "                y_seq.append(one_hot)\n",
    "\n",
    "            seq_lens.append(len(row) - 2)\n",
    "            ys.append(y_seq)\n",
    "            xs.append(row)\n",
    "\n",
    "    xs = sequence.pad_sequences(xs, maxlen=max_seq_len)\n",
    "    ys = sequence.pad_sequences(ys, maxlen=max_seq_len)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert xs.shape[1] == ys.shape[1] == max_seq_len, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Create Train - Test Split\n",
    "# Helper Functions\n",
    "def collapse_results(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i, :]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        for pred_tag in pred_ys:\n",
    "            pred_ys_by_tag[pred_tag].append(1)\n",
    "            # for all other tags, a 0\n",
    "            for tag in (vtags - set([EMPTY_TAG, pred_tag])):\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag\n",
    "\n",
    "def train_dev_split(lst, dev_split):\n",
    "    # random shuffle\n",
    "    shuffle(lst)\n",
    "    num_training = int((1.0 - dev_split) * len(lst))\n",
    "    return lst[:num_training], lst[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coral', {'50'}),\n",
       " ('bleaching', {'50'}),\n",
       " ('is', {'50'}),\n",
       " ('when', {'50'}),\n",
       " ('coral', {'50'}),\n",
       " ('turn', {'50'}),\n",
       " ('white', {'50'}),\n",
       " ('.', set())]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tagged_essays[0]\n",
    "e.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.39 s, sys: 54.7 ms, total: 4.45 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this name for a different function later\n",
    "from CrossValidation import cross_validation as cv\n",
    "\n",
    "folds = cv(tagged_essays, CV_FOLDS)\n",
    "fold2training_data = {}\n",
    "fold2dev_data = {}\n",
    "fold2test_data = {}\n",
    "\n",
    "fold2training_essays = {}\n",
    "fold2dev_essays = {}\n",
    "fold2test_essays = {}\n",
    "\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    # further split into train and dev test\n",
    "    essays_train, essays_dev = train_dev_split(essays_TD, DEV_SPLIT)\n",
    "    fold2training_data[i] = get_training_data(essays_train)\n",
    "    fold2dev_data[i]      = get_training_data(essays_dev)\n",
    "    # Test Data\n",
    "    fold2test_data[i]     = get_training_data(essays_VD)\n",
    "    \n",
    "    # also store essays\n",
    "    fold2training_essays[i] = essays_train\n",
    "    fold2dev_essays[i]      = essays_dev\n",
    "    fold2test_essays[i]     = essays_VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure the generator is incremented on the test data too\n",
    "_,_,_,_ = get_training_data(tagged_essays)\n",
    "#_,_,_,_ = get_training_data(tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(cv_folder + \"td.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2training_data, f)\n",
    "\n",
    "with open(cv_folder + \"td_essays.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2training_essays, f)\n",
    "\n",
    "with open(cv_folder + \"devd.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2dev_data, f)\n",
    "\n",
    "with open(cv_folder + \"devd_essays.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2dev_essays, f)\n",
    "    \n",
    "with open(cv_folder + \"vd.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2test_data, f)\n",
    "    \n",
    "with open(cv_folder + \"vd_essays.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2test_essays, f)\n",
    "    \n",
    "with open(cv_folder + \"generator.dill\", \"wb\") as f:\n",
    "    dill.dump(generator, f)\n",
    "\n",
    "with open(cv_folder + \"vtags.dill\", \"wb\") as f:\n",
    "    dill.dump(vtags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'coral')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.get_id(\"coral\"), generator.get_key(generator.get_id(\"coral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove 100 Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/DeepLearning/WordVectors/pickle_glove_embedding.py\n",
    "# for creating pre-filtered embeddings file\n",
    "import pickle, os\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings_file = \"/Users/simon.hughes/data/word_embeddings/glove.6B/cb_dict_glove.6B.100d.txt\"\n",
    "# read data file\n",
    "with open(embeddings_file, \"rb+\") as f:\n",
    "    cb_emb_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1641 2.5 %\n"
     ]
    }
   ],
   "source": [
    "missed = set()\n",
    "for wd in unique_words:\n",
    "    if wd not in cb_emb_index:\n",
    "        missed.add(wd)\n",
    "print(len(missed), len(unique_words), 100.0 * round(len(missed)/  len(unique_words),4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 141, 1645, 1681)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the test data is represented\n",
    "# seed to maxlen\n",
    "max_test_len = maxlen\n",
    "for essay in tagged_essays_test:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word)  # starts at 0, but 0 used to pad sequences\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "        max_test_len = max(max_test_len, len(sentence) + 2)\n",
    "        \n",
    "max_features_test=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "_,_,_,_ = get_training_data(tagged_essays_test, max_features_test)\n",
    "\n",
    "maxlen, max_test_len, max_features, max_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = list(cb_emb_index.values())[0].shape[0]\n",
    "\n",
    "def get_embedding_matrix(words, idgenerator, max_features, init='uniform', unit_length=False):\n",
    "    embedding_dim = list(cb_emb_index.values())[0].shape[0]\n",
    "    # initialize with a uniform distribution\n",
    "    if init == 'uniform':\n",
    "        # NOTE: the max norms for these is quite low relative to the embeddings\n",
    "        embedding_matrix = np.random.uniform(low=-0.05, high=0.05, size=(max_features, embedding_dim))\n",
    "    elif init == 'zeros':\n",
    "        embedding_matrix = np.zeros(shape=(max_features, embedding_dim), dtype=np.float32)\n",
    "    elif init == 'normal':\n",
    "        raise Exception(\"Need to compute the mean and sd\")\n",
    "        #embedding_matrix = np.random.normal(mean, sd, size=(max_features, embedding_dim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown init type\")\n",
    "    for word in words:\n",
    "        i = idgenerator.get_id(word)\n",
    "        embedding_vector = cb_emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    if unit_length:\n",
    "        norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "        # remove 0 norms to prevent divide by zero\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embedding_matrix = embedding_matrix / norms\n",
    "    return embedding_matrix\n",
    "\n",
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict_classes(xs, batch_size=batch_size, verbose=0)\n",
    "    pred_ys_by_tag = collapse_results(seq_len, preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-09-16 21:30:58.615120', '20180916_213058_615150')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "def get_ts():\n",
    "    # something screws up import so making local\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_file_ts():\n",
    "    # something screws up import so making local\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "\n",
    "embedding_size = EMBEDDING_DIM\n",
    "hidden_size    = 128\n",
    "out_size = len(vtags)\n",
    "batch_size = 128\n",
    "\n",
    "get_ts(), get_file_ts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bi-Directional LSTM With Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Models/Bi-LSTM-4-Anaphora-Binary-Fixed/fold_ix-0_bi_directional-True_hidden_size-128_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.h5'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_signature(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    lcls = locals()\n",
    "    s = \"\"\n",
    "    for k, val in sorted(lcls.items(), key = lambda tpl: (0,tpl[0]) if tpl[0] == 'fold_ix' else (1,tpl[0])):\n",
    "        if val is not None:\n",
    "            s += \"{key}-{val}_\".format(key=k, val=str(val))\n",
    "    return s[:-1]\n",
    "\n",
    "def get_file_name(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fsig = get_file_signature(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    return models_folder + fsig + \".h5\"\n",
    "\n",
    "get_file_name(0, True, True, 2, \"sum\", hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_fold(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    if use_pretrained_embedding:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform',\n",
    "                                                unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=maxlen,\n",
    "                                    trainable=True,\n",
    "                                    mask_zero=True)  # If false, initialize unfound words with all 0's\n",
    "    else:\n",
    "        embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "    if bi_directional:\n",
    "        rnn_layer_fact = lambda: Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"),\n",
    "                                               merge_mode=merge_mode)\n",
    "    else:\n",
    "        rnn_layer_fact = lambda: GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "\n",
    "    X_train, y_train, train_ys_by_tag, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev, y_dev, dev_ys_by_tag, seq_len_dev = fold2dev_data[fold_ix]\n",
    "    X_test, y_test, test_ys_by_tag, seq_len_test = fold2test_data[fold_ix]\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1  # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics, _ = score_predictions(model, X_dev, dev_ys_by_tag, seq_len_dev)\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else:  # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            break\n",
    "\n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    train_micro_metrics, train_predictions_by_tag = score_predictions(model, X_train, train_ys_by_tag, seq_len_train)\n",
    "    test_micro_metrics, test_predictions_by_tag = score_predictions(model, X_test, test_ys_by_tag, seq_len_test)\n",
    "    return model, train_predictions_by_tag, test_predictions_by_tag, train_ys_by_tag, test_ys_by_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metrics_coref_rnn'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.dbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def get_predictions_fname(fold, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fsig = get_file_signature(fold, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    return predictions_folder + fsig + \".dill\"\n",
    "\n",
    "def predictions_to_tags(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    sentence_tags = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        sentence_tags.append(pred_ys)\n",
    "    return sentence_tags\n",
    "    \n",
    "def assign_tags_to_essays(essays, preds, seq_len):\n",
    "    pred_tags = predictions_to_tags(seq_len, preds)\n",
    "    sent_ix = 0\n",
    "    for essay in essays:\n",
    "        ptagged_sentences = []\n",
    "        for sent in essay.sentences:\n",
    "            ptags = pred_tags[sent_ix]\n",
    "            assert len(ptags) == len(sent), \"Sentence and tags don't align - ntags %i , len(sentence) %i\" % ((len(ptags),len(sent)))\n",
    "            ptagged_sentences.append(ptags)\n",
    "            sent_ix += 1\n",
    "        assert len(ptagged_sentences) == len(essay.sentences), \"Lens differ\"\n",
    "        essay.pred_tagged_sentences = ptagged_sentences\n",
    "    assert sent_ix == len(pred_tags), \"Predictions don't align with sequence lens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(use_pretrained_embedding, bi_directional, num_rnns, maerge_mode, hidden_size):\n",
    "    \n",
    "    cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    fold2model = {}\n",
    "    for i in range(CV_FOLDS):\n",
    "        result = evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)         \n",
    "        model, td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = result\n",
    "        \n",
    "        merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "        merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "        merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "        merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "        \n",
    "        fname = get_file_name(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        model.save(fname)    \n",
    "        fold2model[i] = model\n",
    "\n",
    "    SUFFIX = \"_RNN_BINARY_FIXED\"\n",
    "    CB_TAGGING_TD, CB_TAGGING_VD = \"CB_TAGGING_TD\" + SUFFIX, \"CB_TAGGING_VD\" + SUFFIX\n",
    "    parameters = dict(config)\n",
    "    parameters[\"extractors\"] = []\n",
    "    parameters[\"min_feat_freq\"] = 0\n",
    "\n",
    "    parameters[\"use_pretrained_embedding\"] = use_pretrained_embedding\n",
    "    parameters[\"bi-directional\"] = bi_directional\n",
    "    parameters[\"hidden_size\"] = hidden_size\n",
    "    parameters[\"merge_mode\"] = merge_mode\n",
    "    parameters[\"num_rnns\"] = num_rnns\n",
    "\n",
    "    wd_algo = \"RNN\"\n",
    "    wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag,\n",
    "                                               parameters, wd_algo)\n",
    "    wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag,\n",
    "                                               parameters, wd_algo)\n",
    "    avg_f1 = float(processor.get_metric(CB_TAGGING_VD, wd_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "    print(\"CV micro F1: {f1:.4f}\".format(f1=avg_f1))\n",
    "    return fold2model\n",
    "\n",
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_test(num_rnns, merge_mode, hidden_size):\n",
    "    embedding_matrix = get_embedding_matrix(unique_words, generator, max_features_test, init='uniform', unit_length=False)\n",
    "    embedding_layer = Embedding(max_features_test,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_test_len,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    #model.add(TimeDistributed(Dense(out_size)))\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "    \n",
    "    essays_train, essays_dev = train_dev_split(tagged_essays, DEV_SPLIT)\n",
    "    X_train, y_train,  train_ys_bytag_con_sent,  seq_len_train = get_training_data(essays_train, max_test_len)\n",
    "    X_dev,   y_dev,    dev_ys_bytag_con_sent,    seq_len_dev   = get_training_data(essays_dev,   max_test_len)\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "    #for i in range(10):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_bytag_con_sent, seq_len_dev)\n",
    "\n",
    "        print(micro_metrics)\n",
    "        print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Test Dataset - Best F1 Score={f1}\".format(f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop - Train, Predict, Save Essays, Repeat for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model_save_predictions(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    print(str(datetime.datetime.now()))\n",
    "    print(\"Training Model CV\")\n",
    "    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "    fold2model = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "\n",
    "    print(\"Getting predictions, assign to essays\")\n",
    "    # Get predictions by fold\n",
    "    predicts_by_fold = {}\n",
    "    for fold_ix in range(CV_FOLDS):\n",
    "        X_test,  y_test,  test_ys_bytag_con_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "        model = fold2model[fold_ix]\n",
    "        probs = model.predict_classes(X_test)\n",
    "        predicts_by_fold[fold_ix] = probs\n",
    "\n",
    "    # assign predicted tags to essays\n",
    "    all_ptagged_essays = []\n",
    "    for fold in fold2test_essays.keys():\n",
    "        essays = fold2test_essays[fold]\n",
    "        preds = predicts_by_fold[fold]\n",
    "        _,_,_,seq_len = fold2test_data[fold]\n",
    "        assign_tags_to_essays(essays, preds, seq_len)\n",
    "        all_ptagged_essays.extend(essays)\n",
    "\n",
    "    fname = predictions_folder + \"essays_train_\" + get_file_signature(None, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) + \".dill\"\n",
    "    with open(fname, \"wb\") as f:\n",
    "        dill.dump(all_ptagged_essays, f)\n",
    "\n",
    "    print(\"Training test model\")\n",
    "    test_model = evaluate_test(num_rnns, merge_mode, hidden_size)\n",
    "\n",
    "    print(\"Getting test predictions\")\n",
    "    X_test,  y_test,   test_ys_bytag_con_sent,   seq_len_test = get_training_data(tagged_essays_test, max_test_len)\n",
    "    test_preds = test_model.predict_classes(X_test)\n",
    "\n",
    "    print(\"Assigning Test Predictions to Essays\")\n",
    "    assign_tags_to_essays(tagged_essays_test, test_preds, seq_len_test)\n",
    "\n",
    "    fname = predictions_folder + \"essays_test_\" + get_file_signature(None, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) + \".dill\"\n",
    "    with open(fname, \"wb\") as f:\n",
    "        dill.dump(tagged_essays_test, f)\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping num rnns: 2 and hidden size: 256\n",
      "2018-09-17 07:42:29.011673\n",
      "Training Model CV\n",
      "[0] Params 2018-09-17 07:42:29.012117 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=128\n",
      "2018-09-17 07:42:31.776660: Epoch=0\n",
      "2018-09-17 07:44:17.360469: Epoch=1\n",
      "2018-09-17 07:45:36.129564: Epoch=2\n",
      "2018-09-17 07:46:55.001672: Epoch=3\n",
      "2018-09-17 07:48:14.009622: Epoch=4\n",
      "2018-09-17 07:49:32.951984: Epoch=5\n",
      "2018-09-17 07:50:51.885589: Epoch=6\n",
      "2018-09-17 07:52:10.731077: Epoch=7\n",
      "2018-09-17 07:53:29.720860: Epoch=8\n",
      "2018-09-17 07:54:48.665789: Epoch=9\n",
      "2018-09-17 07:56:07.674704: Epoch=10\n",
      "2018-09-17 07:57:48.421133: Epoch=0\n",
      "2018-09-17 07:59:37.738855: Epoch=1\n",
      "2018-09-17 08:00:58.885417: Epoch=2\n",
      "2018-09-17 08:02:20.124494: Epoch=3\n",
      "2018-09-17 08:03:41.859776: Epoch=4\n",
      "2018-09-17 08:05:03.299658: Epoch=5\n",
      "2018-09-17 08:06:24.715628: Epoch=6\n",
      "2018-09-17 08:07:46.223892: Epoch=7\n",
      "2018-09-17 08:09:07.783954: Epoch=8\n",
      "2018-09-17 08:10:29.207563: Epoch=9\n",
      "2018-09-17 08:12:12.523066: Epoch=0\n",
      "2018-09-17 08:14:02.707262: Epoch=1\n",
      "2018-09-17 08:15:23.354979: Epoch=2\n",
      "2018-09-17 08:16:43.938219: Epoch=3\n",
      "2018-09-17 08:18:04.634616: Epoch=4\n",
      "2018-09-17 08:19:25.085739: Epoch=5\n",
      "2018-09-17 08:20:45.644517: Epoch=6\n",
      "2018-09-17 08:22:06.462550: Epoch=7\n",
      "2018-09-17 08:23:48.861849: Epoch=0\n",
      "2018-09-17 08:25:38.391933: Epoch=1\n",
      "2018-09-17 08:26:57.353767: Epoch=2\n",
      "2018-09-17 08:28:16.112177: Epoch=3\n",
      "2018-09-17 08:29:34.979178: Epoch=4\n",
      "2018-09-17 08:30:53.890888: Epoch=5\n",
      "2018-09-17 08:32:12.855027: Epoch=6\n",
      "2018-09-17 08:33:31.816958: Epoch=7\n",
      "2018-09-17 08:34:50.742152: Epoch=8\n",
      "2018-09-17 08:36:09.474594: Epoch=9\n",
      "2018-09-17 08:37:28.567561: Epoch=10\n",
      "2018-09-17 08:38:47.589657: Epoch=11\n",
      "2018-09-17 08:40:27.852450: Epoch=0\n",
      "2018-09-17 08:42:18.188938: Epoch=1\n",
      "2018-09-17 08:43:38.468282: Epoch=2\n",
      "2018-09-17 08:44:58.877063: Epoch=3\n",
      "2018-09-17 08:46:19.142914: Epoch=4\n",
      "2018-09-17 08:47:39.564128: Epoch=5\n",
      "2018-09-17 08:48:59.877892: Epoch=6\n",
      "2018-09-17 08:50:20.121042: Epoch=7\n",
      "CV micro F1: 0.3077\n",
      "[0] Params 2018-09-17 08:52:02.114175 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=128\n",
      "Getting predictions, assign to essays\n",
      "1746/1746 [==============================] - 5s     \n",
      "1560/1560 [==============================] - 5s     \n",
      "1663/1663 [==============================] - 5s     \n",
      "1687/1687 [==============================] - 5s     \n",
      "Training test model\n",
      "2018-09-17 08:52:34.442254: Epoch=0\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9979, Codes:    27\n",
      "\n",
      "2018-09-17 08:55:38.820172: Epoch=1\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9979, Codes:    27\n",
      "\n",
      "2018-09-17 08:58:12.289130: Epoch=2\n",
      "Recall: 0.0370, Precision: 1.0000, F1: 0.0714, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "2018-09-17 09:00:45.489873: Epoch=3\n",
      "Recall: 0.0741, Precision: 1.0000, F1: 0.1379, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "2018-09-17 09:03:19.817236: Epoch=4\n",
      "Recall: 0.1481, Precision: 0.5714, F1: 0.2353, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "2018-09-17 09:05:53.135882: Epoch=5\n",
      "Recall: 0.4074, Precision: 0.3438, F1: 0.3729, Accuracy: 0.9971, Codes:    27\n",
      "\n",
      "2018-09-17 09:08:26.795493: Epoch=6\n",
      "Recall: 0.2593, Precision: 0.4118, F1: 0.3182, Accuracy: 0.9977, Codes:    27\n",
      "\n",
      "2018-09-17 09:11:00.322388: Epoch=7\n",
      "Recall: 0.3333, Precision: 0.3913, F1: 0.3600, Accuracy: 0.9975, Codes:    27\n",
      "\n",
      "2018-09-17 09:13:34.275930: Epoch=8\n",
      "Recall: 0.1852, Precision: 0.3846, F1: 0.2500, Accuracy: 0.9977, Codes:    27\n",
      "\n",
      "Test Dataset - Best F1 Score=0.37288135593220334\n",
      "Getting test predictions\n",
      "1918/1918 [==============================] - 9s     \n",
      "Assigning Test Predictions to Essays\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2018-09-17 09:16:18.720195\n",
      "Training Model CV\n",
      "[1] Params 2018-09-17 09:16:18.720505 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=64\n",
      "2018-09-17 09:16:18.966927: Epoch=0\n",
      "2018-09-17 09:17:32.750807: Epoch=1\n",
      "2018-09-17 09:18:13.294555: Epoch=2\n",
      "2018-09-17 09:18:53.967610: Epoch=3\n",
      "2018-09-17 09:19:45.362233: Epoch=0\n",
      "2018-09-17 09:21:02.011673: Epoch=1\n",
      "2018-09-17 09:21:44.015242: Epoch=2\n",
      "2018-09-17 09:22:26.131411: Epoch=3\n",
      "2018-09-17 09:23:18.993818: Epoch=0\n",
      "2018-09-17 09:24:31.657672: Epoch=1\n",
      "2018-09-17 09:25:13.162304: Epoch=2\n",
      "2018-09-17 09:25:53.853331: Epoch=3\n",
      "2018-09-17 09:26:45.599588: Epoch=0\n",
      "2018-09-17 09:27:54.344185: Epoch=1\n",
      "2018-09-17 09:28:34.392010: Epoch=2\n",
      "2018-09-17 09:29:14.522259: Epoch=3\n",
      "2018-09-17 09:29:54.680066: Epoch=4\n",
      "2018-09-17 09:30:34.767796: Epoch=5\n",
      "2018-09-17 09:31:14.830946: Epoch=6\n",
      "2018-09-17 09:31:54.864619: Epoch=7\n",
      "2018-09-17 09:32:34.937071: Epoch=8\n",
      "2018-09-17 09:33:15.053232: Epoch=9\n",
      "2018-09-17 09:33:55.215258: Epoch=10\n",
      "2018-09-17 09:34:35.347640: Epoch=11\n",
      "2018-09-17 09:35:15.435011: Epoch=12\n",
      "2018-09-17 09:36:06.049131: Epoch=0\n",
      "2018-09-17 09:37:15.944525: Epoch=1\n",
      "2018-09-17 09:37:56.680347: Epoch=2\n",
      "2018-09-17 09:38:37.390506: Epoch=3\n",
      "2018-09-17 09:39:18.057875: Epoch=4\n",
      "2018-09-17 09:39:58.841835: Epoch=5\n",
      "2018-09-17 09:40:39.566499: Epoch=6\n",
      "2018-09-17 09:41:20.310399: Epoch=7\n",
      "2018-09-17 09:42:01.010438: Epoch=8\n",
      "2018-09-17 09:42:41.789239: Epoch=9\n",
      "2018-09-17 09:43:22.560451: Epoch=10\n",
      "2018-09-17 09:44:03.307907: Epoch=11\n",
      "2018-09-17 09:44:44.124810: Epoch=12\n",
      "CV micro F1: 0.1640\n",
      "[1] Params 2018-09-17 09:45:37.066896 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=64\n",
      "Getting predictions, assign to essays\n",
      "1746/1746 [==============================] - 2s     \n",
      "1560/1560 [==============================] - 2s     \n",
      "1687/1687 [==============================] - 2s     \n",
      "Training test model\n",
      "2018-09-17 09:45:53.591709: Epoch=0\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "2018-09-17 09:47:44.660595: Epoch=1\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "2018-09-17 09:49:05.042018: Epoch=2\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "2018-09-17 09:50:25.390512: Epoch=3\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9980, Codes:    27\n",
      "\n",
      "Test Dataset - Best F1 Score=0.0\n",
      "Getting test predictions\n",
      "1918/1918 [==============================] - 3s     \n",
      "Assigning Test Predictions to Essays\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2018-09-17 09:51:50.720226\n",
      "Training Model CV\n",
      "[2] Params 2018-09-17 09:51:50.720543 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=256\n",
      "2018-09-17 09:51:50.859916: Epoch=0\n",
      "2018-09-17 09:53:27.663807: Epoch=1\n",
      "2018-09-17 09:54:48.000396: Epoch=2\n",
      "2018-09-17 09:56:08.113929: Epoch=3\n",
      "2018-09-17 09:57:27.534394: Epoch=4\n",
      "2018-09-17 09:58:46.853118: Epoch=5\n",
      "2018-09-17 10:00:06.104620: Epoch=6\n",
      "2018-09-17 10:01:25.481104: Epoch=7\n",
      "2018-09-17 10:03:07.922584: Epoch=0\n",
      "2018-09-17 10:04:51.249953: Epoch=1\n",
      "2018-09-17 10:06:13.809540: Epoch=2\n",
      "2018-09-17 10:07:36.644059: Epoch=3\n",
      "2018-09-17 10:08:59.282917: Epoch=4\n",
      "2018-09-17 10:10:22.015270: Epoch=5\n",
      "2018-09-17 10:11:44.709819: Epoch=6\n",
      "2018-09-17 10:13:07.560330: Epoch=7\n",
      "2018-09-17 10:14:30.404407: Epoch=8\n",
      "2018-09-17 10:15:53.542615: Epoch=9\n",
      "2018-09-17 10:17:38.429435: Epoch=0\n",
      "2018-09-17 10:19:13.996475: Epoch=1\n",
      "2018-09-17 10:20:35.417311: Epoch=2\n",
      "2018-09-17 10:21:57.193084: Epoch=3\n",
      "2018-09-17 10:23:41.652453: Epoch=0\n",
      "2018-09-17 10:25:23.971448: Epoch=1\n",
      "2018-09-17 10:26:44.417660: Epoch=2\n",
      "2018-09-17 10:28:04.469112: Epoch=3\n",
      "2018-09-17 10:29:24.940864: Epoch=4\n",
      "2018-09-17 10:30:45.088161: Epoch=5\n",
      "2018-09-17 10:32:05.302223: Epoch=6\n",
      "2018-09-17 10:33:25.720809: Epoch=7\n",
      "2018-09-17 10:34:46.102006: Epoch=8\n",
      "2018-09-17 10:36:29.264393: Epoch=0\n",
      "2018-09-17 10:38:04.525963: Epoch=1\n",
      "2018-09-17 10:39:25.907106: Epoch=2\n",
      "2018-09-17 10:40:47.267603: Epoch=3\n",
      "2018-09-17 10:42:08.884554: Epoch=4\n",
      "2018-09-17 10:43:30.569674: Epoch=5\n",
      "2018-09-17 10:44:51.723101: Epoch=6\n",
      "2018-09-17 10:46:12.707149: Epoch=7\n",
      "2018-09-17 10:47:34.558517: Epoch=8\n",
      "2018-09-17 10:48:56.408240: Epoch=9\n",
      "2018-09-17 10:50:18.161460: Epoch=10\n",
      "2018-09-17 10:51:40.101692: Epoch=11\n",
      "2018-09-17 10:53:02.065374: Epoch=12\n",
      "2018-09-17 10:54:23.891202: Epoch=13\n",
      "2018-09-17 10:55:45.619892: Epoch=14\n",
      "CV micro F1: 0.2846\n",
      "[2] Params 2018-09-17 10:57:30.419884 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=256\n",
      "Getting predictions, assign to essays\n",
      "1746/1746 [==============================] - 5s     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560/1560 [==============================] - 4s     \n",
      "1663/1663 [==============================] - 5s     \n",
      "1687/1687 [==============================] - 5s     \n",
      "Training test model\n",
      "2018-09-17 10:58:05.310157: Epoch=0\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9981, Codes:    25\n",
      "\n",
      "2018-09-17 11:00:57.865816: Epoch=1\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9981, Codes:    25\n",
      "\n",
      "2018-09-17 11:03:35.819297: Epoch=2\n",
      "Recall: 0.2000, Precision: 0.6250, F1: 0.3030, Accuracy: 0.9982, Codes:    25\n",
      "\n",
      "2018-09-17 11:06:13.581667: Epoch=3\n",
      "Recall: 0.1200, Precision: 0.7500, F1: 0.2069, Accuracy: 0.9982, Codes:    25\n",
      "\n",
      "2018-09-17 11:08:51.713595: Epoch=4\n",
      "Recall: 0.3200, Precision: 0.6667, F1: 0.4324, Accuracy: 0.9984, Codes:    25\n",
      "\n",
      "2018-09-17 11:11:29.605955: Epoch=5\n",
      "Recall: 0.4400, Precision: 0.5238, F1: 0.4783, Accuracy: 0.9982, Codes:    25\n",
      "\n",
      "2018-09-17 11:14:07.556187: Epoch=6\n",
      "Recall: 0.4000, Precision: 0.5263, F1: 0.4545, Accuracy: 0.9982, Codes:    25\n",
      "\n",
      "2018-09-17 11:16:45.721141: Epoch=7\n",
      "Recall: 0.3200, Precision: 0.5714, F1: 0.4103, Accuracy: 0.9982, Codes:    25\n",
      "\n",
      "2018-09-17 11:19:23.795740: Epoch=8\n",
      "Recall: 0.4000, Precision: 0.4545, F1: 0.4255, Accuracy: 0.9979, Codes:    25\n",
      "\n",
      "Test Dataset - Best F1 Score=0.4782608695652174\n",
      "Getting test predictions\n",
      "1918/1918 [==============================] - 8s     \n",
      "Assigning Test Predictions to Essays\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2018-09-17 11:22:11.690140\n",
      "Training Model CV\n",
      "[3] Params 2018-09-17 11:22:11.690729 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=128\n",
      "2018-09-17 11:22:11.827537: Epoch=0\n",
      "2018-09-17 11:23:10.199456: Epoch=1\n",
      "2018-09-17 11:23:49.553023: Epoch=2\n",
      "2018-09-17 11:24:28.772650: Epoch=3\n",
      "2018-09-17 11:25:18.738885: Epoch=0\n",
      "2018-09-17 11:26:13.795110: Epoch=1\n",
      "2018-09-17 11:26:54.494475: Epoch=2\n",
      "2018-09-17 11:27:35.061781: Epoch=3\n",
      "2018-09-17 11:28:26.068963: Epoch=0\n",
      "2018-09-17 11:29:26.923108: Epoch=1\n",
      "2018-09-17 11:30:07.242960: Epoch=2\n",
      "2018-09-17 11:30:47.546907: Epoch=3\n",
      "2018-09-17 11:31:38.587737: Epoch=0\n",
      "2018-09-17 11:32:32.775569: Epoch=1\n",
      "2018-09-17 11:33:12.380302: Epoch=2\n",
      "2018-09-17 11:33:51.982068: Epoch=3\n",
      "2018-09-17 11:34:42.067303: Epoch=0\n",
      "2018-09-17 11:35:41.002520: Epoch=1\n",
      "2018-09-17 11:36:21.032934: Epoch=2\n",
      "2018-09-17 11:37:01.169637: Epoch=3\n",
      "CV micro F1: 0.0000\n",
      "[3] Params 2018-09-17 11:37:51.849661 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=128\n",
      "Getting predictions, assign to essays\n",
      "1663/1663 [==============================] - 2s     \n",
      "1664/1687 [============================>.] - ETA: 0sTraining test model\n",
      "2018-09-17 11:38:10.619738: Epoch=0\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9974, Codes:    36\n",
      "\n",
      "2018-09-17 11:39:46.714797: Epoch=1\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9974, Codes:    36\n",
      "\n",
      "2018-09-17 11:41:04.129934: Epoch=2\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9974, Codes:    36\n",
      "\n",
      "2018-09-17 11:42:21.501835: Epoch=3\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9974, Codes:    36\n",
      "\n",
      "Test Dataset - Best F1 Score=0.0\n",
      "Getting test predictions\n",
      "1918/1918 [==============================] - 4s     \n",
      "Assigning Test Predictions to Essays\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2018-09-17 11:43:45.102733\n",
      "Training Model CV\n",
      "[4] Params 2018-09-17 11:43:45.103098 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=64\n",
      "2018-09-17 11:43:45.231142: Epoch=0\n",
      "2018-09-17 11:44:20.498490: Epoch=1\n",
      "2018-09-17 11:44:41.376563: Epoch=2\n",
      "2018-09-17 11:45:02.214542: Epoch=3\n",
      "2018-09-17 11:45:28.485032: Epoch=0\n",
      "2018-09-17 11:46:09.503991: Epoch=1\n",
      "2018-09-17 11:46:30.972827: Epoch=2\n",
      "2018-09-17 11:46:52.486408: Epoch=3\n",
      "2018-09-17 11:47:19.631418: Epoch=0\n",
      "2018-09-17 11:47:56.129782: Epoch=1\n",
      "2018-09-17 11:48:17.534758: Epoch=2\n",
      "2018-09-17 11:48:38.861854: Epoch=3\n",
      "2018-09-17 11:49:05.897498: Epoch=0\n",
      "2018-09-17 11:49:41.483996: Epoch=1\n",
      "2018-09-17 11:50:02.326978: Epoch=2\n",
      "2018-09-17 11:50:23.398911: Epoch=3\n",
      "2018-09-17 11:50:49.611346: Epoch=0\n",
      "2018-09-17 11:51:32.133806: Epoch=1\n",
      "2018-09-17 11:51:53.362188: Epoch=2\n",
      "2018-09-17 11:52:14.615422: Epoch=3\n",
      "CV micro F1: 0.0000\n",
      "[4] Params 2018-09-17 11:52:41.282081 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=64\n",
      "Getting predictions, assign to essays\n",
      "1746/1746 [==============================] - 1s     \n",
      "1560/1560 [==============================] - 1s     \n",
      "1663/1663 [==============================] - 1s     \n",
      "1664/1687 [============================>.] - ETA: 0sTraining test model\n",
      "2018-09-17 11:52:52.267149: Epoch=0\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 11:53:52.356101: Epoch=1\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 11:54:34.090312: Epoch=2\n",
      "Recall: 0.0263, Precision: 1.0000, F1: 0.0513, Accuracy: 0.9974, Codes:    38\n",
      "\n",
      "2018-09-17 11:55:15.599651: Epoch=3\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 11:55:57.192916: Epoch=4\n",
      "Recall: 0.2368, Precision: 0.4286, F1: 0.3051, Accuracy: 0.9971, Codes:    38\n",
      "\n",
      "2018-09-17 11:56:39.236879: Epoch=5\n",
      "Recall: 0.0263, Precision: 0.5000, F1: 0.0500, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 11:57:20.922154: Epoch=6\n",
      "Recall: 0.1579, Precision: 0.5455, F1: 0.2449, Accuracy: 0.9974, Codes:    38\n",
      "\n",
      "2018-09-17 11:58:02.536759: Epoch=7\n",
      "Recall: 0.2105, Precision: 0.6154, F1: 0.3137, Accuracy: 0.9975, Codes:    38\n",
      "\n",
      "2018-09-17 11:58:44.269422: Epoch=8\n",
      "Recall: 0.1842, Precision: 0.5385, F1: 0.2745, Accuracy: 0.9974, Codes:    38\n",
      "\n",
      "2018-09-17 11:59:26.244841: Epoch=9\n",
      "Recall: 0.3421, Precision: 0.5000, F1: 0.4063, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 12:00:07.945679: Epoch=10\n",
      "Recall: 0.2895, Precision: 0.5238, F1: 0.3729, Accuracy: 0.9974, Codes:    38\n",
      "\n",
      "2018-09-17 12:00:49.542110: Epoch=11\n",
      "Recall: 0.4211, Precision: 0.5000, F1: 0.4571, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 12:01:31.291475: Epoch=12\n",
      "Recall: 0.2632, Precision: 0.5000, F1: 0.3448, Accuracy: 0.9973, Codes:    38\n",
      "\n",
      "2018-09-17 12:02:13.392502: Epoch=13\n",
      "Recall: 0.3947, Precision: 0.4412, F1: 0.4167, Accuracy: 0.9970, Codes:    38\n",
      "\n",
      "2018-09-17 12:02:54.874662: Epoch=14\n",
      "Recall: 0.2105, Precision: 0.6154, F1: 0.3137, Accuracy: 0.9975, Codes:    38\n",
      "\n",
      "Test Dataset - Best F1 Score=0.45714285714285713\n",
      "Getting test predictions\n",
      "1888/1918 [============================>.] - ETA: 0sAssigning Test Predictions to Essays\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_pretrained_embedding = True\n",
    "bi_directional = True\n",
    "merge_mode = \"sum\"\n",
    "hidden_size = 256 # NOT 256 !!!!\n",
    "\n",
    "for num_rnns in [2,1]:\n",
    "    for hidden_size in [256,128,64]:\n",
    "        # already done\n",
    "        if num_rnns == 2 and hidden_size == 256:\n",
    "            print(\"Skipping num rnns: {rnns} and hidden size: {hidden_size}\".format(\n",
    "                rnns=num_rnns, hidden_size=hidden_size))\n",
    "            continue\n",
    "        train_test_model_save_predictions(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - once satisfied, delete the HYPER_PARAM tuning collections and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras_and_theano_bleeding_edge]",
   "language": "python",
   "name": "conda-env-keras_and_theano_bleeding_edge-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
