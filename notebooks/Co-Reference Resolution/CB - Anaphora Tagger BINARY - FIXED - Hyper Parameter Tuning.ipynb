{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py\n",
    "and also on this code: https://github.com/simonhughes22/PythonNlpResearch/blob/master/notebooks/SEARN/CB%20-%20Keras%20-%20Train%20Tagger%20and%20Save%20CV%20Predictions%20For%20Word%20Tags-NO%20EXPLICIT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "* The reason this nbook exists is that the tags used in the SEARN model and thus also the CoRef model do not seem to have been generated by the best model (they are well below the best reported results)\n",
    "* I am attempting to rectify this here, and if it works, can potentially re-run SEARN experiments also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Change to use 128 hidden units not 256 (was actually optimal settings)\n",
    "* Then Validate accuracy\n",
    "* Is that doesn't work, drop max len and ignore test data for the purposes of the CV run (on training data), then re-institute for training test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check mongo is running\n",
    "def is_mongo_runnning():\n",
    "    import pymongo\n",
    "    client = pymongo.MongoClient(serverSelectionTimeoutMS=100)\n",
    "    db = client.metrics_codes\n",
    "    coll = db.get_collection(\"CB_TAGGING_TD_AVG_PERCEPTRON_MOST_COMMON_TAG\")\n",
    "    l = list(coll.find({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_mongo_runnning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - To Get this working:\n",
    "\n",
    "* Install CUDA and associated libraries, setup path\n",
    "* Install bleeding edge theano (from src)\n",
    "* Make sure the THEANO_FLAGS are set correctly via the environment var, or via the ~/.theanorc file\n",
    "* Install and compile bleeding edge Keras (from src)\n",
    "* `export KERAS_BACKEND=theano`\n",
    "* `export KERAS_IMAGE_DIM_ORDERING='th'`\n",
    "* `sh <project_root>/shell_scipts/setup_environment.sh` to install additional dependencies\n",
    "* **DO NOT SET UNROLL=True** when creating RNN's - causes max recursion issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "* You may need to clean the theano cache. To do so thoroughly, run this command from the shell:\n",
    " * `theano-cache purge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "#from keras.layers import TimeDistributed\n",
    "from keras.layers import TimeDistributedDense\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "from window_based_tagger_config import get_config\n",
    "from DirUtils import dir_exists\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "DATASET = \"CoralBleaching\"\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "training_pickled = settings.data_directory + DATASET + \"/Thesis_Dataset/training.pl\"\n",
    "\n",
    "models_folder = root_folder + \"Models/Bi-LSTM-4-Anaphora-Binary-Fixed/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor(dbname=\"metrics_coref_rnn_fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Folders are Valid and Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Models/Bi-LSTM-4-Anaphora-Binary-Fixed/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_if_missing(folder):\n",
    "    if not dir_exists(folder):\n",
    "        print(\"Dir missing, creating\")\n",
    "        os.makedirs(folder)\n",
    "    assert dir_exists(folder)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "create_dir_if_missing(models_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Essays"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = get_config(training_folder)\n",
    "tagged_essays_tmp = load_process_essays(**config)\n",
    "\n",
    "with open(training_pickled, \"wb+\") as f:\n",
    "    pickle.dump(tagged_essays_tmp, f)\n",
    "del tagged_essays_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 files found\n",
      "226 essays processed\n"
     ]
    }
   ],
   "source": [
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2018-09-01 11:32:25.471925\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "# for essay in tagged_essays_test:\n",
    "#     for sentence in essay.sentences:\n",
    "#         for word, tags in sentence:\n",
    "#             unique_words.add(word)\n",
    "#             for tag in tags:\n",
    "#                 tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if t == \"Anaphor\"))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "assert \"Anaphor\" in regular_tags\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(regular_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor', 'Empty']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Essays into Training Data (Word Ids)\n",
    "\n",
    "* Computes `xs`, `ys`, `ys_bytag` and `seq_lens`\n",
    "* `ys_bytag` includes **all tags** and does **not** focus only on the most common tag\n",
    "* `ys` only includes the most common tag (so we can use cross entropy)\n",
    "* `seq_lens` is without the start and end tags included (so we have to map back and forth to maintain mappings)\n",
    "* `ys_bytag` also excludes the START and END tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Sequence Length, Generate All Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix2tag = {}\n",
    "for ix, t in enumerate(vtags):\n",
    "    ix2tag[ix] = t\n",
    "\n",
    "generator = idGen(seed=1)  # important as we zero pad sequences\n",
    "\n",
    "maxlen = 0\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word)  # starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "\n",
    "def ids2tags(ids):\n",
    "    return [generator.get_key(j) for j in ids]\n",
    "\n",
    "def lbls2tags(ixs):\n",
    "    return [ix2tag[ix] for ix in ixs]\n",
    "\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays, max_seq_len=None):\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = maxlen\n",
    "    \n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_seq = []\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word)  # starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)\n",
    "\n",
    "                # Make sure to include Causer:<num> and Result:<num> tags for the Anaphora labels\n",
    "                tags = set([t.replace(\"Causer:\",\"\").replace(\"Result:\",\"\") for t in tags])\n",
    "               \n",
    "                # remove unwanted tags\n",
    "                tags = vtags.intersection(tags)\n",
    "                # retain all tags for evaluation (not just most common)\n",
    "                # SKIP the START and END tags\n",
    "                if word != START and word != END:\n",
    "                    for t in (vtags - set([EMPTY_TAG])):\n",
    "                        if t in tags:\n",
    "                            ys_bytag[t].append(1)\n",
    "                        else:\n",
    "                            ys_bytag[t].append(0)\n",
    "\n",
    "                # encode ys with most common tag only\n",
    "                if \"explicit\" in tags:\n",
    "                    tags.remove(\"explicit\")\n",
    "\n",
    "                if len(tags) > 1:\n",
    "                    most_common = max(tags, key=lambda t: tag_freq[t])\n",
    "                    tags = set([most_common])\n",
    "                if len(tags) == 0:\n",
    "                    tags.add(EMPTY_TAG)\n",
    "                    \n",
    "                assert len(tags) == 1, \"Wrong number of tags:\" + str(tags)\n",
    "\n",
    "                one_hot = []\n",
    "                for t in vtags:\n",
    "                    if t in tags:\n",
    "                        one_hot.append(1)\n",
    "                    else:\n",
    "                        one_hot.append(0)\n",
    "                y_seq.append(one_hot)\n",
    "\n",
    "            seq_lens.append(len(row) - 2)\n",
    "            ys.append(y_seq)\n",
    "            xs.append(row)\n",
    "\n",
    "    xs = sequence.pad_sequences(xs, maxlen=max_seq_len)\n",
    "    ys = sequence.pad_sequences(ys, maxlen=max_seq_len)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert xs.shape[1] == ys.shape[1] == max_seq_len, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Create Train - Test Split\n",
    "# Helper Functions\n",
    "def collapse_results(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i, :]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        for pred_tag in pred_ys:\n",
    "            pred_ys_by_tag[pred_tag].append(1)\n",
    "            # for all other tags, a 0\n",
    "            for tag in (vtags - set([EMPTY_TAG, pred_tag])):\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag\n",
    "\n",
    "def train_dev_split(lst, dev_split):\n",
    "    # random shuffle\n",
    "    shuffle(lst)\n",
    "    num_training = int((1.0 - dev_split) * len(lst))\n",
    "    return lst[:num_training], lst[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', set()),\n",
       " ('only', set()),\n",
       " ('is', set()),\n",
       " ('there', set()),\n",
       " ('many', set()),\n",
       " ('different', set()),\n",
       " ('corals', set()),\n",
       " ('living', set()),\n",
       " ('in', set()),\n",
       " ('the', set()),\n",
       " ('ocean', set()),\n",
       " (',', set()),\n",
       " ('but', set()),\n",
       " ('many', set()),\n",
       " ('different', set()),\n",
       " ('ways', set()),\n",
       " ('for', set()),\n",
       " ('the', set()),\n",
       " ('corals', {'50'}),\n",
       " ('to', {'50'}),\n",
       " ('become', {'50'}),\n",
       " ('bleached', {'50'}),\n",
       " ('.', set())]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tagged_essays[0]\n",
    "e.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.67 s, sys: 99.5 ms, total: 3.77 s\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this name for a different function later\n",
    "from CrossValidation import cross_validation as cv\n",
    "\n",
    "folds = cv(tagged_essays, CV_FOLDS)\n",
    "fold2training_data = {}\n",
    "fold2dev_data = {}\n",
    "fold2test_data = {}\n",
    "\n",
    "fold2training_essays = {}\n",
    "fold2dev_essays = {}\n",
    "fold2test_essays = {}\n",
    "\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    # further split into train and dev test\n",
    "    essays_train, essays_dev = train_dev_split(essays_TD, DEV_SPLIT)\n",
    "    fold2training_data[i] = get_training_data(essays_train)\n",
    "    fold2dev_data[i]      = get_training_data(essays_dev)\n",
    "    # Test Data\n",
    "    fold2test_data[i]     = get_training_data(essays_VD)\n",
    "    \n",
    "    # also store essays\n",
    "    fold2training_essays[i] = essays_train\n",
    "    fold2dev_essays[i]      = essays_dev\n",
    "    fold2test_essays[i]     = essays_VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure the generator is incremented on the test data too\n",
    "_,_,_,_ = get_training_data(tagged_essays)\n",
    "#_,_,_,_ = get_training_data(tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(cv_folder + \"td.dill\", \"wb\") as f:\n",
    "#     dill.dump(fold2training_data, f)\n",
    "\n",
    "# with open(cv_folder + \"td_essays.dill\", \"wb\") as f:\n",
    "#     dill.dump(fold2training_essays, f)\n",
    "\n",
    "# with open(cv_folder + \"devd.dill\", \"wb\") as f:\n",
    "#     dill.dump(fold2dev_data, f)\n",
    "\n",
    "# with open(cv_folder + \"devd_essays.dill\", \"wb\") as f:\n",
    "#     dill.dump(fold2dev_essays, f)\n",
    "    \n",
    "# with open(cv_folder + \"vd.dill\", \"wb\") as f:\n",
    "#     dill.dump(fold2test_data, f)\n",
    "    \n",
    "# with open(cv_folder + \"vd_essays.dill\", \"wb\") as f:\n",
    "#     dill.dump(fold2test_essays, f)\n",
    "    \n",
    "# with open(cv_folder + \"generator.dill\", \"wb\") as f:\n",
    "#     dill.dump(generator, f)\n",
    "\n",
    "# with open(cv_folder + \"vtags.dill\", \"wb\") as f:\n",
    "#     dill.dump(vtags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 'coral')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.get_id(\"coral\"), generator.get_key(generator.get_id(\"coral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove 100 Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/DeepLearning/WordVectors/pickle_glove_embedding.py\n",
    "# for creating pre-filtered embeddings file\n",
    "import pickle, os\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings_file = \"/Users/simon.hughes/data/word_embeddings/glove.6B/cb_dict_glove.6B.100d.txt\"\n",
    "# read data file\n",
    "with open(embeddings_file, \"rb+\") as f:\n",
    "    cb_emb_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1641 2.5 %\n"
     ]
    }
   ],
   "source": [
    "missed = set()\n",
    "for wd in unique_words:\n",
    "    if wd not in cb_emb_index:\n",
    "        missed.add(wd)\n",
    "print(len(missed), len(unique_words), 100.0 * round(len(missed)/  len(unique_words),4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = list(cb_emb_index.values())[0].shape[0]\n",
    "\n",
    "def get_embedding_matrix(words, idgenerator, max_features, init='uniform', unit_length=False):\n",
    "    embedding_dim = list(cb_emb_index.values())[0].shape[0]\n",
    "    # initialize with a uniform distribution\n",
    "    if init == 'uniform':\n",
    "        # NOTE: the max norms for these is quite low relative to the embeddings\n",
    "        embedding_matrix = np.random.uniform(low=-0.05, high=0.05, size=(max_features, embedding_dim))\n",
    "    elif init == 'zeros':\n",
    "        embedding_matrix = np.zeros(shape=(max_features, embedding_dim), dtype=np.float32)\n",
    "    elif init == 'normal':\n",
    "        raise Exception(\"Need to compute the mean and sd\")\n",
    "        #embedding_matrix = np.random.normal(mean, sd, size=(max_features, embedding_dim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown init type\")\n",
    "    for word in words:\n",
    "        i = idgenerator.get_id(word)\n",
    "        embedding_vector = cb_emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    if unit_length:\n",
    "        norms = np.linalg.norm(embedding_matrix, axis=1, keepdims=True)\n",
    "        # remove 0 norms to prevent divide by zero\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embedding_matrix = embedding_matrix / norms\n",
    "    return embedding_matrix\n",
    "\n",
    "EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict_classes(xs, batch_size=batch_size, verbose=0)\n",
    "    pred_ys_by_tag = collapse_results(seq_len, preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2018-09-01 11:32:54.882616', '20180901_113254_882699')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "def get_ts():\n",
    "    # something screws up import so making local\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_file_ts():\n",
    "    # something screws up import so making local\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "\n",
    "embedding_size = EMBEDDING_DIM\n",
    "hidden_size    = 128\n",
    "out_size = len(vtags)\n",
    "batch_size = 128\n",
    "\n",
    "get_ts(), get_file_ts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bi-Directional LSTM With Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1645"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Models/Bi-LSTM-4-Anaphora-Binary-Fixed/fold_ix-0_bi_directional-True_hidden_size-128_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.h5'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_signature(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    lcls = locals()\n",
    "    s = \"\"\n",
    "    for k, val in sorted(lcls.items(), key = lambda tpl: (0,tpl[0]) if tpl[0] == 'fold_ix' else (1,tpl[0])):\n",
    "        if val is not None:\n",
    "            s += \"{key}-{val}_\".format(key=k, val=str(val))\n",
    "    return s[:-1]\n",
    "\n",
    "def get_file_name(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fsig = get_file_signature(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    return models_folder + fsig + \".h5\"\n",
    "\n",
    "get_file_name(0, True, True, 2, \"sum\", hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_fold(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    if use_pretrained_embedding:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform',\n",
    "                                                unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=maxlen,\n",
    "                                    trainable=True,\n",
    "                                    mask_zero=True)  # If false, initialize unfound words with all 0's\n",
    "    else:\n",
    "        embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "    if bi_directional:\n",
    "        rnn_layer_fact = lambda: Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"),\n",
    "                                               merge_mode=merge_mode)\n",
    "    else:\n",
    "        rnn_layer_fact = lambda: GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "\n",
    "    X_train, y_train, train_ys_by_tag, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev, y_dev, dev_ys_by_tag, seq_len_dev = fold2dev_data[fold_ix]\n",
    "    X_test, y_test, test_ys_by_tag, seq_len_test = fold2test_data[fold_ix]\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1  # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics, _ = score_predictions(model, X_dev, dev_ys_by_tag, seq_len_dev)\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else:  # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            break\n",
    "\n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    train_micro_metrics, train_predictions_by_tag = score_predictions(model, X_train, train_ys_by_tag, seq_len_train)\n",
    "    test_micro_metrics, test_predictions_by_tag = score_predictions(model, X_test, test_ys_by_tag, seq_len_test)\n",
    "    return model, train_predictions_by_tag, test_predictions_by_tag, train_ys_by_tag, test_ys_by_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metrics_coref_rnn_fixed'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.dbname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(use_pretrained_embedding, bi_directional, num_rnns, maerge_mode, hidden_size):\n",
    "    \n",
    "    cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    fold2model = {}\n",
    "    for i in range(CV_FOLDS):\n",
    "        result = evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)         \n",
    "        model, td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = result\n",
    "        \n",
    "        merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "        merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "        merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "        merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)\n",
    "        \n",
    "        fname = get_file_name(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        model.save(fname)    \n",
    "        fold2model[i] = model\n",
    "\n",
    "    SUFFIX = \"_RNN_BINARY_HYPERPARAM_TUNING\"\n",
    "    CB_TAGGING_TD, CB_TAGGING_VD = \"CB_TAGGING_TD\" + SUFFIX, \"CB_TAGGING_VD\" + SUFFIX\n",
    "    parameters = dict(config)\n",
    "    parameters[\"extractors\"] = []\n",
    "    parameters[\"min_feat_freq\"] = 0\n",
    "\n",
    "    parameters[\"use_pretrained_embedding\"] = use_pretrained_embedding\n",
    "    parameters[\"bi-directional\"] = bi_directional\n",
    "    parameters[\"hidden_size\"] = hidden_size\n",
    "    parameters[\"merge_mode\"] = merge_mode\n",
    "    parameters[\"num_rnns\"] = num_rnns\n",
    "\n",
    "    wd_algo = \"RNN\"\n",
    "    wd_td_objectid = processor.persist_results(CB_TAGGING_TD, cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag,\n",
    "                                               parameters, wd_algo)\n",
    "    wd_vd_objectid = processor.persist_results(CB_TAGGING_VD, cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag,\n",
    "                                               parameters, wd_algo)\n",
    "    avg_f1 = float(processor.get_metric(CB_TAGGING_VD, wd_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "    print(\"CV micro F1: {f1:.4f}\".format(f1=avg_f1))\n",
    "    return fold2model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Params 2018-09-01 12:17:56.671507 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=64\n",
      "WARNING:tensorflow:From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1029: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-01 12:17:57,123 : WARNING : From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1029: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:993: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-01 12:17:57,516 : WARNING : From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:993: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1108: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-01 12:17:57,615 : WARNING : From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1108: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-01 12:17:57.632668: Epoch=0\n",
      "2018-09-01 12:18:14.940454: Epoch=1\n",
      "2018-09-01 12:18:27.517005: Epoch=2\n",
      "2018-09-01 12:18:39.793032: Epoch=3\n",
      "2018-09-01 12:18:59.465784: Epoch=0\n",
      "2018-09-01 12:19:16.322303: Epoch=1\n",
      "2018-09-01 12:19:29.228202: Epoch=2\n",
      "2018-09-01 12:19:42.663677: Epoch=3\n",
      "2018-09-01 12:20:03.340421: Epoch=0\n",
      "2018-09-01 12:20:19.764903: Epoch=1\n",
      "2018-09-01 12:20:31.981062: Epoch=2\n",
      "2018-09-01 12:20:44.074663: Epoch=3\n",
      "2018-09-01 12:21:04.624632: Epoch=0\n",
      "2018-09-01 12:21:20.904372: Epoch=1\n",
      "2018-09-01 12:21:33.790324: Epoch=2\n",
      "2018-09-01 12:21:46.797867: Epoch=3\n",
      "2018-09-01 12:22:09.039315: Epoch=0\n",
      "2018-09-01 12:22:27.003270: Epoch=1\n",
      "2018-09-01 12:22:41.022793: Epoch=2\n",
      "2018-09-01 12:22:54.717539: Epoch=3\n",
      "CV micro F1: 0.0000\n",
      "[1] Params 2018-09-01 12:23:16.223296 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=64\n",
      "[2] Params 2018-09-01 12:23:16.223406 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=128\n",
      "2018-09-01 12:23:17.553502: Epoch=0\n",
      "2018-09-01 12:23:49.070317: Epoch=1\n",
      "2018-09-01 12:24:14.806629: Epoch=2\n",
      "2018-09-01 12:24:40.347910: Epoch=3\n",
      "2018-09-01 12:25:23.120011: Epoch=0\n",
      "2018-09-01 12:25:53.510061: Epoch=1\n",
      "2018-09-01 12:26:19.524455: Epoch=2\n",
      "2018-09-01 12:26:46.063618: Epoch=3\n",
      "2018-09-01 12:27:28.958826: Epoch=0\n",
      "2018-09-01 12:28:00.730493: Epoch=1\n",
      "2018-09-01 12:28:26.851115: Epoch=2\n",
      "2018-09-01 12:28:52.473761: Epoch=3\n",
      "2018-09-01 12:29:37.989014: Epoch=0\n",
      "2018-09-01 12:30:13.662333: Epoch=1\n",
      "2018-09-01 12:30:39.054167: Epoch=2\n",
      "2018-09-01 12:31:05.462440: Epoch=3\n",
      "2018-09-01 12:31:49.969945: Epoch=0\n",
      "2018-09-01 12:32:22.967105: Epoch=1\n",
      "2018-09-01 12:32:49.972042: Epoch=2\n",
      "2018-09-01 12:33:17.380515: Epoch=3\n",
      "CV micro F1: 0.0000\n",
      "[2] Params 2018-09-01 12:34:02.705691 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=128\n",
      "[3] Params 2018-09-01 12:34:02.705776 - Embeddings=True, Bi-Direct=True Num_Rnns=1 Hidden_Size=256\n",
      "2018-09-01 12:34:05.012387: Epoch=0\n",
      "2018-09-01 12:35:13.056982: Epoch=1\n",
      "2018-09-01 12:36:17.110596: Epoch=2\n",
      "2018-09-01 12:37:17.463028: Epoch=3\n",
      "2018-09-01 12:38:16.176839: Epoch=4\n",
      "2018-09-01 12:39:20.328296: Epoch=5\n",
      "2018-09-01 12:40:20.011476: Epoch=6\n",
      "2018-09-01 12:41:19.053289: Epoch=7\n",
      "2018-09-01 12:42:17.918937: Epoch=8\n",
      "2018-09-01 12:43:16.146297: Epoch=9\n",
      "2018-09-01 12:44:48.389898: Epoch=0\n",
      "2018-09-01 12:45:52.214044: Epoch=1\n",
      "2018-09-01 12:46:49.570085: Epoch=2\n",
      "2018-09-01 12:47:51.027157: Epoch=3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for use_pretrained_embedding in [True, False]:\n",
    "    for bi_directional in [True, False]:\n",
    "        for num_rnns in [1, 2]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [64, 128, 256]:\n",
    "                    i+=1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    fold2model = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fold2model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589/1589 [==============================] - 10s    \n",
      "1731/1731 [==============================] - 11s    \n",
      "1634/1634 [==============================] - 10s    \n",
      "1696/1696 [==============================] - 10s    \n",
      "1642/1642 [==============================] - 10s    \n"
     ]
    }
   ],
   "source": [
    "predicts_by_fold = {}\n",
    "for fold_ix in range(CV_FOLDS):\n",
    "    X_test,  y_test,  test_ys_bytag_con_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "    model = fold2model[fold_ix]\n",
    "    probs = model.predict_classes(X_test)\n",
    "    predicts_by_fold[fold_ix] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predictions_fname(fold, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fsig = get_file_signature(fold, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    return predictions_folder + fsig + \".dill\"\n",
    "\n",
    "for fold, preds in predicts_by_fold.items():\n",
    "    fname = get_predictions_fname(fold, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    with open(fname, \"wb\") as f:\n",
    "        dill.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def predictions_to_tags(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    sentence_tags = []\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        sentence_tags.append(pred_ys)\n",
    "    return sentence_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Predicted Tags to Essay Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_tags_to_essays(essays, preds, seq_len):\n",
    "    pred_tags = predictions_to_tags(seq_len, preds)\n",
    "    sent_ix = 0\n",
    "    for essay in essays:\n",
    "        ptagged_sentences = []\n",
    "        for sent in essay.sentences:\n",
    "            ptags = pred_tags[sent_ix]\n",
    "            assert len(ptags) == len(sent), \"Sentence and tags don't align - ntags %i , len(sentence) %i\" % ((len(ptags),len(sent)))\n",
    "            ptagged_sentences.append(ptags)\n",
    "            sent_ix += 1\n",
    "        assert len(ptagged_sentences) == len(essay.sentences), \"Lens differ\"\n",
    "        essay.pred_tagged_sentences = ptagged_sentences\n",
    "    assert sent_ix == len(pred_tags), \"Predictions don't align with sequence lens\"\n",
    "\n",
    "all_ptagged_essays = []\n",
    "for fold in fold2test_essays.keys():\n",
    "    essays = fold2test_essays[fold]\n",
    "    preds = predicts_by_fold[fold]\n",
    "    _,_,_,seq_len = fold2test_data[fold]\n",
    "    assign_tags_to_essays(essays, preds, seq_len)\n",
    "    all_ptagged_essays.extend(essays)\n",
    "\n",
    "fname = predictions_folder + \"essays_train_\" + get_file_signature(None, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) + \".dill\"\n",
    "with open(fname, \"wb\") as f:\n",
    "    dill.dump(all_ptagged_essays, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert len(all_ptagged_essays) == len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# essay = all_ptagged_essays[0]\n",
    "# essay.pred_tagged_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 141, 1645, 1681)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed to maxlen\n",
    "max_test_len = maxlen\n",
    "for essay in tagged_essays_test:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word)  # starts at 0, but 0 used to pad sequences\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "        max_test_len = max(max_test_len, len(sentence) + 2)\n",
    "        \n",
    "max_features_test=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences\n",
    "_,_,_,_ = get_training_data(tagged_essays_test, max_features_test)\n",
    "\n",
    "maxlen, max_test_len, max_features, max_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_test(num_rnns, merge_mode, hidden_size):\n",
    "    embedding_matrix = get_embedding_matrix(unique_words, generator, max_features_test, init='uniform', unit_length=False)\n",
    "    embedding_layer = Embedding(max_features_test,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=max_test_len,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    #model.add(TimeDistributed(Dense(out_size)))\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "    \n",
    "    essays_train, essays_dev = train_dev_split(tagged_essays, DEV_SPLIT)\n",
    "    X_train, y_train,  train_ys_bytag_con_sent,  seq_len_train = get_training_data(essays_train, max_test_len)\n",
    "    X_dev,   y_dev,    dev_ys_bytag_con_sent,    seq_len_dev   = get_training_data(essays_dev,   max_test_len)\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "    #for i in range(10):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_bytag_con_sent, seq_len_dev)\n",
    "\n",
    "        print(micro_metrics)\n",
    "        print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Fold[{ix}] - Best F1 Score={f1}\".format(ix=fold_ix, f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 141)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen, max_test_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 sum 256\n",
      "2018-08-12 00:46:56.035541: Epoch=0\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9975, Codes:    37\n",
      "\n",
      "2018-08-12 00:52:43.990674: Epoch=1\n",
      "Recall: 0.0000, Precision: 0.0000, F1: 0.0000, Accuracy: 0.9975, Codes:    37\n",
      "\n",
      "2018-08-12 00:58:04.875319: Epoch=2\n",
      "Recall: 0.0270, Precision: 1.0000, F1: 0.0526, Accuracy: 0.9976, Codes:    37\n",
      "\n",
      "2018-08-12 01:03:25.930000: Epoch=3\n",
      "Recall: 0.2973, Precision: 0.3929, F1: 0.3385, Accuracy: 0.9971, Codes:    37\n",
      "\n",
      "2018-08-12 01:08:46.674905: Epoch=4\n",
      "Recall: 0.1081, Precision: 0.8000, F1: 0.1905, Accuracy: 0.9977, Codes:    37\n",
      "\n",
      "2018-08-12 01:14:07.278627: Epoch=5\n",
      "Recall: 0.2162, Precision: 0.5000, F1: 0.3019, Accuracy: 0.9975, Codes:    37\n",
      "\n",
      "2018-08-12 01:19:27.452569: Epoch=6\n",
      "Recall: 0.2162, Precision: 0.5000, F1: 0.3019, Accuracy: 0.9975, Codes:    37\n",
      "\n",
      "Fold[4] - Best F1 Score=0.3384615384615385\n"
     ]
    }
   ],
   "source": [
    "#test_model = evaluate_test(2, \"sum\", 256)\n",
    "print(num_rnns, merge_mode, hidden_size)\n",
    "test_model = evaluate_test(num_rnns, merge_mode, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1918/1918 [==============================] - 18s    \n"
     ]
    }
   ],
   "source": [
    "X_test,  y_test,   test_ys_bytag_con_sent,   seq_len_test = get_training_data(tagged_essays_test, max_test_len)\n",
    "test_preds = test_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1918, 141), 0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape, len(test_ys_bytag_con_sent['4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assign_tags_to_essays(tagged_essays_test, test_preds, seq_len_test)\n",
    "\n",
    "fname = predictions_folder + \"essays_test_\" + get_file_signature(None, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size) + \".dill\"\n",
    "with open(fname, \"wb\") as f:\n",
    "    dill.dump(tagged_essays_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('well', set()), 'Empty'),\n",
       " (('based', set()), 'Empty'),\n",
       " (('on', set()), 'Empty'),\n",
       " (('what', set()), 'Empty'),\n",
       " (('i', set()), 'Empty'),\n",
       " (('read', set()), 'Empty'),\n",
       " (('the', set()), 'Empty'),\n",
       " (('corals', {'50'}), 'Empty'),\n",
       " (('are', {'50'}), 'Empty'),\n",
       " (('INFREQUENT', {'50'}), 'Empty'),\n",
       " (('their', {'50'}), 'Empty'),\n",
       " (('colors', {'50'}), 'Empty'),\n",
       " ((',', set()), 'Empty'),\n",
       " (('coral', {'50'}), 'Empty'),\n",
       " (('bleaching', {'50'}), 'Empty'),\n",
       " (('are', set()), 'Empty'),\n",
       " (('a', set()), 'Empty'),\n",
       " (('serious', set()), 'Empty'),\n",
       " (('problem', set()), 'Empty'),\n",
       " (('with', set()), 'Empty'),\n",
       " (('a', set()), 'Empty'),\n",
       " (('serious', set()), 'Empty'),\n",
       " (('impact', set()), 'Empty'),\n",
       " (('on', set()), 'Empty'),\n",
       " (('the', set()), 'Empty'),\n",
       " (('worlds', set()), 'Empty'),\n",
       " (('coral', set()), 'Empty'),\n",
       " (('reefs', set()), 'Empty'),\n",
       " (('.', set()), 'Empty')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tagged_essays_test[0]\n",
    "list(zip(e.sentences[0],e.pred_tagged_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
