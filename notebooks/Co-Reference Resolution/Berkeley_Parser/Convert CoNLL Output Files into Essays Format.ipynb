{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "coref root:     /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/\n",
      "berkeley coref: /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/Berkeley/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "from FindFiles import find_files\n",
    "from Settings import Settings\n",
    "from CoRefHelper import EMPTY\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "DATASET = \"SkinCancer\" # CoralBleaching | SkinCancer\n",
    "PARTITION = \"Training\" # Training | Test\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "merged_predictions_folder = root_folder + \"CoReference/\"\n",
    "coref_folder = root_folder + \"CoReference/Berkeley/\"\n",
    "print(\"coref root:    \", merged_predictions_folder)\n",
    "print(\"berkeley coref:\", coref_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_essays(folder, partition):\n",
    "    essay_files = find_files(folder)\n",
    "    if partition == \"Training\":\n",
    "        essay_files = [e for e in essay_files if \"train\" in e]\n",
    "    else:\n",
    "        essay_files = [e for e in essay_files if \"test\" in e]\n",
    "    assert len(essay_files) == 1\n",
    "    print(\"Found file\", essay_files[0])\n",
    "    with open(essay_files[0], \"rb\") as f:\n",
    "        loaded_essays = dill.load(f)\n",
    "    return loaded_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/training_processed.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays = get_essays(merged_predictions_folder, PARTITION)\n",
    "len(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map essays to a dict to lookup when processing connl file\n",
    "name2essay = dict()\n",
    "for e in essays:\n",
    "    name2essay[e.name] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "connll_file = \"/Users/simon.hughes/Google Drive/PhD/Data/berkely_ner/{dataset}/{partition}/corefner/output.conll\".format(\n",
    "    dataset=DATASET, partition=PARTITION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Essays with Berkeley Output, Assert Words and Sentences are the Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "pos_tally = defaultdict(int)\n",
    "ner_tally = defaultdict(int)\n",
    "coref_tally = defaultdict(int)\n",
    "\n",
    "processed_essays = set()\n",
    "with open(connll_file, \"r+\") as f:\n",
    "    current_essay = None\n",
    "    sent_ix = -1\n",
    "    word_ix = -1\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"#begin document\"):\n",
    "            sent_ix = 0    \n",
    "            word_ix = -1\n",
    "            fname = line[line.find(\"(\")+1:line.find(\")\")].replace(\".txt\",\".ann\")            \n",
    "            assert fname in name2essay, \"Missing: %s\" % fname\n",
    "            current_essay = name2essay[fname]           \n",
    "        elif line.startswith(\"#end document\"):            \n",
    "            assert sent_ix == len(current_essay.sentences), \"Sentences should have the same length\"\n",
    "            sent_ix = -1\n",
    "            word_ix = -1\n",
    "            processed_essays.add(fname)\n",
    "            #current_essay = None\n",
    "        elif line == \"\":\n",
    "            # new sentence or end of document\n",
    "            assert word_ix == len(current_essay.sentences[sent_ix]) - 1\n",
    "            sent_ix += 1\n",
    "            word_ix = -1\n",
    "            pass\n",
    "        else:\n",
    "            word_ix += 1\n",
    "            parts = line.split(\"\\t\")\n",
    "            \n",
    "            assert len(parts) == 12\n",
    "            assert parts[0][:-4] == current_essay.name[:-4]\n",
    "            \n",
    "            berkeley_word = parts[3]\n",
    "            \n",
    "            berkeley_pos = parts[4]\n",
    "            berkeley_corefids = parts[-1].replace(\"(\",\"\").replace(\")\",\"\").replace(\"-\",\"\")\n",
    "            coref_tally[berkeley_corefids] +=1\n",
    "            berkeley_set_corefids = set(berkeley_corefids.split(\"|\"))\n",
    "            if \"\" in berkeley_set_corefids:\n",
    "                berkeley_set_corefids.remove(\"\")\n",
    "\n",
    "            berkeley_ner = parts[-2].replace(\"(\",\"\").replace(\")\",\"\").replace(\"*\",\"\")\n",
    "            if not berkeley_ner:\n",
    "                berkeley_ner = EMPTY\n",
    "            ner_tally[berkeley_ner] += 1\n",
    "            pos_tally[berkeley_pos] += 1\n",
    "            \n",
    "            # look up items from the Stanford parser\n",
    "            stanford_word, _ = current_essay.sentences[sent_ix][word_ix]\n",
    "            stanford_pos = current_essay.pred_pos_tags_sentences[sent_ix][word_ix]\n",
    "            stanford_ner = current_essay.pred_ner_tags_sentences[sent_ix][word_ix]\n",
    "            assert stanford_word == berkeley_word\n",
    "            # UPDATE THE Essays with the Berkeley Parser Output\n",
    "            current_essay.pred_pos_tags_sentences[sent_ix][word_ix] = berkeley_pos\n",
    "            current_essay.pred_ner_tags_sentences[sent_ix][word_ix] = berkeley_ner\n",
    "            current_essay.pred_corefids[sent_ix][word_ix] = berkeley_set_corefids\n",
    "\n",
    "# make sure all essays are in the file\n",
    "assert len(essays) == len(processed_essays), \"Should have processed all essays\"\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/training_processed.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(870, 870)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berkeley_essays = list(name2essay.values()) \n",
    "#berkeley_essays = list(essays)\n",
    "# reload for comparison\n",
    "stanford_essays = get_essays(merged_predictions_folder, PARTITION)\n",
    "len(berkeley_essays), len(stanford_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(berkeley_essays) == len(stanford_essays)\n",
    "\n",
    "# assumes essays are in the same order\n",
    "num_words = 0\n",
    "diff_words = 0\n",
    "diff_pos = 0\n",
    "diff_ner = 0\n",
    "diff_corefs = 0\n",
    "\n",
    "for eix in range(len(stanford_essays)):\n",
    "    sessay = stanford_essays[eix]\n",
    "    bessay = berkeley_essays[eix]\n",
    "    assert sessay.name == bessay.name\n",
    "    assert sessay != bessay, \"Memory refs should differ\"\n",
    "#     print(sessay.name, bessay.name)\n",
    "    assert len(sessay.sentences) == len(bessay.sentences)\n",
    "    for sent_ix in range(len(sessay.sentences)):\n",
    "        ssentence = sessay.sentences[sent_ix]\n",
    "        bsentence = bessay.sentences[sent_ix]\n",
    "        assert len(ssentence) == len(bsentence)\n",
    "        for word_ix in range(len(ssentence)):\n",
    "            num_words += 1\n",
    "            \n",
    "            sword = ssentence[word_ix]\n",
    "            bword = bsentence[word_ix]\n",
    "            if sword != bword:\n",
    "                diff_words +=1\n",
    "            \n",
    "            spos = sessay.pred_pos_tags_sentences[sent_ix][word_ix]\n",
    "            bpos = bessay.pred_pos_tags_sentences[sent_ix][word_ix]\n",
    "            if spos != bpos:\n",
    "                diff_pos +=1\n",
    "            \n",
    "            sner = sessay.pred_ner_tags_sentences[sent_ix][word_ix]\n",
    "            bner = bessay.pred_ner_tags_sentences[sent_ix][word_ix]\n",
    "            if sner != bner:\n",
    "                diff_ner +=1\n",
    "            \n",
    "            scoref_set = sessay.pred_corefids[sent_ix][word_ix]\n",
    "            assert type(scoref_set) == set\n",
    "            bcoref_set = bessay.pred_corefids[sent_ix][word_ix]\n",
    "            assert type(scoref_set) == type(bcoref_set)\n",
    "            if bcoref_set != scoref_set:\n",
    "                diff_corefs +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145471, 0, 10453, 13009, 21626)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words, diff_words, diff_pos, diff_ner, diff_corefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145471, 0.0, 0.0718562462621416, 0.08942675859793361, 0.14866193261887248)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words, diff_words/num_words, diff_pos/num_words, diff_ner/num_words, diff_corefs/num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert diff_pos > 100 and diff_ner > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags match 90% of the time, but NER and corefids differ at least 99% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the parent folder if it does not exist\n",
    "import pathlib\n",
    "pathlib.Path(coref_folder).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/Berkeley/training_processed.dill Persisted\n"
     ]
    }
   ],
   "source": [
    "dill_fname = \"{folder}{partition}_processed.dill\".format(folder=coref_folder, partition=PARTITION.lower())\n",
    "with open(dill_fname, \"wb+\") as fout:\n",
    "    dill.dump(berkeley_essays, fout)\n",
    "print(dill_fname, \"Persisted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
