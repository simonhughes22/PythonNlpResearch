{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from Settings import Settings\n",
    "from window_based_tagger_config import get_config\n",
    "from FindFiles import find_files\n",
    "from DirUtils import dir_exists\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY_TAG = \"Empty\"\n",
    "ANAPHORA = \"Anaphor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.4 :: Anaconda, Inc.\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.3.0\n",
      "anaconda-client==1.6.11\n",
      "appnope==0.1.0\n",
      "argcomplete==1.9.4\n",
      "asn1crypto==0.24.0\n",
      "astor==0.7.1\n",
      "beautifulsoup4==4.6.0\n",
      "bleach==2.1.2\n",
      "boto==2.47.0\n",
      "boto3==1.5.36\n",
      "botocore==1.8.50\n",
      "bz2file==0.98\n",
      "certifi==2018.1.18\n",
      "cffi==1.11.4\n",
      "chardet==3.0.4\n",
      "clyent==1.2.2\n",
      "costcla==0.5\n",
      "cryptography==2.1.4\n",
      "cycler==0.10.0\n",
      "cymem==1.31.2\n",
      "cytoolz==0.8.2\n",
      "decorator==4.2.1\n",
      "dicecore==1.13\n",
      "dill==0.2.8.2\n",
      "docutils==0.14\n",
      "entrypoints==0.2.3\n",
      "ftfy==4.4.3\n",
      "gast==0.2.0\n",
      "gensim==0.13.4\n",
      "grpcio==1.14.0\n",
      "h5py==2.7.0\n",
      "hdbscan==0.8.12\n",
      "html5lib==1.0.1\n",
      "idna==2.6\n",
      "ipykernel==4.8.2\n",
      "ipython==6.2.1\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.1.2\n",
      "jedi==0.11.1\n",
      "Jinja2==2.10\n",
      "jmespath==0.9.3\n",
      "joblib==0.9.4\n",
      "jsonschema==2.6.0\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.2.2\n",
      "jupyter-console==5.2.0\n",
      "jupyter-core==4.4.0\n",
      "jupyterlab==0.31.8\n",
      "jupyterlab-launcher==0.10.5\n",
      "Keras==1.2.2\n",
      "Keras-Applications==1.0.4\n",
      "Keras-Preprocessing==1.0.2\n",
      "Markdown==2.6.11\n",
      "MarkupSafe==1.0\n",
      "matplotlib==2.0.0\n",
      "mistune==0.8.3\n",
      "murmurhash==0.26.4\n",
      "nb-anacondacloud==1.4.0\n",
      "nb-conda==2.2.1\n",
      "nb-conda-kernels==2.1.0\n",
      "nbconvert==5.3.1\n",
      "nbformat==4.4.0\n",
      "nbpresent==3.0.2\n",
      "nltk==3.2.2\n",
      "nose==1.3.7\n",
      "notebook==5.4.0\n",
      "numpy==1.14.1\n",
      "pandas==0.22.0\n",
      "pandocfilters==1.4.2\n",
      "parso==0.1.1\n",
      "pathlib==1.0.1\n",
      "pexpect==4.4.0\n",
      "pickleshare==0.7.4\n",
      "Pillow==5.0.0\n",
      "plac==0.9.6\n",
      "preshed==1.0.0\n",
      "prompt-toolkit==1.0.15\n",
      "protobuf==3.6.0\n",
      "ptyprocess==0.5.2\n",
      "pycparser==2.18\n",
      "pyea==0.2\n",
      "Pygments==2.2.0\n",
      "pymongo==3.4.0\n",
      "pyOpenSSL==17.5.0\n",
      "pyparsing==2.2.0\n",
      "PySocks==1.6.7\n",
      "python-dateutil==2.6.1\n",
      "pytz==2018.3\n",
      "PyYAML==3.12\n",
      "pyzmq==17.0.0\n",
      "qtconsole==4.3.1\n",
      "regex==2017.4.5\n",
      "requests==2.14.2\n",
      "s3transfer==0.1.13\n",
      "scikit-learn==0.18.1\n",
      "scikit-optimize==0.3\n",
      "scipy==1.0.0\n",
      "seaborn==0.7.1\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "six==1.11.0\n",
      "smart-open==1.5.6\n",
      "spacy==1.8.2\n",
      "tensorboard==1.9.0\n",
      "tensorflow==1.9.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.1\n",
      "testpath==0.3.1\n",
      "Theano==1.0.2\n",
      "thinc==6.5.2\n",
      "toolz==0.9.0\n",
      "tornado==4.5.3\n",
      "tqdm==4.19.5\n",
      "traitlets==4.3.2\n",
      "ujson==1.35\n",
      "urllib3==1.22\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.14.1\n",
      "widgetsnbextension==3.1.4\n",
      "wordcloud==1.3.1\n",
      "wrapt==1.10.11\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "DATASET = \"CoralBleaching\"\n",
    "#DATASET = \"SkinCancer\"\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "training_pickled = settings.data_directory + DATASET + \"/Thesis_Dataset/training.pl\"\n",
    "\n",
    "# PREDICTIONS FOLDERS\n",
    "#anaphor_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/\"\n",
    "anaphor_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/\"\n",
    "tag_predictions_folder = root_folder + \"Predictions/Bi-LSTM_fixed/\"\n",
    "#tag_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anaphor_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid\n"
     ]
    }
   ],
   "source": [
    "assert dir_exists(anaphor_predictions_folder)\n",
    "print(\"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls '/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-SEARN/'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid\n"
     ]
    }
   ],
   "source": [
    "assert dir_exists(tag_predictions_folder)\n",
    "print(\"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tagged essays\n",
    "def load_tagged_essays(folder):\n",
    "    files = find_files(folder, \"essay.*.dill\")\n",
    "    # multiple runs with different hidden layer sizes?\n",
    "    if len(files) > 2:        \n",
    "        files = find_files(folder, \"essays.*256.*.dill\")\n",
    "    for f in files:\n",
    "        print(f)\n",
    "    assert len(files) == 2, \"Wrong number of tagged files:\" + str(len(files))\n",
    "    for f in files:\n",
    "        assert \"_train_\" in f or \"_test_\" in f, \"Wrong files matched\"\n",
    "    train_tagged_fname = [f for file in files if \"_train_\" in f][0]\n",
    "    test_tagged_fname = list(set(files).difference([train_tagged_fname]))[0]\n",
    "\n",
    "    # NOTE - is this throws an error, upgrade to dill 2.8.2. Version 2.6 had a bug in it\n",
    "    with open(train_tagged_fname, \"rb\") as f:\n",
    "        tagged_essays_train = dill.load(f)\n",
    "    with open(test_tagged_fname, \"rb\") as f:\n",
    "        tagged_essays_test  = dill.load(f)\n",
    "    return (tagged_essays_train, tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana_tagged_tr, ana_tagged_test = load_tagged_essays(anaphor_predictions_folder)\n",
    "len(ana_tagged_tr), len(ana_tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-SEARN/essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/Bi-LSTM-4-SEARN/essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load concept code (cc) tagged essays\n",
    "cc_tagged_tr, cc_tagged_test = load_tagged_essays(tag_predictions_folder)\n",
    "len(cc_tagged_tr), len(cc_tagged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Essays (Untagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 files found\n",
      "226 essays processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do I need to do this? good for validation below, but not needed otherwise\n",
    "with open(training_pickled, \"rb+\") as f:\n",
    "    untagged_essays_train = pickle.load(f)\n",
    "\n",
    "test_config = get_config(test_folder)\n",
    "untagged_essays_test = load_process_essays(**test_config)\n",
    "\n",
    "len(untagged_essays_train), len(untagged_essays_test) # 902, 226"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate all essay sets are equal and the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_the_same(essay_sets):\n",
    "    unique_fnames = [] # list of sets of str (fnames)\n",
    "    for essay_collection in essay_sets:\n",
    "        names = set()\n",
    "        for e in essay_collection:\n",
    "            names.add(e.name)\n",
    "        unique_fnames.append(names)\n",
    "    for a in unique_fnames:\n",
    "        print(len(a))\n",
    "        for b in unique_fnames:\n",
    "            assert len(a) == len(b), \"lens don't match\"\n",
    "            assert a == b, \"don't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902\n",
      "902\n",
      "902\n"
     ]
    }
   ],
   "source": [
    "names_the_same([ana_tagged_tr, cc_tagged_tr, untagged_essays_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "226\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "names_the_same([ana_tagged_test, cc_tagged_test, untagged_essays_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essays_2_hash_map(essays):\n",
    "    lu = {}\n",
    "    for e in essays:\n",
    "        lu[e.name] = e\n",
    "    return lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 902 essays\n",
      "Validation Passed\n",
      "Validating 902 essays\n",
      "Validation Passed\n",
      "Validating 226 essays\n",
      "Validation Passed\n",
      "Validating 226 essays\n",
      "Validation Passed\n"
     ]
    }
   ],
   "source": [
    "# checks the number of words and sentences are the same for 2 sets of essays\n",
    "def validate_tagged_essays(essays_a, essays_b):\n",
    "    # make sure obj is not the same\n",
    "    assert essays_a != essays_b\n",
    "    print(\"Validating\", len(essays_a), \"essays\")\n",
    "    assert len(essays_a) == len(essays_b), \"Lens don't match\"\n",
    "    \n",
    "    a_hmap = essays_2_hash_map(essays_a)\n",
    "    b_hmap = essays_2_hash_map(essays_b)\n",
    "    \n",
    "    # same essays?\n",
    "    assert a_hmap.keys() == b_hmap.keys()\n",
    "    intersect = set(a_hmap.keys()).intersection(b_hmap.keys())\n",
    "    assert len(intersect) == len(a_hmap.keys())\n",
    "    assert len(a_hmap.keys()) > 1    \n",
    "    assert len(a_hmap.keys()) == len(b_hmap.keys())\n",
    "    \n",
    "    for key, a_essay in a_hmap.items():\n",
    "        b_essay = b_hmap[key]\n",
    "        # assert NOT the same obj ref\n",
    "        assert a_essay != b_essay\n",
    "        assert len(a_essay.sentences) == len(b_essay.sentences)\n",
    "        assert len(a_essay.sentences) > 0\n",
    "        assert len(b_essay.sentences) > 0\n",
    "        for i in range(len(a_essay.sentences)):\n",
    "            a_sent = a_essay.sentences[i]\n",
    "            b_sent = b_essay.sentences[i]\n",
    "            # the same lists?\n",
    "            assert a_sent == b_sent\n",
    "            assert len(a_sent) == len(b_sent)\n",
    "            if not len(a_sent) == len(b_sent):\n",
    "                print(key, \"\\tsent-ix:\", i, \"lens\", len(a_sent), len(b_sent))\n",
    "            for wd_ix, (a_wd, a_tags) in enumerate(a_sent):\n",
    "                b_wd, b_tags = b_sent[wd_ix]\n",
    "                assert a_wd   == b_wd,   \"Words don't match: {a} - {b}\".format(a=a_wd, b=b_wd)\n",
    "                assert a_tags == b_tags, \"Tags don't match: {a} - {b}\".format(a=str(a_tags), b=str(b_tags))\n",
    "                \n",
    "    print(\"Validation Passed\")\n",
    "    return None\n",
    "\n",
    "validate_tagged_essays(untagged_essays_train, ana_tagged_tr)\n",
    "validate_tagged_essays(cc_tagged_tr, ana_tagged_tr)\n",
    "validate_tagged_essays(untagged_essays_test, ana_tagged_test)\n",
    "validate_tagged_essays(cc_tagged_test, ana_tagged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Two Sets of Tagged Essays\n",
    "* The anaphora essays were not tagged with concept codes, and vice versa, so need to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tagged_essays(tagged_ana, tagged_cc):\n",
    "    ana_hmap = essays_2_hash_map(tagged_ana)\n",
    "    cc_hmap = essays_2_hash_map(tagged_cc)\n",
    "    \n",
    "    assert ana_hmap.keys() == cc_hmap.keys()\n",
    "    assert len(ana_hmap.keys()) == len(cc_hmap.keys())\n",
    "    \n",
    "    for key, cc_essay in cc_hmap.items():\n",
    "        ana_essay = ana_hmap[key]\n",
    "        assert len(ana_essay.sentences) == len(cc_essay.sentences)\n",
    "        cc_essay.ana_tagged_sentences = ana_essay.pred_tagged_sentences\n",
    "        for i in range(len(ana_essay.sentences)):\n",
    "            ana_sent = ana_essay.sentences[i]\n",
    "            ana_ptags = ana_essay.pred_tagged_sentences[i]\n",
    "            cc_sent = ana_essay.sentences[i]\n",
    "            cc_ptags = cc_essay.pred_tagged_sentences[i]\n",
    "            assert len(ana_sent) == len(cc_sent)\n",
    "            assert len(cc_sent) == len(cc_ptags)\n",
    "    return tagged_cc\n",
    "\n",
    "merged_essays_tr   = merge_tagged_essays(tagged_ana=ana_tagged_tr, tagged_cc=cc_tagged_tr)\n",
    "merged_essays_test = merge_tagged_essays(tagged_ana=ana_tagged_test, tagged_cc=cc_tagged_test)\n",
    "len(merged_essays_tr), len(merged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Compute Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_tally(tally):\n",
    "    total = sum(tally.values())\n",
    "    norm_tally = {}\n",
    "    for tag, freq in tally.items():\n",
    "        norm_tally[tag] = freq/total\n",
    "    return norm_tally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '5b', '6', '7', '11', '12', '13', '14', '50']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally = defaultdict(int)\n",
    "for e in untagged_essays_train:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                tally[t] +=1\n",
    "\n",
    "lst_all_tags = list(tally.keys())\n",
    "regular_tags = sorted(set((t for t in lst_all_tags if t[0].isdigit())), key = lambda s: int(s.replace('b','')))\n",
    "assert EMPTY_TAG not in regular_tags, \"Empty tag in list of regular tags\"\n",
    "regular_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Label Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 0.11969692414083406),\n",
       " ('11', 0.02522625454764124),\n",
       " ('12', 0.021768543852791724),\n",
       " ('13', 0.052316666165549176),\n",
       " ('14', 0.06329113924050633),\n",
       " ('2', 0.024444511260110047),\n",
       " ('3', 0.14014251781472684),\n",
       " ('4', 0.05258726961123305),\n",
       " ('5', 0.017348687573288432),\n",
       " ('50', 0.3345861270633513),\n",
       " ('5b', 0.01819056495986049),\n",
       " ('6', 0.037403409603415615),\n",
       " ('7', 0.09299738416669172)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_tally = dict([k,v] for k,v in tally.items() if k in set(regular_tags))\n",
    "sorted(norm_tally(cc_tally).items(), key = lambda tpl: tpl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 0.11399586815084332),\n",
       " ('11', 0.02596281335758996),\n",
       " ('12', 0.02235515401930252),\n",
       " ('13', 0.046282877493755974),\n",
       " ('14', 0.060096820942924976),\n",
       " ('2', 0.027257870555949554),\n",
       " ('3', 0.15050414726650427),\n",
       " ('4', 0.05920261478215288),\n",
       " ('5', 0.02476026024482748),\n",
       " ('50', 0.33699238383028585),\n",
       " ('5b', 0.005981930868613363),\n",
       " ('6', 0.037217477105238816),\n",
       " ('7', 0.08938978138201104)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptag_tally = defaultdict(int)\n",
    "for e in merged_essays_tr:   \n",
    "    for sent in e.pred_tagged_sentences:\n",
    "        for tag in sent:\n",
    "            ptag_tally[tag] +=1\n",
    "            \n",
    "norm_ptag_tally = norm_tally(dict([(k,v) for k,v in ptag_tally.items() if k != EMPTY_TAG]))\n",
    "sorted(norm_ptag_tally.items(), key = lambda tpl: tpl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Empty',\n",
       " '50',\n",
       " '4',\n",
       " '11',\n",
       " '7',\n",
       " '3',\n",
       " '1',\n",
       " '6',\n",
       " '14',\n",
       " '13',\n",
       " '5',\n",
       " '5b',\n",
       " '2',\n",
       " '12']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ptag_tally.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predicted and Actual Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(tag, expected_tag_set):\n",
    "    if tag in expected_tag_set:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_wd_level_lbs(essays, expected_tags):\n",
    "    expected_tags = set(expected_tags)\n",
    "    ysbycode = defaultdict(list)\n",
    "    for e in essays:\n",
    "        for sent in e.sentences:\n",
    "            for wd, tag_set in sent:\n",
    "                for etag in expected_tags:\n",
    "                    ysbycode[etag].append(get_label(etag, tag_set))\n",
    "    return ysbycode    \n",
    "\n",
    "# for pred tags\n",
    "def get_wd_level_preds(essays, expected_tags):\n",
    "    expected_tags = set(expected_tags)\n",
    "    ysbycode = defaultdict(list)\n",
    "    for e in essays:\n",
    "        for sentix in range(len(e.sentences)):\n",
    "            p_ccodes = e.pred_tagged_sentences[sentix]\n",
    "            p_ana_tags = e.ana_tagged_sentences[sentix]\n",
    "            assert len(p_ccodes) == len(p_ana_tags), \"Different length predictions\"\n",
    "            for wordix in range(len(p_ccodes)):\n",
    "                ptag_set = set([p_ccodes[wordix], p_ana_tags[wordix]])\n",
    "                assert len(ptag_set) >=1, \"No tags found\"\n",
    "                for exp_tag in expected_tags:\n",
    "                    ysbycode[exp_tag].append(get_label(exp_tag, ptag_set))    \n",
    "    return ysbycode   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Metrics on Concept Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 137166 13 137166\n"
     ]
    }
   ],
   "source": [
    "#TODO - get predicted ccodes and anaphora labels, merge into one set of preds and filter by expected_tags.\n",
    "# this give flexibility to look at anaphora, cc or both\n",
    "act_cc_ys_bycode = get_wd_level_lbs(merged_essays_tr, regular_tags)\n",
    "pred_cc_ys_bycode = get_wd_level_preds(merged_essays_tr, regular_tags)\n",
    "\n",
    "assert len(act_cc_ys_bycode.keys()) == len(pred_cc_ys_bycode.keys()) == len(regular_tags), \"Miss-matched codes\"\n",
    "assert len(act_cc_ys_bycode[\"50\"]) == len(pred_cc_ys_bycode[\"50\"]), \"Different numbers of words\"\n",
    "\n",
    "print(len(act_cc_ys_bycode), len(act_cc_ys_bycode[\"50\"]), len(pred_cc_ys_bycode), len(pred_cc_ys_bycode[\"50\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': Recall: 0.7913, Precision: 0.8520, F1: 0.8205, Accuracy: 0.9900, Codes:  3981,\n",
       " '11': Recall: 0.9094, Precision: 0.9062, F1: 0.9078, Accuracy: 0.9989, Codes:   839,\n",
       " '12': Recall: 0.9047, Precision: 0.9034, F1: 0.9041, Accuracy: 0.9990, Codes:   724,\n",
       " '13': Recall: 0.7063, Precision: 0.8188, F1: 0.7584, Accuracy: 0.9943, Codes:  1740,\n",
       " '14': Recall: 0.7487, Precision: 0.8086, F1: 0.7775, Accuracy: 0.9934, Codes:  2105,\n",
       " '2': Recall: 0.6654, Precision: 0.6120, F1: 0.6376, Accuracy: 0.9955, Codes:   813,\n",
       " '3': Recall: 0.7747, Precision: 0.7398, F1: 0.7569, Accuracy: 0.9831, Codes:  4661,\n",
       " '4': Recall: 0.8319, Precision: 0.7578, F1: 0.7931, Accuracy: 0.9945, Codes:  1749,\n",
       " '5': Recall: 0.4575, Precision: 0.3288, F1: 0.3826, Accuracy: 0.9938, Codes:   577,\n",
       " '50': Recall: 0.8776, Precision: 0.8936, F1: 0.8855, Accuracy: 0.9816, Codes: 11128,\n",
       " '5b': Recall: 0.1273, Precision: 0.3969, F1: 0.1927, Accuracy: 0.9953, Codes:   605,\n",
       " '6': Recall: 0.8521, Precision: 0.8782, F1: 0.8650, Accuracy: 0.9976, Codes:  1244,\n",
       " '7': Recall: 0.7401, Precision: 0.7896, F1: 0.7640, Accuracy: 0.9897, Codes:  3093}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ResultsProcessor.compute_metrics(act_cc_ys_bycode, pred_cc_ys_bycode)\n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(act_cc_ys_bycode, pred_cc_ys_bycode)\n",
    "#metrics.update(mean_metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.992811629704154,\n",
       " 'data_points': 1783158.0,\n",
       " 'f1_score': 0.8048713655046431,\n",
       " 'num_codes': 33259.0,\n",
       " 'precision': 0.8151460022817675,\n",
       " 'recall': 0.7948525211221023}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics[__MICRO_F1__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate Metrics on Anaphora Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 137166 1 137166\n"
     ]
    }
   ],
   "source": [
    "#TODO - get predicted ccodes and anaphora labels, merge into one set of preds and filter by expected_tags.\n",
    "# this give flexibility to look at anaphora, cc or both\n",
    "ana_tags = [ANAPHORA]\n",
    "\n",
    "act_ana_ys_bycode  = get_wd_level_lbs(merged_essays_tr, ana_tags)\n",
    "pred_ana_ys_bycode = get_wd_level_preds(merged_essays_tr, ana_tags)\n",
    "\n",
    "assert len(act_ana_ys_bycode.keys()) == len(pred_ana_ys_bycode.keys()) == len(ana_tags), \"Miss-matched codes\"\n",
    "assert len(act_ana_ys_bycode[ANAPHORA]) == len(pred_ana_ys_bycode[ANAPHORA]), \"Different numbers of words\"\n",
    "\n",
    "print(len(act_ana_ys_bycode), len(act_ana_ys_bycode[ANAPHORA]), len(pred_ana_ys_bycode), len(pred_ana_ys_bycode[ANAPHORA]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Anaphor': Recall: 0.2059, Precision: 0.3804, F1: 0.2672, Accuracy: 0.9972, Codes:   340}\n"
     ]
    }
   ],
   "source": [
    "metrics_ana = ResultsProcessor.compute_metrics(act_ana_ys_bycode, pred_ana_ys_bycode)\n",
    "mean_metrics_ana = ResultsProcessor.compute_mean_metrics(act_ana_ys_bycode, pred_ana_ys_bycode)\n",
    "#metrics.update(mean_metrics)\n",
    "print(metrics_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9972004724202791,\n",
       " 'data_points': 137166.0,\n",
       " 'f1_score': 0.26717557251908397,\n",
       " 'num_codes': 340.0,\n",
       " 'precision': 0.3804347826086957,\n",
       " 'recall': 0.20588235294117646}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics_ana[__MICRO_F1__]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
