{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    " - Take the merged predictions and evaluate the prediction accuracy using the 2 different approaches\n",
    " 1. Look at the anaphora tags and then cross-reference co-reference labels\n",
    " 2. Use the co-reference chains directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "from FindFiles import find_files\n",
    "from Settings import Settings\n",
    "from CoRefHelper import EMPTY\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "DATASET = \"CoralBleaching\" # CoralBleaching | SkinCancer\n",
    "PARTITION = \"Training\" # Training | Test\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "merged_predictions_folder = root_folder + \"CoReference/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_files = find_files(merged_predictions_folder)\n",
    "if PARTITION == \"Training\":\n",
    "    essay_files = [e for e in essay_files if \"train\" in e]\n",
    "assert len(essay_files) == 1\n",
    "with open(essay_files[0], \"rb\") as f:\n",
    "    essays = dill.load(f)\n",
    "len(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/training_processed.dill']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_essays(essays):\n",
    "    for e in essays:    \n",
    "        # map coref ids to sent_ix, wd_ix tuples\n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            sent     = e.sentences[sent_ix]\n",
    "            ana_tags = e.ana_tagged_sentences[sent_ix]\n",
    "            coref_ids= e.pred_corefids[sent_ix]\n",
    "            ner_tags = e.pred_ner_tags_sentences[sent_ix]\n",
    "            pos_tags = e.pred_pos_tags_sentences[sent_ix]\n",
    "            ptags    = e.pred_tagged_sentences[sent_ix]\n",
    "\n",
    "            assert len(sent) == len(coref_ids)\n",
    "\n",
    "            assert len(sent) == len(ana_tags) == len(coref_ids) == len(ner_tags) == len(pos_tags) == len(ptags),\\\n",
    "                (len(sent), len(ana_tags), len(coref_ids), len(ner_tags), len(pos_tags), len(ptags), e.name, sent_ix)\n",
    "            assert len(sent) > 0\n",
    "            \n",
    "validate_essays(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_essay_attributes(essays, attribute_name=\"pred_pos_tags_sentences\"):\n",
    "    tally = defaultdict(int)\n",
    "    for e in essays:\n",
    "        nested_list = getattr(e, attribute_name)\n",
    "        for lst in nested_list:\n",
    "            for item in lst:\n",
    "                if type(item) == str:\n",
    "                    tally[item] +=1\n",
    "                elif type(item) == set:\n",
    "                    for i in item:\n",
    "                        tally[i] +=1\n",
    "                else:\n",
    "                    raise Exception(\"Unexpected item type\")\n",
    "    return tally\n",
    "\n",
    "def pivot_words_by_essay_attributes(essays, attribute_name=\"pred_pos_tags_sentences\"):\n",
    "    nested_tally = defaultdict(lambda : defaultdict(int))\n",
    "    for e in essays:\n",
    "        nested_list = getattr(e, attribute_name)\n",
    "        for sent_ix, lst in enumerate(nested_list):\n",
    "            sentence = e.sentences[sent_ix]\n",
    "            wds, tags = zip(*sentence)\n",
    "            for wd_ix, item in enumerate(lst):\n",
    "                wd = wds[wd_ix]\n",
    "                if type(item) == str:\n",
    "                    nested_tally[item][wd] +=1\n",
    "                elif type(item) == set:\n",
    "                    for i in item:\n",
    "                        tally[i][wd] +=1\n",
    "                else:\n",
    "                    raise Exception(\"Unexpected item type\")\n",
    "    return nested_tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tally = tally_essay_attributes(essays, attribute_name=\"pred_ner_tags_sentences\")\n",
    "pos_tally = tally_essay_attributes(essays, attribute_name=\"pred_pos_tags_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Anaphor Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[13]',\n",
       " 'Anaphor:[14]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[5b]',\n",
       " 'Anaphor:[6]',\n",
       " 'Anaphor:[7]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "cc_tally = defaultdict(int)\n",
    "cr_tally = defaultdict(int)\n",
    "reg_tally = defaultdict(int)\n",
    "for e in essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"->\" in t:\n",
    "                        cr_tally[t] += 1\n",
    "                    elif \"Anaphor:[\" in t:\n",
    "                        cc_tally[t] += 1\n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "all_ana_tags = sorted(cc_tally.keys())\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_ana_tags), len(reg_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chain(e):\n",
    "    \"\"\" Takes an essay object, and creats a map of Dict[str, List[Tuple{int,int}]]\n",
    "        which maps a coref id (essay scope) to a list of (sent_ix,wd_ix) pairs\n",
    "    \"\"\"\n",
    "    corefid_2_chain = defaultdict(list)\n",
    "    for sent_ix in range(len(e.sentences)):\n",
    "        sent     = e.sentences[sent_ix]\n",
    "        coref_ids= e.pred_corefids[sent_ix]\n",
    "        for wd_ix in range(len(sent)):\n",
    "            wd_coref_ids = coref_ids[wd_ix] # Set[str]\n",
    "            for cr_id in wd_coref_ids:\n",
    "                pair = (sent_ix, wd_ix)\n",
    "                corefid_2_chain[cr_id].append(pair)\n",
    "    return corefid_2_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\t [(0, 23), (0, 24), (0, 25)]\n",
      "2\n",
      "\t [(7, 0), (7, 1)]\n",
      "\t [(7, 8)]\n",
      "3\n",
      "\t [(2, 0), (2, 1)]\n",
      "\t [(3, 2)]\n",
      "\t [(3, 9)]\n"
     ]
    }
   ],
   "source": [
    "def build_segmented_chain(e):\n",
    "    \"\"\" Takes an essay object, and creats a map of Dict[str, List[List[Tuple{int,int}]]\n",
    "        which maps a coref id (essay scope) to a nested list of (sent_ix,wd_ix) pairs.\n",
    "        The nested list has a separate inner list for every distinct coreference seq/phrase\n",
    "    \"\"\"\n",
    "\n",
    "    corefid_2_chain = build_chain(e)\n",
    "    corefid_2_segmented_chain = dict()\n",
    "    for cref, pairs in corefid_2_chain.items():\n",
    "        segmented = [[pairs[0]]]\n",
    "        corefid_2_segmented_chain[cref] = segmented\n",
    "        last_sent_ix, last_wd_ix = pairs[0]\n",
    "        for pair in pairs[1:]:\n",
    "            sent_ix, wd_ix = pair\n",
    "            if sent_ix != last_sent_ix or (wd_ix - last_wd_ix) > 1:\n",
    "                # create a new nested list\n",
    "                segmented.append([])\n",
    "            # append pair to last list item\n",
    "            segmented[-1].append(pair)        \n",
    "            last_sent_ix, last_wd_ix = pair\n",
    "    return corefid_2_segmented_chain\n",
    "\n",
    "corefid_2_segmented_chain = build_segmented_chain(essays[2])\n",
    "for cref, seg_chain in sorted(corefid_2_segmented_chain.items()):\n",
    "    print(cref)\n",
    "    for lst in seg_chain:\n",
    "        print(\"\\t\", str(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processessays import Essay\n",
    "import warnings\n",
    "\n",
    "def get_ana_tagged_essays(essays, format_ana_tags=True, filter_to_predicted_tags=True, look_back_only=True,\n",
    "                         max_cref_phrase_len=None, ner_filter=None, pos_filter=None\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Create a copy of essays, augmenting the pred_tagged_sentences object with additional anaphora tags\n",
    "    \n",
    "    essays:                   List[Essay] objects - merged tagged essays\n",
    "    format_ana_tags:          bool - Add ana tags as Anaphor[xyz] or as just the regular concept codes\n",
    "    filter_to_predicted_tags: bool - Filter to just the predicted anaphor tags\n",
    "    look_back_only:           bool - Only look to coreferences occuring earlier in the essay\n",
    "    max_cref_phrase_len:      Union(int,None) - if specified, maximum coreference length to consider\n",
    "    ner_filter:               Union(Set[str],None) - if specified, filters to words with one of those NER tags\n",
    "    pos_filter:               Union(Set[str],None) - if specified, filters to words with one of those POS tags\n",
    "    \"\"\"\n",
    "    if ner_filter and EMPTY in ner_filter:\n",
    "            warnings.warn(\"EMPTY tag in NER filter \", UserWarning)\n",
    "    if pos_filter and EMPTY in pos_filter:\n",
    "            warnings.warn(\"EMPTY tag in POS filter \", UserWarning)\n",
    "    \n",
    "    ana_tagged_essays = []\n",
    "    for eix, e in enumerate(essays):\n",
    "\n",
    "        ana_tagged_e = Essay(e.name, e.sentences)\n",
    "        ana_tagged_e.pred_tagged_sentences = []\n",
    "        ana_tagged_essays.append(ana_tagged_e)\n",
    "\n",
    "        # map coref ids to sent_ix, wd_ix tuples\n",
    "        corefid_2_chain = build_segmented_chain(e)\n",
    "\n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            ana_tagged_sent = []\n",
    "            ana_tagged_e.pred_tagged_sentences.append(ana_tagged_sent)\n",
    "\n",
    "            sent     = e.sentences[sent_ix]\n",
    "            ana_tags = e.ana_tagged_sentences[sent_ix]\n",
    "            coref_ids= e.pred_corefids[sent_ix]\n",
    "            ner_tags = e.pred_ner_tags_sentences[sent_ix]\n",
    "            pos_tags = e.pred_pos_tags_sentences[sent_ix]\n",
    "            ptags    = e.pred_tagged_sentences[sent_ix]    \n",
    "\n",
    "            for wd_ix in range(len(sent)):                            \n",
    "                pos_tag = pos_tags[wd_ix] # NER tag                \n",
    "                ner_tag = ner_tags[wd_ix] # POS tag\n",
    "                    \n",
    "                word, _ = sent[wd_ix] # ignore actual tags\n",
    "                pred_cc_tag = ptags[wd_ix] # predict cc tag\n",
    "                \n",
    "                is_ana_tag = ana_tags[wd_ix] == ANAPHORA\n",
    "                wd_coref_ids = coref_ids[wd_ix] # Set[str]\n",
    "\n",
    "                wd_ptags = set()\n",
    "                # add predicted concept code tag (filtered out by evaluation code, which filters to specific tags)\n",
    "                if pred_cc_tag != EMPTY:\n",
    "                    wd_ptags.add(pred_cc_tag)\n",
    "\n",
    "                ana_tagged_sent.append(wd_ptags)\n",
    "                if len(wd_coref_ids) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get codes for corresponding co-ref chain entries\n",
    "                if ((filter_to_predicted_tags and is_ana_tag) or not filter_to_predicted_tags):                        \n",
    "                    \n",
    "                    for cr_id in wd_coref_ids:                        \n",
    "                        segmented_chain = corefid_2_chain[cr_id]\n",
    "                        for cref_phrase in segmented_chain: # iterate thru the list of sent_ix,wd_ix's\n",
    "                                                            # in 1 cref phrase\n",
    "                            \n",
    "                            # FILTERS\n",
    "                            # LENGTH FILTER\n",
    "                            if max_cref_phrase_len and len(cref_phrase) > max_cref_phrase_len:\n",
    "                                continue\n",
    "                            # POS FILTER\n",
    "                            if pos_filter and pos_tag not in pos_filter:\n",
    "                                continue\n",
    "                            # NER TAG TYPE FILTER\n",
    "                            if ner_filter and ner_tag not in ner_filter:\n",
    "                                continue\n",
    "                                \n",
    "                            for ch_sent_ix, ch_wd_ix in cref_phrase:\n",
    "                                # if it's the current word, skip\n",
    "                                if ch_sent_ix == sent_ix and ch_wd_ix == wd_ix:\n",
    "                                    continue\n",
    "                                # for anaphors only - only look at chain ixs before the current word\n",
    "                                # if's it's after the current word in the essay, skip\n",
    "                                if look_back_only:\n",
    "                                    # sentence later in the essay, or same sentence but word is after current word\n",
    "                                    if ch_sent_ix > sent_ix or \\\n",
    "                                      (ch_sent_ix == sent_ix and ch_wd_ix >= wd_ix):\n",
    "                                        continue\n",
    "\n",
    "                                chain_ptag = e.pred_tagged_sentences[ch_sent_ix][ch_wd_ix]\n",
    "                                if chain_ptag != EMPTY:\n",
    "                                    code = chain_ptag\n",
    "                                    if format_ana_tags:\n",
    "                                        code =  \"{anaphora}:[{code}]\".format(\n",
    "                                            anaphora=ANAPHORA, code=chain_ptag)\n",
    "                                    wd_ptags.add(code)\n",
    "    # validation check    \n",
    "    #   check essay and sent lengths align\n",
    "    for e in ana_tagged_essays:\n",
    "        assert len(e.sentences) == len(e.pred_tagged_sentences)\n",
    "        for ix in range(len(e.sentences)):\n",
    "            assert len(e.sentences[ix]) == len(e.pred_tagged_sentences[ix])\n",
    "    return ana_tagged_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results_procesor import ResultsProcessor\n",
    "\n",
    "# Modify this function from the Resultsprocessor so that it works with Set[str] of predicted tags \n",
    "# as well as scalar strings\n",
    "def get_wd_level_preds(essays, expected_tags):\n",
    "    expected_tags = set(expected_tags)\n",
    "    ysbycode = defaultdict(list)\n",
    "    for e in essays:\n",
    "        for sentix in range(len(e.sentences)):\n",
    "            p_ccodes = e.pred_tagged_sentences[sentix]\n",
    "            for wordix in range(len(p_ccodes)):\n",
    "                tags = p_ccodes[wordix]\n",
    "                if type(tags) == str:\n",
    "                    ptag_set = {tags}\n",
    "                elif type(tags) in (set,list):\n",
    "                    ptag_set = set(tags)   \n",
    "                else:\n",
    "                    raise Exception(\"Unrecognized tag type\")\n",
    "                for exp_tag in expected_tags:\n",
    "                    ysbycode[exp_tag].append(ResultsProcessor._ResultsProcessor__get_label_(exp_tag, ptag_set))\n",
    "    return ysbycode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results_procesor import metrics_to_df\n",
    "\n",
    "def get_df(mean_metrics):\n",
    "    df = metrics_to_df(mean_metrics)\n",
    "    df = df[[\"code\",\"recall\",\"precision\",\"f1_score\",\"data_points\"]]\n",
    "    df = df.sort_values(\"code\")\n",
    "    return df[~df.code.str.contains(\"MEAN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(essays, expected_tags, micro_only=False):\n",
    "    act_ys_bycode  = ResultsProcessor.get_wd_level_lbs(essays,  expected_tags=expected_tags)\n",
    "    pred_ys_bycode = get_wd_level_preds(essays, expected_tags=expected_tags)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "    df = get_df(mean_metrics)\n",
    "    if micro_only:\n",
    "        df = df[df.code == \"MICRO_F1\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map CoRefs To Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_backwd_tagged_essays = get_ana_tagged_essays(essays, look_back_only=True)\n",
    "ana_both_tagged_essays = get_ana_tagged_essays(essays, look_back_only=False)\n",
    "# map new tags to existing labels, not anaphora labels\n",
    "collapsed_ana_tagged_essays = get_ana_tagged_essays(essays, format_ana_tags=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't filter to anaphora tags\n",
    "cref_tagged_essays           = get_ana_tagged_essays(essays,\n",
    "                                                     filter_to_predicted_tags=False, \n",
    "                                                     look_back_only=True) # worse if both directions\n",
    "collapsed_cref_tagged_essays = get_ana_tagged_essays(essays, \n",
    "                                                     format_ana_tags=False,\n",
    "                                                     filter_to_predicted_tags=False, \n",
    "                                                     look_back_only=True) # worse if both directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Accuracy using Anaphora Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Accuracy on Anaphora Tags Only - Word Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look Backwards Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.020349   0.241379  0.037534    1783158.0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "get_metrics(ana_backwd_tagged_essays , all_ana_tags,  micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.020349   0.241379  0.037534    1783158.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lopok backwards only (assume anaphora references are prior to anaphor)\n",
    "get_metrics(ana_backwd_tagged_essays, all_ana_tags,  micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look Forwards and Backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.036458</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.020349      0.175  0.036458    1783158.0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look forward and backward (lower as expected - note that this only lowers precision, \n",
    "#   recall is the same so all backwards)\n",
    "get_metrics(ana_both_tagged_essays, all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy with No Anaphora Tagging (i.e. Baseline Regular CC Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.820049</td>\n",
       "      <td>0.846703</td>\n",
       "      <td>0.833163</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.820049   0.846703  0.833163    1783158.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(essays, expected_tags=reg_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy with Anaphora Tagging (Regular Concept Codes) - Maps Anaphora[xyz] to Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.820049</td>\n",
       "      <td>0.84602</td>\n",
       "      <td>0.832832</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.820049    0.84602  0.832832    1783158.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(collapsed_ana_tagged_essays, expected_tags=reg_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For CB Training Data, it Mildly Hurts the F1 Score (very slighly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Accuracy Using Cref Predictions Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Accuracy on Anaphora Tags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Anaphor:[11]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Anaphor:[12]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Anaphor:[13]</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Anaphor:[14]</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anaphor:[1]</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.017107</td>\n",
       "      <td>0.030942</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Anaphor:[2]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anaphor:[3]</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anaphor:[4]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anaphor:[50]</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.006152</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anaphor:[5]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Anaphor:[5b]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Anaphor:[6]</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anaphor:[7]</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>0.011034</td>\n",
       "      <td>137166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MACRO_F1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.107558</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code    recall  precision  f1_score  data_points\n",
       "11  Anaphor:[11]  0.666667   0.024096  0.046512     137166.0\n",
       "12  Anaphor:[12]  0.000000   0.000000  0.000000     137166.0\n",
       "8   Anaphor:[13]  0.064516   0.012903  0.021505     137166.0\n",
       "9   Anaphor:[14]  0.178571   0.022222  0.039526     137166.0\n",
       "0    Anaphor:[1]  0.161765   0.017107  0.030942     137166.0\n",
       "6    Anaphor:[2]  0.000000   0.000000  0.000000     137166.0\n",
       "2    Anaphor:[3]  0.025641   0.002564  0.004662     137166.0\n",
       "7    Anaphor:[4]  0.000000   0.000000  0.000000     137166.0\n",
       "3   Anaphor:[50]  0.204545   0.003123  0.006152     137166.0\n",
       "4    Anaphor:[5]  0.000000   0.000000  0.000000     137166.0\n",
       "5   Anaphor:[5b]  0.000000   0.000000  0.000000     137166.0\n",
       "10   Anaphor:[6]  0.055556   0.002793  0.005319     137166.0\n",
       "1    Anaphor:[7]  0.072727   0.005970  0.011034     137166.0\n",
       "18      MACRO_F1       NaN        NaN       NaN          NaN\n",
       "17      MICRO_F1  0.107558   0.006188  0.011703    1783158.0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(cref_tagged_essays, expected_tags=all_ana_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy with No Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.820049</td>\n",
       "      <td>0.846703</td>\n",
       "      <td>0.833163</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.820049   0.846703  0.833163    1783158.0"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(essays, expected_tags=reg_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy with Anaphora Tagging (Regular Concept Codes) - Maps Anaphora[xyz] to Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.826212</td>\n",
       "      <td>0.775586</td>\n",
       "      <td>0.800099</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.826212   0.775586  0.800099    1783158.0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(collapsed_cref_tagged_essays, expected_tags=reg_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quite a Negative Impact on F1 Score overall (CB Train - drops from .83 to 0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Additional Filters to Improve Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Length Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.020349   0.241379  0.037534    1783158.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "get_metrics(ana_backwd_tagged_essays, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.002907   0.333333  0.005764    1783158.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision is higher, but overall F1 is lower as recall dropped a lot\n",
    "get_metrics(get_ana_tagged_essays(essays, max_cref_phrase_len=1), expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.107558</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.107558   0.006188  0.011703    1783158.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "get_metrics(cref_tagged_essays, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.026163</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.019694</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.026163   0.015789  0.019694    1783158.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Filter to single word references - better than with no length filter \n",
    "#  Note - this is still backwards looking\n",
    "cref_tagged_essays_maxph_1 = get_ana_tagged_essays(essays, filter_to_predicted_tags=False, max_cref_phrase_len=1)\n",
    "get_metrics(cref_tagged_essays_maxph_1, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.06686</td>\n",
       "      <td>0.00726</td>\n",
       "      <td>0.013098</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code   recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.06686    0.00726  0.013098    1783158.0"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length filter = 2 - worse than for length = 1\n",
    "cref_tagged_essays_maxph_2 = get_ana_tagged_essays(essays, filter_to_predicted_tags=False, max_cref_phrase_len=2)\n",
    "get_metrics(cref_tagged_essays_maxph_2, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try NER Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CAUSE_OF_DEATH', 649),\n",
       " ('TITLE', 149),\n",
       " ('NUMBER', 1131),\n",
       " ('DATE', 828),\n",
       " ('PERCENT', 328),\n",
       " ('ORDINAL', 103),\n",
       " ('SET', 301),\n",
       " ('COUNTRY', 127),\n",
       " ('DURATION', 200)]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,v) for k,v in ner_tally.items() if v > 100 and k != EMPTY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_filter = {\"CAUSE_OF_DEATH\", \"TITLE\", \"SET\", \"DURATION\"}\n",
    "assert len(ner_filter.intersection(ner_tally.keys())) == len(ner_filter), \"Bad NER filter entry(ies)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CAUSE_OF_DEATH',\n",
      " [('storms', 280),\n",
      "  ('starvation', 125),\n",
      "  ('disease', 108),\n",
      "  ('hurricanes', 41),\n",
      "  ('storm', 35),\n",
      "  ('diseases', 21),\n",
      "  ('hurricane', 9),\n",
      "  ('natural', 8),\n",
      "  ('INFREQUENT', 7),\n",
      "  ('disaster', 4),\n",
      "  ('disasters', 4),\n",
      "  ('causes', 2),\n",
      "  ('cause', 2),\n",
      "  ('lightning', 2),\n",
      "  ('carbon', 1)])\n",
      "\n",
      "('TITLE',\n",
      " [('cold', 65),\n",
      "  ('marine', 33),\n",
      "  ('merchant', 17),\n",
      "  ('biologist', 14),\n",
      "  ('INFREQUENT', 9),\n",
      "  ('scientist', 6),\n",
      "  ('fisherman', 2),\n",
      "  ('producer', 2),\n",
      "  ('guide', 1)])\n",
      "\n",
      "('NUMBER',\n",
      " [('one', 375),\n",
      "  ('00', 375),\n",
      "  ('0', 240),\n",
      "  ('two', 30),\n",
      "  ('000', 25),\n",
      "  ('85of', 21),\n",
      "  ('80of', 19),\n",
      "  ('three', 7),\n",
      "  ('85f', 6),\n",
      "  ('10of', 6),\n",
      "  ('INFREQUENT', 5),\n",
      "  ('10f', 4),\n",
      "  ('80f', 4),\n",
      "  ('and', 2),\n",
      "  ('0000', 2),\n",
      "  ('five', 2),\n",
      "  ('four', 2),\n",
      "  ('00000', 1),\n",
      "  ('70f', 1),\n",
      "  ('10o', 1),\n",
      "  ('co2', 1),\n",
      "  ('thousand', 1),\n",
      "  ('sixteen', 1)])\n",
      "\n",
      "('DATE',\n",
      " [('0000', 402),\n",
      "  ('00', 140),\n",
      "  ('the', 59),\n",
      "  ('year', 56),\n",
      "  ('now', 50),\n",
      "  ('once', 23),\n",
      "  ('of', 15),\n",
      "  ('this', 9),\n",
      "  ('INFREQUENT', 7),\n",
      "  (',', 6),\n",
      "  ('years', 6),\n",
      "  ('0', 6),\n",
      "  ('today', 5),\n",
      "  ('past', 4),\n",
      "  ('-', 4),\n",
      "  ('current', 4),\n",
      "  ('may', 4),\n",
      "  ('present', 3),\n",
      "  ('same', 3),\n",
      "  ('recently', 3),\n",
      "  ('future', 2),\n",
      "  ('currently', 2),\n",
      "  ('june', 2),\n",
      "  ('and', 2),\n",
      "  ('right', 2),\n",
      "  ('day', 1),\n",
      "  ('to', 1),\n",
      "  ('following', 1),\n",
      "  ('fall', 1),\n",
      "  ('about', 1),\n",
      "  ('second', 1),\n",
      "  ('exact', 1),\n",
      "  ('first', 1),\n",
      "  ('time', 1)])\n",
      "\n",
      "('PERCENT',\n",
      " [('00', 160),\n",
      "  ('%', 158),\n",
      "  ('percent', 6),\n",
      "  ('0', 2),\n",
      "  ('sixteen', 1),\n",
      "  ('0000', 1)])\n",
      "\n",
      "('ORDINAL',\n",
      " [('first', 69), ('second', 22), ('third', 8), ('2nd', 2), ('fourth', 2)])\n",
      "\n",
      "('SET',\n",
      " [('every', 97),\n",
      "  ('years', 74),\n",
      "  ('few', 73),\n",
      "  ('year', 31),\n",
      "  ('each', 12),\n",
      "  ('INFREQUENT', 4),\n",
      "  ('day', 3),\n",
      "  ('couple', 3),\n",
      "  ('of', 3),\n",
      "  ('since', 1)])\n",
      "\n",
      "('COUNTRY',\n",
      " [('america', 34),\n",
      "  ('australia', 25),\n",
      "  ('guinea', 23),\n",
      "  ('papua', 19),\n",
      "  ('new', 19),\n",
      "  ('mexico', 3),\n",
      "  ('united', 2),\n",
      "  ('states', 2)])\n",
      "\n",
      "('DURATION',\n",
      " [('year', 68),\n",
      "  ('years', 58),\n",
      "  ('the', 33),\n",
      "  ('-', 8),\n",
      "  ('INFREQUENT', 6),\n",
      "  ('a', 5),\n",
      "  ('few', 4),\n",
      "  ('one', 3),\n",
      "  ('00', 3),\n",
      "  ('of', 2),\n",
      "  ('day', 2),\n",
      "  ('past', 2),\n",
      "  ('couple', 1),\n",
      "  ('to', 1),\n",
      "  ('recent', 1),\n",
      "  ('last', 1),\n",
      "  ('about', 1),\n",
      "  ('future', 1)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CAUSE_OF_DEATH, TITLE, SET, and DURATION look promising\n",
    "keys = [k for k,v in ner_tally.items() if v > 100 and k != EMPTY]\n",
    "ner_wd_tally = pivot_words_by_essay_attributes(essays, attribute_name=\"pred_ner_tags_sentences\")\n",
    "for k in keys:\n",
    "    pprint((k,sorted(ner_wd_tally[k].items(), key=lambda tpl: -tpl[1])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.020349   0.241379  0.037534    1783158.0"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "get_metrics(ana_backwd_tagged_essays, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  recall  precision  f1_score  data_points\n",
       "17  MICRO_F1     0.0        0.0       0.0    1783158.0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter to select NER Tags\n",
    "ana_backwd_tagged_essays_ner_fltr = get_ana_tagged_essays(essays, ner_filter=ner_filter)\n",
    "get_metrics(ana_backwd_tagged_essays_ner_fltr, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  recall  precision  f1_score  data_points\n",
       "17  MICRO_F1     0.0        0.0       0.0    1783158.0"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter to all NER tags (excluding empty)\n",
    "ner_filter_all = set(ner_tally.keys())\n",
    "ner_filter_all.remove(EMPTY)\n",
    "\n",
    "ana_backwd_tagged_essays_ner_fltr_all = get_ana_tagged_essays(essays, ner_filter=ner_filter_all)\n",
    "get_metrics(ana_backwd_tagged_essays_ner_fltr_all, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.107558</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.107558   0.006188  0.011703    1783158.0"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "get_metrics(cref_tagged_essays, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  recall  precision  f1_score  data_points\n",
       "17  MICRO_F1     0.0        0.0       0.0    1783158.0"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select NER filter\n",
    "cref_tagged_essays_ner_fltr = get_ana_tagged_essays(essays, \n",
    "                                                              filter_to_predicted_tags=False, \n",
    "                                                              ner_filter=ner_filter)\n",
    "get_metrics(cref_tagged_essays_ner_fltr, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.008696</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>1783158.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code    recall  precision  f1_score  data_points\n",
       "17  MICRO_F1  0.002907   0.008696  0.004357    1783158.0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all NER tagged words\n",
    "# precision is higher but recall much lower\n",
    "cref_tagged_essays_ner_fltr_all = get_ana_tagged_essays(essays, \n",
    "                                                              filter_to_predicted_tags=False, \n",
    "                                                              ner_filter=ner_filter_all)\n",
    "get_metrics(cref_tagged_essays_ner_fltr_all, expected_tags=all_ana_tags, micro_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try POS Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- Try different filters on the cref chain mappings\n",
    "- For the anaphora tag tagging approach, don't limit to looking backwards only\n",
    "- Limit to NER tags, POS tag types, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
