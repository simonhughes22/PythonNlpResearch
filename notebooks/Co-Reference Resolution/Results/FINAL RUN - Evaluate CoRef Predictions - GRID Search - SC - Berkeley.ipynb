{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    " - Take the merged predictions and evaluate the prediction accuracy using the 2 different approaches\n",
    " 1. Look at the anaphora tags and then cross-reference co-reference labels\n",
    " 2. Use the co-reference chains directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "CoRef Data:  /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/Berkeley/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "from results_procesor import ResultsProcessor\n",
    "from results_common import get_essays, validate_essays, tally_essay_attributes\n",
    "from process_essays_coref import get_coref_processed_essays\n",
    "\n",
    "# progress bar widget\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET = \"SkinCancer\" # CoralBleaching | SkinCancer\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "berkeley_coref_predictions_folder = root_folder + \"CoReference/Berkeley/\"\n",
    "# Which algorithm?\n",
    "coref_predictions_folder = berkeley_coref_predictions_folder\n",
    "print(\"CoRef Data: \", coref_predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_wd_level_preds(essays, expected_tags):\n",
    "#     expected_tags = set(expected_tags)\n",
    "#     ysbycode = defaultdict(list)\n",
    "#     for e in essays:\n",
    "#         for sentix in range(len(e.sentences)):\n",
    "#             p_ccodes = e.pred_tagged_sentences[sentix]\n",
    "#             for wordix in range(len(p_ccodes)):\n",
    "#                 tags = p_ccodes[wordix]\n",
    "#                 if type(tags) == str:\n",
    "#                     ptag_set = {tags}\n",
    "#                 elif type(tags) in (set,list):\n",
    "#                     ptag_set = set(tags)   \n",
    "#                 else:\n",
    "#                     raise Exception(\"Unrecognized tag type\")\n",
    "#                 for exp_tag in expected_tags:\n",
    "#                     ysbycode[exp_tag].append(ResultsProcessor._ResultsProcessor__get_label_(exp_tag, ptag_set))\n",
    "#     return ysbycode\n",
    "\n",
    "# def get_metrics_raw(essays, expected_tags):\n",
    "#     act_ys_bycode  = ResultsProcessor.get_wd_level_lbs(essays,  expected_tags=expected_tags)\n",
    "#     pred_ys_bycode = get_wd_level_preds(essays, expected_tags=expected_tags)\n",
    "#     mean_metrics = ResultsProcessor.compute_mean_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "#     return mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/Berkeley/training_processed.dill\n"
     ]
    }
   ],
   "source": [
    "training_essays = get_essays(coref_predictions_folder, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/CoReference/Berkeley/test_processed.dill\n"
     ]
    }
   ],
   "source": [
    "test_essays = get_essays(coref_predictions_folder, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = training_essays + test_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essays validated\n",
      "Essays validated\n"
     ]
    }
   ],
   "source": [
    "validate_essays(training_essays)\n",
    "validate_essays(test_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_tally = tally_essay_attributes(all_essays, attribute_name=\"pred_ner_tags_sentences\")\n",
    "pos_tally = tally_essay_attributes(all_essays, attribute_name=\"pred_pos_tags_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Anaphor Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[6]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "cc_tally = defaultdict(int)\n",
    "cr_tally = defaultdict(int)\n",
    "reg_tally = defaultdict(int)\n",
    "for e in all_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"->\" in t:\n",
    "                        cr_tally[t] += 1\n",
    "                    elif \"Anaphor:[\" in t and \"rhetorical\" not in t:\n",
    "                        cc_tally[t] += 1\n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "all_ana_tags = sorted(cc_tally.keys())\n",
    "assert len(reg_tags) == len(all_ana_tags)\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reg_tags), len(all_ana_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAREST_REF_ONLY = \"Nearest reference\"\n",
    "MAX_ANA_PHRASE = \"Max ana phrase\"\n",
    "MAX_CHAIN_PHRASE = \"Max chain phrase\"\n",
    "POS_ANA_FLTR = \"POS ana filter\"\n",
    "POS_CHAIN_FLTR = \"Pos chain filter\"\n",
    "\n",
    "def blank_if_none(val):\n",
    "    return \"-\" if (val is None or not val or str(val).lower() == \"none\") else val\n",
    "\n",
    "def replace_if_blank(val, replace):\n",
    "    if val == \"\" or val == \"-\":\n",
    "        return replace\n",
    "    return val\n",
    "\n",
    "def process_sort_results(df_results):\n",
    "    df_disp = df_results[[\"f1_score\",\"precision\",\"recall\", \n",
    "                          NEAREST_REF_ONLY, MAX_ANA_PHRASE, MAX_CHAIN_PHRASE, POS_ANA_FLTR, POS_CHAIN_FLTR]]\n",
    "    return df_disp.sort_values(\"f1_score\", ascending=False)\n",
    "\n",
    "def filter_test_results_to_best_training_results(df_train_raw, df_test_raw):\n",
    "    # make sure sorted to the top result\n",
    "    df_train_raw_sorted = df_train_raw.sort_values(\"f1_score\", ascending=False, inplace=False)\n",
    "    top_row = df_train_raw_sorted.iloc[0]\n",
    "    filtered_df = df_test_raw[df_test_raw[NEAREST_REF_ONLY] == top_row[NEAREST_REF_ONLY]]\n",
    "    filtered_df = filtered_df[filtered_df[MAX_ANA_PHRASE]   == top_row[MAX_ANA_PHRASE]]\n",
    "    filtered_df = filtered_df[filtered_df[MAX_CHAIN_PHRASE] == top_row[MAX_CHAIN_PHRASE]]\n",
    "    filtered_df = filtered_df[filtered_df[POS_ANA_FLTR]     == top_row[POS_ANA_FLTR]]\n",
    "    filtered_df = filtered_df[filtered_df[POS_CHAIN_FLTR]   == top_row[POS_CHAIN_FLTR]]\n",
    "    print(len(filtered_df))\n",
    "    return process_sort_results(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare POS Tag Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NN', 'NNP', 'NNS'},\n",
       " {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'},\n",
       " {'PRP', 'PRP$', 'WP', 'WP$'},\n",
       " {'DT', 'PDT', 'WDT'},\n",
       " {'DT', 'PDT', 'PRP', 'PRP$', 'WDT', 'WP', 'WP$'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_nouns = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"NN\"])\n",
    "pos_verbs = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"VB\"])\n",
    "pos_pronouns = {\"PRP\",\"PRP$\", \"WP\", \"WP$\"}\n",
    "pos_determiners = {\"DT\",\"WDT\",\"PDT\"} # the, a, which, that, etc\n",
    "pos_pron_dt = pos_pronouns | pos_determiners\n",
    "# for meaning of pen treebank tags - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_nouns, pos_verbs, pos_pronouns, pos_determiners, pos_pron_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pos_filter = {\n",
    "            \"None\": None,\n",
    "            \"PRN\": pos_pronouns,\n",
    "            \"DT\": pos_determiners,\n",
    "            \"PRN+DT\": pos_pron_dt\n",
    "}\n",
    "\n",
    "dict_pos_ch_filter = {\n",
    "    \"None\": None,\n",
    "    \"NN\": pos_nouns,\n",
    "    \"VB\": pos_verbs,\n",
    "    \"NN+VB\": pos_nouns | pos_verbs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_len = [None,1,2,3,5,10,20]\n",
    "nearest_ref_only_values = [True,False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With Anaphora Prediction Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(essays, format_ana_tags, filter_to_predicted_tags, expected_tags):\n",
    "\n",
    "    # set up progress bar\n",
    "    max_count = len(nearest_ref_only_values) * len(phrase_len) * len(phrase_len) * len(dict_pos_filter) * len(dict_pos_ch_filter)\n",
    "    iprogress_bar = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    display(iprogress_bar) # display the bar\n",
    "\n",
    "    rows_ana = []\n",
    "    \n",
    "    for nearest_ref_only in nearest_ref_only_values:\n",
    "        for pos_ana_key, pos_ana_filter in dict_pos_filter.items():\n",
    "            for pos_ch_key, pos_ch_filter in dict_pos_ch_filter.items():                \n",
    "                for max_ana_phrase_len in phrase_len:\n",
    "                    for max_cref_phrase_len in phrase_len:\n",
    "\n",
    "                        proc_essays = get_coref_processed_essays(\n",
    "                            essays=essays, format_ana_tags=format_ana_tags, \n",
    "                            ner_ch_filter=None, look_back_only=True,\n",
    "                            filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                            max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                            pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter, \n",
    "                            nearest_ref_only=nearest_ref_only)\n",
    "                        \n",
    "                        metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags)\n",
    "                        row = metrics[\"MICRO_F1\"]\n",
    "                        row[NEAREST_REF_ONLY] = blank_if_none(nearest_ref_only)\n",
    "                        row[MAX_ANA_PHRASE]   = blank_if_none(max_ana_phrase_len)\n",
    "                        row[MAX_CHAIN_PHRASE] = blank_if_none(max_cref_phrase_len)\n",
    "                        row[POS_ANA_FLTR]     = blank_if_none(pos_ana_key)\n",
    "                        row[POS_CHAIN_FLTR]   = blank_if_none(pos_ch_key)\n",
    "                        rows_ana.append(row)\n",
    "                        iprogress_bar.value += 1\n",
    "\n",
    "    df_results = pd.DataFrame(rows_ana)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics_with_optimal_settings(train_essays, test_essays, filter_to_predicted_tags,\n",
    "                    nearest_ref_only, pos_ana_key, pos_ch_key, max_ana_phrase_len, max_cref_phrase_len):\n",
    "\n",
    "    # Anaphora tags train and test\n",
    "    expected_tags = all_ana_tags\n",
    "    format_ana_tags=True\n",
    "    \n",
    "    df = get_metrics(essays=train_essays, \n",
    "                     expected_tags=expected_tags, \n",
    "                     format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)\n",
    "    df_train = process_sort_results(df)\n",
    "    \n",
    "    df = get_metrics(essays=test_essays, \n",
    "                    expected_tags=expected_tags,\n",
    "                    format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)\n",
    "    df_test = process_sort_results(df)\n",
    "    \n",
    "    # CC Accuracy without Anaphora Resolution\n",
    "    expected_tags = reg_tags\n",
    "    format_ana_tags = True # Set this to true so that we ignore anaphora resolution for the next 2\n",
    "    \n",
    "    df = get_metrics(essays=train_essays, \n",
    "                     expected_tags=expected_tags, \n",
    "                     format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)\n",
    "    df_train_cc_reg = process_sort_results(df)\n",
    "    \n",
    "    df = get_metrics(essays=test_essays, \n",
    "                    expected_tags=expected_tags,\n",
    "                    format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)\n",
    "    df_test_cc_reg = process_sort_results(df)\n",
    "    \n",
    "    # CC Accuracy with Anaphora Resolution\n",
    "    format_ana_tags=False\n",
    "    \n",
    "    df = get_metrics(essays=train_essays, \n",
    "                     expected_tags=expected_tags, \n",
    "                     format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)\n",
    "    df_train_cc_ana = process_sort_results(df)\n",
    "    \n",
    "    df = get_metrics(essays=test_essays, \n",
    "                    expected_tags=expected_tags,\n",
    "                    format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)\n",
    "    df_test_cc_ana = process_sort_results(df)\n",
    "    \n",
    "    df_concat = pd.concat([\n",
    "        df_train, df_test,\n",
    "        df_train_cc_reg, df_train_cc_ana,\n",
    "        df_test_cc_reg, df_test_cc_ana,\n",
    "    ])\n",
    "    df_concat= df_concat[[\"f1_score\",\"precision\", \"recall\"]]\n",
    "    \n",
    "    experiment_type = \"Ana\" if filter_to_predicted_tags else \"Cref\"\n",
    "    df_concat[\"Data Set\"] = [experiment_type + \" Train\",     experiment_type + \" Test\", \n",
    "                             \"CC Train Reg\",   \"CC Train \" + experiment_type,\n",
    "                             \"CC Test Reg\",    \"CC Test  \" + experiment_type]\n",
    "    return df_concat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(essays, format_ana_tags, filter_to_predicted_tags, expected_tags,\n",
    "                    nearest_ref_only, pos_ana_key, pos_ch_key, max_ana_phrase_len, max_cref_phrase_len):\n",
    "    \n",
    "    pos_ana_filter = dict_pos_filter[pos_ana_key]\n",
    "    pos_ch_filter  = dict_pos_ch_filter[pos_ch_key]\n",
    "    \n",
    "    proc_essays = get_coref_processed_essays(\n",
    "                            essays=essays, format_ana_tags=format_ana_tags, \n",
    "                            ner_ch_filter=None, look_back_only=True,\n",
    "                            filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                            max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                            pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter, \n",
    "                            nearest_ref_only=nearest_ref_only)\n",
    "                        \n",
    "    act_ys_bycode  = ResultsProcessor.get_wd_level_lbs(proc_essays,  expected_tags=expected_tags)\n",
    "    pred_ys_bycode = get_wd_level_preds(proc_essays, expected_tags=expected_tags)\n",
    "    return act_ys_bycode, pred_ys_bycode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(essays, format_ana_tags, filter_to_predicted_tags, expected_tags,\n",
    "                    nearest_ref_only, pos_ana_key, pos_ch_key, max_ana_phrase_len, max_cref_phrase_len):\n",
    "    \n",
    "    pos_ana_filter = dict_pos_filter[pos_ana_key]\n",
    "    pos_ch_filter  = dict_pos_ch_filter[pos_ch_key]\n",
    "    \n",
    "    proc_essays = get_coref_processed_essays(\n",
    "                            essays=essays, format_ana_tags=format_ana_tags, \n",
    "                            ner_ch_filter=None, look_back_only=True,\n",
    "                            filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                            max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                            pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter, \n",
    "                            nearest_ref_only=nearest_ref_only)\n",
    "                        \n",
    "    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags)\n",
    "    row = metrics[\"MICRO_F1\"]\n",
    "    row[NEAREST_REF_ONLY] = blank_if_none(nearest_ref_only)\n",
    "    row[MAX_ANA_PHRASE]   = blank_if_none(max_ana_phrase_len)\n",
    "    row[MAX_CHAIN_PHRASE] = blank_if_none(max_cref_phrase_len)\n",
    "    row[POS_ANA_FLTR]     = blank_if_none(pos_ana_key)\n",
    "    row[POS_CHAIN_FLTR]   = blank_if_none(pos_ch_key)\n",
    "    df_results = pd.DataFrame([row])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wd_level_preds(essays, expected_tags):\n",
    "    expected_tags = set(expected_tags)\n",
    "    ysbycode = defaultdict(list)\n",
    "    for e in essays:\n",
    "        for sentix in range(len(e.sentences)):\n",
    "            p_ccodes = e.pred_tagged_sentences[sentix]\n",
    "            for wordix in range(len(p_ccodes)):\n",
    "                tags = p_ccodes[wordix]\n",
    "                if type(tags) == str:\n",
    "                    ptag_set = {tags}\n",
    "                elif type(tags) in (set,list):\n",
    "                    ptag_set = set(tags)   \n",
    "                else:\n",
    "                    raise Exception(\"Unrecognized tag type\")\n",
    "                for exp_tag in expected_tags:\n",
    "                    ysbycode[exp_tag].append(ResultsProcessor._ResultsProcessor__get_label_(exp_tag, ptag_set))\n",
    "    return ysbycode\n",
    "\n",
    "def get_metrics_raw(essays, expected_tags):\n",
    "    act_ys_bycode  = ResultsProcessor.get_wd_level_lbs(essays,  expected_tags=expected_tags)\n",
    "    pred_ys_bycode = get_wd_level_preds(essays, expected_tags=expected_tags)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "    return mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_ref_only, pos_ana_key, pos_ch_key, max_ana_phrase_len, max_cref_phrase_len = (False, 'None', 'NN+VB', 5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_predicted_tags=True\n",
    "\n",
    "df_all_ana = get_all_metrics_with_optimal_settings(train_essays=training_essays, test_essays=test_essays,\n",
    "        filter_to_predicted_tags=filter_to_predicted_tags, nearest_ref_only=nearest_ref_only, \n",
    "        pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "        max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>Data Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.035785</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>Ana Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.074766</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>Ana Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822463</td>\n",
       "      <td>0.805007</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>CC Train Reg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822040</td>\n",
       "      <td>0.805007</td>\n",
       "      <td>0.839810</td>\n",
       "      <td>CC Train Ana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840573</td>\n",
       "      <td>0.830139</td>\n",
       "      <td>0.851273</td>\n",
       "      <td>CC Test Reg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839524</td>\n",
       "      <td>0.830139</td>\n",
       "      <td>0.849124</td>\n",
       "      <td>CC Test  Ana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score    recall  precision      Data Set\n",
       "0  0.035785  0.019027   0.300000     Ana Train\n",
       "0  0.129032  0.074766   0.470588      Ana Test\n",
       "0  0.822463  0.805007   0.840692  CC Train Reg\n",
       "0  0.822040  0.805007   0.839810  CC Train Ana\n",
       "0  0.840573  0.830139   0.851273   CC Test Reg\n",
       "0  0.839524  0.830139   0.849124  CC Test  Ana"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"Ana Train\", \"Ana Test\" - Train/Test with Anaphor predicted tags\n",
    "\n",
    "\"CC Train Reg\",  \"CC Test Reg\" - Concept codes withour anaphora tagging\n",
    "\"CC Train Ana\", \"CC Test Ana\"  - Concept codes with anaphora tagging\n",
    "\"\"\"\n",
    "df_all_ana[[\"f1_score\", \"recall\", \"precision\", \"Data Set\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_tags = all_ana_tags\n",
    "format_ana_tags=True\n",
    "    \n",
    "tr_ysbycode, tr_predsbycode = get_predictions(essays=training_essays, expected_tags=expected_tags, \n",
    "                    format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_tags = all_ana_tags\n",
    "format_ana_tags=True\n",
    "    \n",
    "test_ysbycode, test_predsbycode = get_predictions(essays=test_essays, expected_tags=expected_tags, \n",
    "                    format_ana_tags=format_ana_tags,\n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                    nearest_ref_only=nearest_ref_only, pos_ana_key=pos_ana_key, pos_ch_key=pos_ch_key, \n",
    "                               max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.019027484143763214,\n",
       " 'precision': 0.3,\n",
       " 'f1_score': 0.03578528827037773,\n",
       " 'accuracy': 0.9996295557953896,\n",
       " 'num_codes': 473.0,\n",
       " 'data_points': 1309239.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = ResultsProcessor.compute_mean_metrics(tr_ysbycode, tr_predsbycode)\n",
    "mean_metrics[\"MICRO_F1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': 0.07476635514018691,\n",
       " 'precision': 0.47058823529411764,\n",
       " 'f1_score': 0.1290322580645161,\n",
       " 'accuracy': 0.9996610360996554,\n",
       " 'num_codes': 107.0,\n",
       " 'data_points': 318618.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = ResultsProcessor.compute_mean_metrics(test_ysbycode, test_predsbycode)\n",
    "mean_metrics[\"MICRO_F1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsProcessor.persist_predictions(\"COREF_SC_GRID_BERKELEY_TD\", tr_predsbycode,   tr_ysbycode)\n",
    "ResultsProcessor.persist_predictions(\"COREF_SC_GRID_BERKELEY_VD\", test_predsbycode, test_ysbycode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
