{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    " - Take the merged predictions and evaluate the prediction accuracy using the 2 different approaches\n",
    " 1. Look at the anaphora tags and then cross-reference co-reference labels\n",
    " 2. Use the co-reference chains directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "CoRef Data:  /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "from FindFiles import find_files\n",
    "from Settings import Settings\n",
    "from CoRefHelper import EMPTY\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "from results_procesor import ResultsProcessor\n",
    "from results_procesor import metrics_to_df\n",
    "\n",
    "# progress bar widget\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET = \"CoralBleaching\" # CoralBleaching | SkinCancer\n",
    "PARTITION = \"Training\" # Training | Test\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "print(\"CoRef Data: \", stanford_coref_predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/training_processed.dill\n"
     ]
    }
   ],
   "source": [
    "def get_essays(folder, partition):\n",
    "    essay_files = find_files(folder)\n",
    "    if partition == \"Training\":\n",
    "        essay_files = [e for e in essay_files if \"train\" in e]\n",
    "    else:\n",
    "        essay_files = [e for e in essay_files if \"test\" in e]\n",
    "    assert len(essay_files) == 1\n",
    "    print(\"Found file\", essay_files[0])\n",
    "    with open(essay_files[0], \"rb\") as f:\n",
    "        loaded_essays = dill.load(f)\n",
    "    return loaded_essays\n",
    "\n",
    "essays = get_essays(stanford_coref_predictions_folder, PARTITION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_essays(essays):\n",
    "    for e in essays:    \n",
    "        # map coref ids to sent_ix, wd_ix tuples\n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            sent     = e.sentences[sent_ix]\n",
    "            ana_tags = e.ana_tagged_sentences[sent_ix]\n",
    "            coref_ids= e.pred_corefids[sent_ix]\n",
    "            ner_tags = e.pred_ner_tags_sentences[sent_ix]\n",
    "            pos_tags = e.pred_pos_tags_sentences[sent_ix]\n",
    "            ptags    = e.pred_tagged_sentences[sent_ix]\n",
    "\n",
    "            assert len(sent) == len(coref_ids)\n",
    "\n",
    "            assert len(sent) == len(ana_tags) == len(coref_ids) == len(ner_tags) == len(pos_tags) == len(ptags),\\\n",
    "                (len(sent), len(ana_tags), len(coref_ids), len(ner_tags), len(pos_tags), len(ptags), e.name, sent_ix)\n",
    "            assert len(sent) > 0\n",
    "            \n",
    "validate_essays(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tally_essay_attributes(essays, attribute_name=\"pred_pos_tags_sentences\"):\n",
    "    tally = defaultdict(int)\n",
    "    for e in essays:\n",
    "        nested_list = getattr(e, attribute_name)\n",
    "        for lst in nested_list:\n",
    "            for item in lst:\n",
    "                if type(item) == str:\n",
    "                    tally[item] +=1\n",
    "                elif type(item) == set:\n",
    "                    for i in item:\n",
    "                        tally[i] +=1\n",
    "                else:\n",
    "                    raise Exception(\"Unexpected item type\")\n",
    "    return tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tally = tally_essay_attributes(essays, attribute_name=\"pred_ner_tags_sentences\")\n",
    "pos_tally = tally_essay_attributes(essays, attribute_name=\"pred_pos_tags_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Anaphor Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[13]',\n",
       " 'Anaphor:[14]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[5b]',\n",
       " 'Anaphor:[6]',\n",
       " 'Anaphor:[7]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "cc_tally = defaultdict(int)\n",
    "cr_tally = defaultdict(int)\n",
    "reg_tally = defaultdict(int)\n",
    "for e in essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"->\" in t:\n",
    "                        cr_tally[t] += 1\n",
    "                    elif \"Anaphor:[\" in t:\n",
    "                        cc_tally[t] += 1\n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "all_ana_tags = sorted(cc_tally.keys())\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chain(e):\n",
    "    \"\"\" Takes an essay object, and creats a map of Dict[str, List[Tuple{int,int}]]\n",
    "        which maps a coref id (essay scope) to a list of (sent_ix,wd_ix) pairs\n",
    "    \"\"\"\n",
    "    corefid_2_chain = defaultdict(list)\n",
    "    for sent_ix in range(len(e.sentences)):\n",
    "        sent     = e.sentences[sent_ix]\n",
    "        coref_ids= e.pred_corefids[sent_ix]\n",
    "        for wd_ix in range(len(sent)):\n",
    "            wd_coref_ids = coref_ids[wd_ix] # Set[str]\n",
    "            for cr_id in wd_coref_ids:\n",
    "                pair = (sent_ix, wd_ix)\n",
    "                corefid_2_chain[cr_id].append(pair)\n",
    "    return corefid_2_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_segmented_chain(e):\n",
    "    \"\"\" Takes an essay object, and creats a map of Dict[str, List[List[Tuple{int,int}]]\n",
    "        which maps a coref id (essay scope) to a nested list of (sent_ix,wd_ix) pairs.\n",
    "        The nested list has a separate inner list for every distinct coreference seq/phrase\n",
    "    \"\"\"\n",
    "\n",
    "    corefid_2_chain = build_chain(e)\n",
    "    corefid_2_segmented_chain = dict()\n",
    "    for cref, pairs in corefid_2_chain.items():\n",
    "        segmented = [[pairs[0]]]\n",
    "        corefid_2_segmented_chain[cref] = segmented\n",
    "        last_sent_ix, last_wd_ix = pairs[0]\n",
    "        for pair in pairs[1:]:\n",
    "            sent_ix, wd_ix = pair\n",
    "            if sent_ix != last_sent_ix or (wd_ix - last_wd_ix) > 1:\n",
    "                # create a new nested list\n",
    "                segmented.append([])\n",
    "            # append pair to last list item\n",
    "            segmented[-1].append(pair)        \n",
    "            last_sent_ix, last_wd_ix = pair\n",
    "    return corefid_2_segmented_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\t [(0, 23), (0, 24), (0, 25)]\n",
      "2\n",
      "\t [(7, 0), (7, 1)]\n",
      "\t [(7, 8)]\n",
      "3\n",
      "\t [(2, 0), (2, 1)]\n",
      "\t [(3, 2)]\n",
      "\t [(3, 9)]\n"
     ]
    }
   ],
   "source": [
    "corefid_2_segmented_chain = build_segmented_chain(essays[2])\n",
    "for cref, seg_chain in sorted(corefid_2_segmented_chain.items()):\n",
    "    print(cref)\n",
    "    for lst in seg_chain:\n",
    "        print(\"\\t\", str(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processessays import Essay\n",
    "import warnings\n",
    "\n",
    "def get_processed_essays(essays, format_ana_tags=True, filter_to_predicted_tags=True, look_back_only=True,\n",
    "                         max_cref_phrase_len=None, ner_ch_filter=None, pos_filter=None, pos_ch_filter=None\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Create a copy of essays, augmenting the pred_tagged_sentences object with additional anaphora tags\n",
    "    \n",
    "    essays:                   List[Essay] objects - merged tagged essays\n",
    "    format_ana_tags:          bool - Add ana tags as Anaphor[xyz] or as just the regular concept codes\n",
    "    filter_to_predicted_tags: bool - Filter to just the predicted anaphor tags\n",
    "    look_back_only:           bool - Only look to coreferences occuring earlier in the essay\n",
    "    max_cref_phrase_len:      Union(int,None) - if specified, maximum coreference length to consider\n",
    "    ner_ch_filter:            Union(Set[str],None) - if specified, filters to words in the cref chain\n",
    "                                with one of those NER tags\n",
    "    pos_filter:               Union(Set[str],None) - if specified, filters crefs to words with one of those POS tags\n",
    "    pos_ch_filter:            Union(Set[str],None) - if specified, filters to words in the cref chain \n",
    "                                with one of those POS tags\n",
    "    \"\"\"\n",
    "    if ner_ch_filter and EMPTY in ner_ch_filter:\n",
    "        warnings.warn(\"EMPTY tag in NER filter \", UserWarning)\n",
    "    if pos_filter and EMPTY in pos_filter:\n",
    "        warnings.warn(\"EMPTY tag in POS filter \", UserWarning)\n",
    "    if pos_ch_filter and EMPTY in pos_ch_filter:\n",
    "        warnings.warn(\"EMPTY tag in POS chain filter \", UserWarning)\n",
    "    \n",
    "    ana_tagged_essays = []\n",
    "    for eix, e in enumerate(essays):\n",
    "\n",
    "        ana_tagged_e = Essay(e.name, e.sentences)\n",
    "        ana_tagged_e.pred_tagged_sentences = []\n",
    "        ana_tagged_essays.append(ana_tagged_e)\n",
    "\n",
    "        # map coref ids to sent_ix, wd_ix tuples\n",
    "        corefid_2_chain = build_segmented_chain(e)\n",
    "\n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            ana_tagged_sent = []\n",
    "            ana_tagged_e.pred_tagged_sentences.append(ana_tagged_sent)\n",
    "\n",
    "            sent     = e.sentences[sent_ix]\n",
    "            \n",
    "            # SENTENCE LEVEL TAGS / PREDICTIONS\n",
    "            ana_tags = e.ana_tagged_sentences[sent_ix]\n",
    "            coref_ids= e.pred_corefids[sent_ix]\n",
    "            ner_tags = e.pred_ner_tags_sentences[sent_ix]\n",
    "            pos_tags = e.pred_pos_tags_sentences[sent_ix]\n",
    "            ptags    = e.pred_tagged_sentences[sent_ix]    \n",
    "\n",
    "            for wd_ix in range(len(sent)):                            \n",
    "                pos_tag = pos_tags[wd_ix] # POS tag             \n",
    "                \n",
    "                word, _ = sent[wd_ix] # ignore actual tags\n",
    "                pred_cc_tag = ptags[wd_ix] # predict cc tag\n",
    "                \n",
    "                is_ana_tag = ana_tags[wd_ix] == ANAPHORA\n",
    "                wd_coref_ids = coref_ids[wd_ix] # Set[str]\n",
    "\n",
    "                # note we are changing this to a set rather than a single string\n",
    "                wd_ptags = set()\n",
    "                # add predicted concept code tag (filtered out by evaluation code, which filters to specific tags)\n",
    "                if pred_cc_tag != EMPTY:\n",
    "                    wd_ptags.add(pred_cc_tag)\n",
    "\n",
    "                # initialize predicted tags, inc. cc tag\n",
    "                # DON'T run continue until after this point\n",
    "                ana_tagged_sent.append(wd_ptags)\n",
    "                \n",
    "                if len(wd_coref_ids) == 0:\n",
    "                    continue\n",
    "\n",
    "                # POS FILTER - for cref words and NOT words in the cref chain\n",
    "                if pos_filter and pos_tag not in pos_filter:\n",
    "                    continue\n",
    "                    \n",
    "                if filter_to_predicted_tags and not is_ana_tag:\n",
    "                    continue\n",
    "                    \n",
    "                # Get codes for corresponding co-ref chain entries\n",
    "                for cr_id in wd_coref_ids:                        \n",
    "                    segmented_chain = corefid_2_chain[cr_id]\n",
    "                    for cref_phrase in segmented_chain: # iterate thru the list of sent_ix,wd_ix's\n",
    "                                                        # in 1 cref phrase\n",
    "\n",
    "                        # LENGTH FILTER\n",
    "                        if max_cref_phrase_len and len(cref_phrase) > max_cref_phrase_len:\n",
    "                            continue                            \n",
    "\n",
    "                        for ch_sent_ix, ch_wd_ix in cref_phrase:\n",
    "                            # if it's the current word, skip\n",
    "                            if ch_sent_ix == sent_ix and ch_wd_ix == wd_ix:\n",
    "                                continue\n",
    "                            # for anaphors only - only look at chain ixs before the current word\n",
    "                            # if's it's after the current word in the essay, skip\n",
    "                            if look_back_only:\n",
    "                                # sentence later in the essay, or same sentence but word is after current word\n",
    "                                if ch_sent_ix > sent_ix or \\\n",
    "                                  (ch_sent_ix == sent_ix and ch_wd_ix >= wd_ix):\n",
    "                                    continue\n",
    "\n",
    "                            chain_ptag = e.pred_tagged_sentences[ch_sent_ix][ch_wd_ix]\n",
    "                            ch_ner_tag = e.pred_ner_tags_sentences[ch_sent_ix][ch_wd_ix]\n",
    "                            ch_pos_tag = e.pred_pos_tags_sentences[ch_sent_ix][ch_wd_ix]\n",
    "\n",
    "                            # CHAIN WORD TYPE FILTERS\n",
    "                            # NER TAG TYPE FILTER - on chain \n",
    "                            if ner_ch_filter and ch_ner_tag not in ner_ch_filter:\n",
    "                                continue\n",
    "                            # POS TAG TYPE FILTER - on chain\n",
    "                            if pos_ch_filter and ch_pos_tag not in pos_ch_filter:\n",
    "                                continue\n",
    "\n",
    "                            if chain_ptag != EMPTY:\n",
    "                                code = chain_ptag\n",
    "                                if format_ana_tags:\n",
    "                                    code =  \"{anaphora}:[{code}]\".format(\n",
    "                                        anaphora=ANAPHORA, code=chain_ptag)\n",
    "                                wd_ptags.add(code)\n",
    "    # validation check    \n",
    "    #   check essay and sent lengths align\n",
    "    for e in ana_tagged_essays:\n",
    "        assert len(e.sentences) == len(e.pred_tagged_sentences)\n",
    "        for ix in range(len(e.sentences)):\n",
    "            assert len(e.sentences[ix]) == len(e.pred_tagged_sentences[ix])\n",
    "            \n",
    "    return ana_tagged_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this function from the Resultsprocessor so that it works with Set[str] of predicted tags \n",
    "# as well as scalar strings\n",
    "def get_wd_level_preds(essays, expected_tags):\n",
    "    expected_tags = set(expected_tags)\n",
    "    ysbycode = defaultdict(list)\n",
    "    for e in essays:\n",
    "        for sentix in range(len(e.sentences)):\n",
    "            p_ccodes = e.pred_tagged_sentences[sentix]\n",
    "            for wordix in range(len(p_ccodes)):\n",
    "                tags = p_ccodes[wordix]\n",
    "                if type(tags) == str:\n",
    "                    ptag_set = {tags}\n",
    "                elif type(tags) in (set,list):\n",
    "                    ptag_set = set(tags)   \n",
    "                else:\n",
    "                    raise Exception(\"Unrecognized tag type\")\n",
    "                for exp_tag in expected_tags:\n",
    "                    ysbycode[exp_tag].append(ResultsProcessor._ResultsProcessor__get_label_(exp_tag, ptag_set))\n",
    "    return ysbycode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(mean_metrics):\n",
    "    df = metrics_to_df(mean_metrics)\n",
    "    df = df[[\"code\",\"recall\",\"precision\",\"f1_score\",\"data_points\"]]\n",
    "    df = df.sort_values(\"code\")\n",
    "    return df[~df.code.str.contains(\"MEAN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_raw(essays, expected_tags, micro_only=False):\n",
    "    act_ys_bycode  = ResultsProcessor.get_wd_level_lbs(essays,  expected_tags=expected_tags)\n",
    "    pred_ys_bycode = get_wd_level_preds(essays, expected_tags=expected_tags)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "    return mean_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(essays, expected_tags, micro_only=False):\n",
    "    act_ys_bycode  = ResultsProcessor.get_wd_level_lbs(essays,  expected_tags=expected_tags)\n",
    "    pred_ys_bycode = get_wd_level_preds(essays, expected_tags=expected_tags)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(act_ys_bycode, pred_ys_bycode)\n",
    "    df = get_df(mean_metrics)\n",
    "    if micro_only:\n",
    "        df = df[df.code == \"MICRO_F1\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NN', 'NNP', 'NNPS', 'NNS'},\n",
       " {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'},\n",
       " {'IN', 'NN', 'NNP', 'NNPS', 'NNS', 'PRP', 'PRP$', 'WP', 'WP$'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_nouns = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"NN\"])\n",
    "pos_verbs = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"VB\"])\n",
    "pos_pronouns = {\"PRP\",\"PRP$\", \"WP\", \"WP$\"}\n",
    "pos_determiners = {\"DT\",\"WDT\",\"PDT\"} # the, a, which, that, etc\n",
    "pos_pron_dt = pos_pronouns | pos_determiners\n",
    "# for meaning of pen treebank tags - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_filter = {\"IN\"} | pos_nouns | pos_pronouns # WDT is a Wh Determiner, and PDT is pre-determiner, such as 'all' or 'half'\n",
    "pos_nouns, pos_verbs, pos_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pos_filter = {\n",
    "            \"None\": None,\n",
    "            \"PRN\": pos_pronouns,\n",
    "            \"DT\": pos_determiners,\n",
    "            \"PRN+DT\": pos_pron_dt\n",
    "}\n",
    "\n",
    "dict_pos_ch_filter = {\n",
    "    \"None\": None,\n",
    "    \"NN\": pos_nouns,\n",
    "    \"VB\": pos_verbs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_if_none(val):\n",
    "    return \"-\" if (val is None or not val or str(val).lower() == \"none\") else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_len = [None] + list(range(1,11))\n",
    "phrase_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back_vals = [True,False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With Anaphora Prediction Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f4fb928f084f87be8a9d3be30714fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0, max=264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_to_predicted_tags = True\n",
    "format_ana_tags=True # Format tags with Anaphora[xyz]\n",
    "\n",
    "# set up progress bar\n",
    "max_count = len(look_back_vals)  * len(phrase_len) * len(dict_pos_filter) * len(dict_pos_ch_filter)\n",
    "iprogress_bar = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(iprogress_bar) # display the bar\n",
    "\n",
    "LOOK_BACK = \"Look back\"\n",
    "MAX_PHRASE = \"Max phrase\"\n",
    "POS_FLTR = \"POS filter\"\n",
    "POS_CHAIN_FLTR = \"Pos chain filter\"\n",
    "\n",
    "rows_ana = []\n",
    "for look_back_only in look_back_vals:\n",
    "    for pos_key, pos_filter in dict_pos_filter.items():\n",
    "        for pos_ch_key, pos_ch_filter in dict_pos_ch_filter.items():                \n",
    "            for max_cref_phrase_len in phrase_len:\n",
    "                proc_essays = get_processed_essays(\n",
    "                    essays=essays, format_ana_tags=format_ana_tags, \n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, look_back_only=look_back_only,\n",
    "                    max_cref_phrase_len=max_cref_phrase_len, ner_ch_filter=None, \n",
    "                    pos_filter=pos_filter, pos_ch_filter=pos_ch_filter)\n",
    "                metrics = get_metrics_raw(proc_essays, all_ana_tags,  micro_only=True)\n",
    "                row = metrics[\"MICRO_F1\"]\n",
    "                row[LOOK_BACK] = look_back_only\n",
    "                row[MAX_PHRASE] = blank_if_none(max_cref_phrase_len)\n",
    "                row[POS_FLTR] = blank_if_none(pos_key)\n",
    "                row[POS_CHAIN_FLTR] = blank_if_none(pos_ch_key)\n",
    "                rows_ana.append(row)\n",
    "                iprogress_bar.value += 1\n",
    "\n",
    "df_results_ana = pd.DataFrame(rows_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Look back</th>\n",
       "      <th>Max phrase</th>\n",
       "      <th>POS filter</th>\n",
       "      <th>Pos chain filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>True</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.020349</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>PRN+DT</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1_score  precision    recall Look back Max phrase POS filter  \\\n",
       "0    0.037534   0.241379  0.020349      True          -          -   \n",
       "10   0.037534   0.241379  0.020349      True         10          -   \n",
       "21   0.037534   0.241379  0.020349      True         10          -   \n",
       "20   0.037534   0.241379  0.020349      True          9          -   \n",
       "109  0.037534   0.241379  0.020349      True         10     PRN+DT   \n",
       "\n",
       "    Pos chain filter  \n",
       "0                  -  \n",
       "10                 -  \n",
       "21                NN  \n",
       "20                NN  \n",
       "109                -  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disp = df_results_ana[[\"f1_score\",\"precision\",\"recall\", LOOK_BACK, MAX_PHRASE, POS_FLTR, POS_CHAIN_FLTR]]\n",
    "df_disp.sort_values(\"f1_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search without Anaphora Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a98b33175cb42e99da657c4095dbda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0, max=264)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_to_predicted_tags = False # just use the raw coref chains\n",
    "format_ana_tags=True # Format tags with Anaphora[xyz]\n",
    "\n",
    "# set up progress bar\n",
    "max_count = len(look_back_vals)  * len(phrase_len) * len(dict_pos_filter) * len(dict_pos_ch_filter)\n",
    "iprogress_bar = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(iprogress_bar) # display the bar\n",
    "\n",
    "LOOK_BACK = \"Look back\"\n",
    "MAX_PHRASE = \"Max phrase\"\n",
    "POS_FLTR = \"POS filter\"\n",
    "POS_CHAIN_FLTR = \"Pos chain filter\"\n",
    "\n",
    "rows_chain = []\n",
    "for look_back_only in look_back_vals:\n",
    "    for pos_key, pos_filter in dict_pos_filter.items():\n",
    "        for pos_ch_key, pos_ch_filter in dict_pos_ch_filter.items():                \n",
    "            for max_cref_phrase_len in phrase_len:\n",
    "                proc_essays = get_processed_essays(\n",
    "                    essays=essays, format_ana_tags=format_ana_tags, \n",
    "                    filter_to_predicted_tags=filter_to_predicted_tags, look_back_only=look_back_only,\n",
    "                    max_cref_phrase_len=max_cref_phrase_len, ner_ch_filter=None, \n",
    "                    pos_filter=pos_filter, pos_ch_filter=pos_ch_filter)\n",
    "                metrics = get_metrics_raw(proc_essays, all_ana_tags,  micro_only=True)\n",
    "                row = metrics[\"MICRO_F1\"]\n",
    "                row[LOOK_BACK] = look_back_only\n",
    "                row[MAX_PHRASE] = blank_if_none(max_cref_phrase_len)\n",
    "                row[POS_FLTR] = blank_if_none(pos_key)\n",
    "                row[POS_CHAIN_FLTR] = blank_if_none(pos_ch_key)\n",
    "                rows_chain.append(row)\n",
    "                iprogress_bar.value += 1\n",
    "\n",
    "df_results_chain = pd.DataFrame(rows_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp = df_results_chain[[\"f1_score\",\"precision\",\"recall\", LOOK_BACK, MAX_PHRASE, POS_FLTR, POS_CHAIN_FLTR]]\n",
    "df_disp.sort_values(\"f1_score\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - delete this cell\n",
    "del rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
