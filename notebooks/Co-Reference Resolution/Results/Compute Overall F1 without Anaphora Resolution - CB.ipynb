{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    " - The other techniques for finding anaphoric concept codes did not work well. Why not instead use the anaphora tagging model and just use the most recently predicted concept code as the antecedent to grad the code from? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "CoRef Data:  /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "from results_common import get_essays, validate_essays, tally_essay_attributes\n",
    "from process_essays_coref import get_coref_processed_essays\n",
    "from metrics import get_metrics_raw\n",
    "\n",
    "# progress bar widget\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET = \"CoralBleaching\" # CoralBleaching | SkinCancer\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "berkeley_coref_predictions_folder = root_folder + \"CoReference/Berkeley/\"\n",
    "# Which algorithm?\n",
    "coref_predictions_folder = berkeley_coref_predictions_folder\n",
    "print(\"CoRef Data: \", stanford_coref_predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/training_processed.dill\n"
     ]
    }
   ],
   "source": [
    "training_essays = get_essays(coref_predictions_folder, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/Berkeley/test_processed.dill\n"
     ]
    }
   ],
   "source": [
    "test_essays = get_essays(coref_predictions_folder, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = training_essays + test_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essays validated\n",
      "Essays validated\n"
     ]
    }
   ],
   "source": [
    "validate_essays(training_essays)\n",
    "validate_essays(test_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Anaphor Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[13]',\n",
       " 'Anaphor:[14]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[5b]',\n",
       " 'Anaphor:[6]',\n",
       " 'Anaphor:[7]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "cc_tally = defaultdict(int)\n",
    "cr_tally = defaultdict(int)\n",
    "reg_tally = defaultdict(int)\n",
    "for e in all_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"->\" in t:\n",
    "                        cr_tally[t] += 1\n",
    "                    elif \"Anaphor:[\" in t:\n",
    "                        cc_tally[t] += 1\n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "all_ana_tags = sorted(cc_tally.keys())\n",
    "assert len(reg_tags) == len(all_ana_tags)\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blank_if_none(val):\n",
    "    return \"-\" if (val is None or not val or str(val).lower() == \"none\") else val\n",
    "\n",
    "def replace_if_blank(val, replace):\n",
    "    if val == \"\" or val == \"-\":\n",
    "        return replace\n",
    "    return val\n",
    "\n",
    "def process_sort_results(df_results):\n",
    "    df_disp = df_results[[\"f1_score\",\"precision\",\"recall\"]]\n",
    "    return df_disp.sort_values(\"f1_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With Anaphora Prediction Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_essays_coref import fix_coref_ids, build_segmented_chain\n",
    "from processessays import Essay\n",
    "from CoRefHelper import EMPTY\n",
    "from BrattEssay import ANAPHORA\n",
    "from collections import defaultdict\n",
    "\n",
    "def processed_essays_replace_ana_tags_with_regular(essays):\n",
    "\n",
    "    ana_tagged_essays = []\n",
    "    for eix, e in enumerate(essays):\n",
    "\n",
    "        fix_coref_ids(e)\n",
    "        seq_pred_tags = [] # all predicted tags\n",
    "        \n",
    "        new_sentences = []\n",
    "        ana_tagged_e = Essay(e.name, new_sentences)\n",
    "        ana_tagged_e.pred_tagged_sentences   = list(e.pred_tagged_sentences)\n",
    "        ana_tagged_e.pred_pos_tags_sentences = list(e.pred_pos_tags_sentences)\n",
    "        ana_tagged_e.pred_ner_tags_sentences = list(e.pred_pos_tags_sentences)\n",
    "        ana_tagged_e.ana_tagged_sentences    = list(e.ana_tagged_sentences)\n",
    "        ana_tagged_e.pred_corefids           = list(e.pred_corefids)\n",
    "        ana_tagged_essays.append(ana_tagged_e)\n",
    "    \n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            new_sent = []\n",
    "            new_sentences.append(new_sent)\n",
    "            \n",
    "            sent = e.sentences[sent_ix]\n",
    "            for wd, tags in sent:\n",
    "                new_tags = set()\n",
    "                for t in tags:\n",
    "                    if t.startswith(\"Anaphor:[\"):\n",
    "                        t_old = t\n",
    "                        t = t.replace(\"Anaphor:[\",\"\").replace(\"]\",\"\")\n",
    "#                         print(t_old, t)\n",
    "                    new_tags.add(t)\n",
    "                new_sent.append((wd, new_tags))\n",
    "            assert len(new_sent) == len(sent)\n",
    "        assert len(new_sentences) == len(e.sentences)\n",
    "    \n",
    "    # validation check\n",
    "    #   check essay and sent lengths align\n",
    "    for e in ana_tagged_essays:\n",
    "        assert len(e.sentences) == len(e.pred_tagged_sentences), (e.name, len(e.sentences),len(e.pred_tagged_sentences))\n",
    "        for ix in range(len(e.sentences)):\n",
    "            assert len(e.sentences[ix]) == len(e.pred_tagged_sentences[ix]), (len(e.sentences[ix]), len(e.pred_tagged_sentences[ix]))\n",
    "\n",
    "    return ana_tagged_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_essays_coref import fix_coref_ids, build_segmented_chain\n",
    "from processessays import Essay\n",
    "from CoRefHelper import EMPTY\n",
    "from BrattEssay import ANAPHORA\n",
    "from collections import defaultdict\n",
    "\n",
    "def processed_essays_predict_most_recent_tag(essays):\n",
    "\n",
    "    ana_tagged_essays = []\n",
    "    for eix, e in enumerate(essays):\n",
    "\n",
    "        fix_coref_ids(e)\n",
    "        seq_pred_tags = [] # all predicted tags\n",
    "        \n",
    "        ana_tagged_e = Essay(e.name, e.sentences)\n",
    "        ana_tagged_e.pred_tagged_sentences = []\n",
    "        ana_tagged_e.pred_pos_tags_sentences = list(e.pred_pos_tags_sentences)\n",
    "        ana_tagged_e.pred_ner_tags_sentences = list(e.pred_pos_tags_sentences)\n",
    "        ana_tagged_e.ana_tagged_sentences    = list(e.ana_tagged_sentences)\n",
    "        ana_tagged_e.pred_corefids           = list(e.pred_corefids)\n",
    "        ana_tagged_essays.append(ana_tagged_e)\n",
    "\n",
    "        # map coref ids to sent_ix, wd_ix tuples\n",
    "        corefid_2_chain = build_segmented_chain(e)\n",
    "    \n",
    "\n",
    "        # now look for ana tags that are also corefs, and cross reference\n",
    "        for sent_ix in range(len(e.sentences)):\n",
    "            ana_tagged_sent = []\n",
    "            ana_tagged_e.pred_tagged_sentences.append(ana_tagged_sent)\n",
    "\n",
    "            sent = e.sentences[sent_ix]\n",
    "\n",
    "            # SENTENCE LEVEL TAGS / PREDICTIONS\n",
    "            ana_tags = e.ana_tagged_sentences[sent_ix]\n",
    "            coref_ids = e.pred_corefids[sent_ix]\n",
    "            # ner_tags = e.pred_ner_tags_sentences[sent_ix]\n",
    "            pos_tags = e.pred_pos_tags_sentences[sent_ix]\n",
    "            ptags = e.pred_tagged_sentences[sent_ix]\n",
    "\n",
    "            for wd_ix in range(len(sent)):\n",
    "                pos_tag = pos_tags[wd_ix]  # POS tag\n",
    "\n",
    "                word, _ = sent[wd_ix]  # ignore actual tags\n",
    "                pred_cc_tag = ptags[wd_ix]  # predict cc tag\n",
    "\n",
    "                is_ana_tag = ana_tags[wd_ix] == ANAPHORA\n",
    "                wd_coref_ids = coref_ids[wd_ix]  # Set[str]\n",
    "\n",
    "                # note we are changing this to a set rather than a single string\n",
    "                wd_ptags = set()\n",
    "                # initialize predicted tags, inc. cc tag\n",
    "                # DON'T run continue until after this point\n",
    "                ana_tagged_sent.append(wd_ptags)\n",
    "\n",
    "                # add predicted concept code tag (filtered out by evaluation code, which filters to specific tags)\n",
    "                if pred_cc_tag != EMPTY:\n",
    "                    seq_pred_tags.append(pred_cc_tag)\n",
    "                    wd_ptags.add(pred_cc_tag)\n",
    "                \n",
    "    # validation check\n",
    "    #   check essay and sent lengths align\n",
    "    for e in ana_tagged_essays:\n",
    "        assert len(e.sentences) == len(e.pred_tagged_sentences)\n",
    "        for ix in range(len(e.sentences)):\n",
    "            assert len(e.sentences[ix]) == len(e.pred_tagged_sentences[ix])\n",
    "\n",
    "    return ana_tagged_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(essays, expected_tags):\n",
    "\n",
    "    rows_ana = []\n",
    "    proc_essays = processed_essays_predict_most_recent_tag(essays=essays)\n",
    "\n",
    "    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)\n",
    "    row = metrics[\"MICRO_F1\"]\n",
    "    rows_ana.append(row)\n",
    "\n",
    "    df_results = pd.DataFrame(rows_ana)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(essays, expected_tags):\n",
    "\n",
    "    proc_essays = processed_essays_predict_most_recent_tag(essays=essays)\n",
    "                        \n",
    "    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)\n",
    "    row = metrics[\"MICRO_F1\"]\n",
    "    df_results = pd.DataFrame([row])\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_essays_replaced = processed_essays_replace_ana_tags_with_regular(training_essays)\n",
    "test_essays_replaced  = processed_essays_replace_ana_tags_with_regular(test_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.833163</td>\n",
       "      <td>0.846703</td>\n",
       "      <td>0.820049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score  precision    recall\n",
       "0  0.833163   0.846703  0.820049"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = grid_search(essays=training_essays, expected_tags=reg_tags)\n",
    "process_sort_results(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Ana Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82928</td>\n",
       "      <td>0.846889</td>\n",
       "      <td>0.812388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score  precision    recall\n",
       "0   0.82928   0.846889  0.812388"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = grid_search(essays=train_essays_replaced, expected_tags=reg_tags)\n",
    "process_sort_results(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849319</td>\n",
       "      <td>0.849257</td>\n",
       "      <td>0.849382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score  precision    recall\n",
       "0  0.849319   0.849257  0.849382"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = grid_search(essays=test_essays, expected_tags=reg_tags)\n",
    "process_sort_results(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Replaced Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.847137</td>\n",
       "      <td>0.849257</td>\n",
       "      <td>0.845027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score  precision    recall\n",
       "0  0.847137   0.849257  0.845027"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = grid_search(essays=test_essays_replaced, expected_tags=reg_tags)\n",
    "process_sort_results(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
