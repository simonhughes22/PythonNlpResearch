{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    " - Take the merged predictions and evaluate the prediction accuracy using the 2 different approaches\n",
    " 1. Look at the anaphora tags and then cross-reference co-reference labels\n",
    " 2. Use the co-reference chains directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "CoRef Data:  /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pandas as pd\n",
    "\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "from results_common import get_essays, validate_essays, tally_essay_attributes\n",
    "from process_essays_coref import get_coref_processed_essays\n",
    "from metrics import get_metrics_raw\n",
    "\n",
    "# progress bar widget\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "DATASET = \"CoralBleaching\" # CoralBleaching | SkinCancer\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "stanford_coref_predictions_folder = root_folder + \"CoReference/\"\n",
    "print(\"CoRef Data: \", stanford_coref_predictions_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/training_processed.dill\n"
     ]
    }
   ],
   "source": [
    "training_essays = get_essays(stanford_coref_predictions_folder, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file /Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/CoReference/test_processed.dill\n"
     ]
    }
   ],
   "source": [
    "test_essays = get_essays(stanford_coref_predictions_folder, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = training_essays + test_essays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essays validated\n",
      "Essays validated\n"
     ]
    }
   ],
   "source": [
    "validate_essays(training_essays)\n",
    "validate_essays(test_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_tally = tally_essay_attributes(all_essays, attribute_name=\"pred_ner_tags_sentences\")\n",
    "pos_tally = tally_essay_attributes(all_essays, attribute_name=\"pred_pos_tags_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Anaphor Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anaphor:[11]',\n",
       " 'Anaphor:[12]',\n",
       " 'Anaphor:[13]',\n",
       " 'Anaphor:[14]',\n",
       " 'Anaphor:[1]',\n",
       " 'Anaphor:[2]',\n",
       " 'Anaphor:[3]',\n",
       " 'Anaphor:[4]',\n",
       " 'Anaphor:[50]',\n",
       " 'Anaphor:[5]',\n",
       " 'Anaphor:[5b]',\n",
       " 'Anaphor:[6]',\n",
       " 'Anaphor:[7]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import is_a_regular_code\n",
    "\n",
    "cc_tally = defaultdict(int)\n",
    "cr_tally = defaultdict(int)\n",
    "reg_tally = defaultdict(int)\n",
    "for e in all_essays:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                if is_a_regular_code(t):\n",
    "                    reg_tally[t] += 1\n",
    "                if ANAPHORA in t and \"other\" not in t:\n",
    "                    if \"->\" in t:\n",
    "                        cr_tally[t] += 1\n",
    "                    elif \"Anaphor:[\" in t:\n",
    "                        cc_tally[t] += 1\n",
    "\n",
    "reg_tags = sorted(reg_tally.keys())\n",
    "all_ana_tags = sorted(cc_tally.keys())\n",
    "assert len(reg_tags) == len(all_ana_tags)\n",
    "all_ana_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = \"Look back\"\n",
    "MAX_ANA_PHRASE = \"Max ana phrase\"\n",
    "MAX_CHAIN_PHRASE = \"Max chain phrase\"\n",
    "POS_ANA_FLTR = \"POS ana filter\"\n",
    "POS_CHAIN_FLTR = \"Pos chain filter\"\n",
    "\n",
    "def blank_if_none(val):\n",
    "    return \"-\" if (val is None or not val or str(val).lower() == \"none\") else val\n",
    "\n",
    "def process_sort_results(df_results):\n",
    "    df_disp = df_results[[\"f1_score\",\"precision\",\"recall\", MAX_ANA_PHRASE, MAX_CHAIN_PHRASE, POS_ANA_FLTR, POS_CHAIN_FLTR]]\n",
    "    return df_disp.sort_values(\"f1_score\", ascending=False)\n",
    "\n",
    "def filter_test_results_to_best_training_results(df_train_raw, df_test_raw):\n",
    "    # make sure sorted to the top result\n",
    "    df_train_raw_sorted = df_train_raw.sort_values(\"f1_score\", ascending=False, inplace=False)\n",
    "    top_row = df_train_raw_sorted.iloc[0]\n",
    "    filtered_df = df_test_raw[df_test_raw[LOOK_BACK]        == top_row[LOOK_BACK]]\n",
    "    filtered_df = filtered_df[filtered_df[MAX_ANA_PHRASE]   == top_row[MAX_ANA_PHRASE]]\n",
    "    filtered_df = filtered_df[filtered_df[MAX_CHAIN_PHRASE] == top_row[MAX_CHAIN_PHRASE]]\n",
    "    filtered_df = filtered_df[filtered_df[POS_ANA_FLTR]     == top_row[POS_ANA_FLTR]]\n",
    "    filtered_df = filtered_df[filtered_df[POS_CHAIN_FLTR]   == top_row[POS_CHAIN_FLTR]]\n",
    "    print(len(filtered_df))\n",
    "    return process_sort_results(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare POS Tag Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NN', 'NNP', 'NNPS', 'NNS'},\n",
       " {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'},\n",
       " {'PRP', 'PRP$', 'WP', 'WP$'},\n",
       " {'DT', 'PDT', 'WDT'},\n",
       " {'DT', 'PDT', 'PRP', 'PRP$', 'WDT', 'WP', 'WP$'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_nouns = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"NN\"])\n",
    "pos_verbs = set([pos for pos in pos_tally.keys() if pos.strip()[:2] == \"VB\"])\n",
    "pos_pronouns = {\"PRP\",\"PRP$\", \"WP\", \"WP$\"}\n",
    "pos_determiners = {\"DT\",\"WDT\",\"PDT\"} # the, a, which, that, etc\n",
    "pos_pron_dt = pos_pronouns | pos_determiners\n",
    "# for meaning of pen treebank tags - https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "pos_nouns, pos_verbs, pos_pronouns, pos_determiners, pos_pron_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pos_filter = {\n",
    "            \"None\": None,\n",
    "            \"PRN\": pos_pronouns,\n",
    "            \"DT\": pos_determiners,\n",
    "            \"PRN+DT\": pos_pron_dt\n",
    "}\n",
    "\n",
    "dict_pos_ch_filter = {\n",
    "    \"None\": None,\n",
    "    \"NN\": pos_nouns,\n",
    "    \"VB\": pos_verbs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_len = [None,1,2,3,5,10,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search With Anaphora Prediction Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(essays, format_ana_tags, filter_to_predicted_tags, expected_tags):\n",
    "\n",
    "    # set up progress bar\n",
    "    max_count = len(look_back_vals)  * len(phrase_len) * len(dict_pos_filter) * len(dict_pos_ch_filter)\n",
    "    iprogress_bar = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "    display(iprogress_bar) # display the bar\n",
    "\n",
    "    rows_ana = []\n",
    "    \n",
    "    for pos_ana_key, pos_ana_filter in dict_pos_filter.items():\n",
    "        for pos_ch_key, pos_ch_filter in dict_pos_ch_filter.items():                \n",
    "            for max_ana_phrase_len in phrase_len:\n",
    "                for max_cref_phrase_len in phrase_len:\n",
    "                    \n",
    "                    proc_essays = get_coref_processed_essays(\n",
    "                        essays=essays, format_ana_tags=format_ana_tags, \n",
    "                        ner_ch_filter=None, look_back_only=True,\n",
    "                        filter_to_predicted_tags=filter_to_predicted_tags, \n",
    "                        max_ana_phrase_len=max_ana_phrase_len, max_cref_phrase_len=max_cref_phrase_len, \n",
    "                        pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter)\n",
    "                    metrics = get_metrics_raw(proc_essays, expected_tags=expected_tags,  micro_only=True)\n",
    "                    row = metrics[\"MICRO_F1\"]\n",
    "                    row[MAX_ANA_PHRASE] = blank_if_none(max_ana_phrase_len)\n",
    "                    row[MAX_CHAIN_PHRASE] = blank_if_none(max_cref_phrase_len)\n",
    "                    row[POS_ANA_FLTR]     = blank_if_none(pos_ana_key)\n",
    "                    row[POS_CHAIN_FLTR]   = blank_if_none(pos_ch_key)\n",
    "                    rows_ana.append(row)\n",
    "                    iprogress_bar.value += 1\n",
    "\n",
    "    df_results = pd.DataFrame(rows_ana)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f242ac74c148d6b45e89355da84632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0, max=168)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1bf3e8c324d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m df_train_raw = grid_search(essays=training_essays, \n\u001b[1;32m      5\u001b[0m                            \u001b[0mexpected_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_ana_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                            filter_to_predicted_tags=filter_to_predicted_tags, format_ana_tags=format_ana_tags)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-36814b70a209>\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(essays, format_ana_tags, filter_to_predicted_tags, expected_tags)\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0mfilter_to_predicted_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_to_predicted_tags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mmax_ana_phrase_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_ana_phrase_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cref_phrase_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_cref_phrase_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                         pos_ana_filter=pos_ana_filter, pos_ch_filter=pos_ch_filter)\n\u001b[0m\u001b[1;32m     21\u001b[0m                     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metrics_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_essays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_tags\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmicro_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MICRO_F1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/NlpResearch/PythonNlpResearch/notebooks/Co-Reference Resolution/Results/process_essays_coref.py\u001b[0m in \u001b[0;36mget_coref_processed_essays\u001b[0;34m(essays, format_ana_tags, filter_to_predicted_tags, look_back_only, max_ana_phrase_len, max_cref_phrase_len, ner_ch_filter, pos_ana_filter, pos_ch_filter)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m# note we are changing this to a set rather than a single string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mwd_ptags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# add predicted concept code tag (filtered out by evaluation code, which filters to specific tags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpred_cc_tag\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mEMPTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filter_to_predicted_tags = True\n",
    "format_ana_tags=True # Format tags with Anaphora[xyz]\n",
    "\n",
    "df_train_raw = grid_search(essays=training_essays, \n",
    "                           expected_tags=all_ana_tags,\n",
    "                           filter_to_predicted_tags=filter_to_predicted_tags, format_ana_tags=format_ana_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sort_results(df_train_raw).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_predicted_tags = True\n",
    "format_ana_tags=True # Format tags with Anaphora[xyz]\n",
    "\n",
    "df_test_raw = grid_search(essays=test_essays, \n",
    "                          expected_tags=all_ana_tags,\n",
    "                           filter_to_predicted_tags=filter_to_predicted_tags, format_ana_tags=format_ana_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sort_results(df_test_raw).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_test_results_to_best_training_results(df_train_raw, df_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search without Anaphora Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_predicted_tags = False\n",
    "format_ana_tags=True # Format tags with Anaphora[xyz]\n",
    "\n",
    "df_train_raw_cref = grid_search(essays=training_essays, \n",
    "                                expected_tags=all_ana_tags,\n",
    "                           filter_to_predicted_tags=filter_to_predicted_tags, format_ana_tags=format_ana_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sort_results(df_train_raw_cref).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_to_predicted_tags = False\n",
    "format_ana_tags=True # Format tags with Anaphora[xyz]\n",
    "\n",
    "df_test_raw_cref = grid_search(essays=test_essays, \n",
    "                               expected_tags=all_ana_tags,\n",
    "                           filter_to_predicted_tags=filter_to_predicted_tags, format_ana_tags=format_ana_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_sort_results(df_test_raw_cref).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_test_results_to_best_training_results(df_train_raw_cref, df_test_raw_cref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- ~~Make look-back-only always true~~\n",
    "  - **instead, add an option to pick the nearest preceding referent only**\n",
    "- ~~add a length filter to look at the length of the anaphor as well as the antecedents~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [[(4, 6), (4, 7), (4, 8)],\n",
       "  [(5, 0), (5, 1)],\n",
       "  [(5, 23), (5, 24), (5, 25)]],\n",
       " '2': [[(1, 11), (1, 12), (1, 13)]],\n",
       " '3': [[(3, 0), (3, 1)], [(3, 9)]]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from process_essays_coref import build_segmented_chain\n",
    "\n",
    "corefid_chain = build_segmented_chain(training_essays[10])\n",
    "corefid_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 6), (4, 7), (4, 8)]\n",
      "(4, 6) (4, 8)\n",
      "\n",
      "[(5, 0), (5, 1)]\n",
      "(5, 0) (5, 1)\n",
      "\n",
      "[(5, 23), (5, 24), (5, 25)]\n",
      "(5, 23) (5, 25)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chain = corefid_chain[\"1\"]\n",
    "for phrase in chain:\n",
    "    print(phrase)\n",
    "    print(phrase[0], phrase[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coref_length(corefid_chain,  corefid, sent_ix, word_ix):\n",
    "    chain = corefid_chain[corefid]\n",
    "    length = 0\n",
    "    for phrase in chain:\n",
    "        first_pair = phrase[0]\n",
    "        last_pair = phrase[-1]\n",
    "        first_sent, first_word = first_pair\n",
    "        last_sent,  last_word = last_pair\n",
    "        \n",
    "        # within same sentence\n",
    "        if sent_ix >= first_sent and sent_ix <= last_sent:\n",
    "            assert first_sent == last_sent, \"Phrase spans different sentences\"\n",
    "            if word_ix >= first_word and word_ix <= last_word:\n",
    "                length = max(length, (last_word-first_word)+1)\n",
    "    assert length > 0, \"can't find matching coref\"\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_coref_length(corefid_chain, \"1\", 5, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
