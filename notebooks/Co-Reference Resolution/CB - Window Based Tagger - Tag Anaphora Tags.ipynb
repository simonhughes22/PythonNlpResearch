{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import Settings\n",
    "from CrossValidation import cross_validation\n",
    "from Decorators import memoize_to_disk\n",
    "from featureextractionfunctions import *\n",
    "from featurevectorizer import FeatureVectorizer\n",
    "from load_data import load_process_essays, extract_features\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE_WD_FEATS     = True\n",
    "\n",
    "MIN_FEAT_FREQ       = 5        # 5 best so far\n",
    "CV_FOLDS            = 5\n",
    "\n",
    "MIN_TAG_FREQ        = 5\n",
    "LOOK_BACK           = 0     # how many sentences to look back when predicting tags\n",
    "\n",
    "WINDOW_SIZE         = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "settings = Settings.Settings()\n",
    "\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "folder =                            root_folder + \"Training/\"\n",
    "processed_essay_filename_prefix =   root_folder + \"Pickled/essays_proc_pickled_\"\n",
    "features_filename_prefix =          root_folder + \"Pickled/feats_pickled_\"\n",
    "\n",
    "config = get_config(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle Key: folder_/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Training/_include_normal_False_include_vague_True_lower_case_True_min_df_2_min_sentence_length_3_remove_infrequent_False_remove_punctuation_False_remove_stop_words_False_replace_nums_True_spelling_correct_True_stem_False_window_size_9\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" Load Essays \"\"\"\n",
    "mem_process_essays = memoize_to_disk(filename_prefix=processed_essay_filename_prefix)(load_process_essays)\n",
    "tagged_essays = mem_process_essays( **config )\n",
    "# \"\"\" End load Essays \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"window_size\"] = WINDOW_SIZE\n",
    "offset = int((config[\"window_size\"] - 1) / 2)\n",
    "\n",
    "unigram_bow_window = fact_extract_bow_ngram_features(offset, 1)\n",
    "unigram_window_stemmed = fact_extract_positional_word_features_stemmed(offset)\n",
    "bigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 2)\n",
    "trigram_window_stemmed = fact_extract_ngram_features_stemmed(offset, 3)\n",
    "pos_tag_window = fact_extract_positional_POS_features(offset)\n",
    "\n",
    "# optimal feats from the tuning\n",
    "feat_extractors = [\n",
    "        unigram_window_stemmed,\n",
    "        bigram_window_stemmed,\n",
    "        pos_tag_window,\n",
    "        unigram_bow_window,\n",
    "        trigram_window_stemmed\n",
    "    ]\n",
    "feat_config = dict(list(config.items()) + [(\"extractors\", feat_extractors)])\n",
    "\n",
    "\"\"\" LOAD FEATURES \"\"\"\n",
    "essay_feats = extract_features(tagged_essays, **feat_config)\n",
    "\n",
    "\"\"\" DEFINE TAGS \"\"\"\n",
    "_, lst_all_tags = flatten_to_wordlevel_feat_tags(essay_feats)\n",
    "regular_tags = list(set((t for t in flatten(lst_all_tags) if t.lower().strip() == \"anaphor\" )))\n",
    "\n",
    "\"\"\" works best with all the pair-wise causal relation codes \"\"\"\n",
    "wd_train_tags = regular_tags\n",
    "wd_test_tags = regular_tags\n",
    "\n",
    "\"\"\" CLASSIFIERS \"\"\"\n",
    "fn_create_wd_cls = lambda: LogisticRegression()  # C=1, dual = False seems optimal\n",
    "wd_algo = str(fn_create_wd_cls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tagger(essays_TD, essays_VD, wd_test_tags, wd_train_tags):\n",
    "    # TD and VD are lists of Essay objects. The sentences are lists\n",
    "    # of featureextractortransformer.Word objects\n",
    "    \"\"\" Data Partitioning and Training \"\"\"\n",
    "    td_feats, td_tags = flatten_to_wordlevel_feat_tags(essays_TD)\n",
    "    vd_feats, vd_tags = flatten_to_wordlevel_feat_tags(essays_VD)\n",
    "    feature_transformer = FeatureVectorizer(min_feature_freq=MIN_FEAT_FREQ, sparse=SPARSE_WD_FEATS)\n",
    "    td_X, vd_X = feature_transformer.fit_transform(td_feats), feature_transformer.transform(vd_feats)\n",
    "    wd_td_ys_bytag = get_wordlevel_ys_by_code(td_tags, wd_train_tags)\n",
    "    wd_vd_ys_bytag = get_wordlevel_ys_by_code(vd_tags, wd_train_tags)\n",
    "    \"\"\" TRAIN Tagger \"\"\"\n",
    "    tag2word_classifier = train_classifier_per_code(td_X, wd_td_ys_bytag, lambda: LogisticRegression(),\n",
    "                                                    wd_train_tags, verbose=False)\n",
    "    \"\"\" TEST Tagger \"\"\"\n",
    "    td_wd_predictions_by_code = test_classifier_per_code(td_X, tag2word_classifier, wd_test_tags)\n",
    "    vd_wd_predictions_by_code = test_classifier_per_code(vd_X, tag2word_classifier, wd_test_tags)\n",
    "    return td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = cross_validation(essay_feats, CV_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 3.89 s, total: 1min 40s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Gather metrics per fold\n",
    "cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "essays_preds = []\n",
    "for (essays_TD, essays_VD) in folds:\n",
    "    \n",
    "    result = train_tagger(essays_TD, essays_VD, wd_test_tags, wd_train_tags)    \n",
    "    td_wd_predictions_by_code, vd_wd_predictions_by_code, wd_td_ys_bytag, wd_vd_ys_bytag = result\n",
    "    \n",
    "    essays_preds.append( (essays_VD, vd_wd_predictions_by_code) )\n",
    "    \n",
    "    merge_dictionaries(wd_td_ys_bytag, cv_wd_td_ys_by_tag)\n",
    "    merge_dictionaries(wd_vd_ys_bytag, cv_wd_vd_ys_by_tag)\n",
    "    merge_dictionaries(td_wd_predictions_by_code, cv_wd_td_predictions_by_tag)\n",
    "    merge_dictionaries(vd_wd_predictions_by_code, cv_wd_vd_predictions_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(essays_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO - train with optimal hyper params, assign tags to the essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
