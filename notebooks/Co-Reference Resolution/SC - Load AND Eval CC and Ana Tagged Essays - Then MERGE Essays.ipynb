{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pickle\n",
    "import pathlib\n",
    "from pprint import pprint\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from Settings import Settings\n",
    "from window_based_tagger_config import get_config\n",
    "from FindFiles import find_files\n",
    "from DirUtils import dir_exists\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY_TAG = \"Empty\"\n",
    "ANAPHORA = \"Anaphor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.4 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.3.0\r\n",
      "anaconda-client==1.6.11\r\n",
      "appnope==0.1.0\r\n",
      "argcomplete==1.9.4\r\n",
      "asn1crypto==0.24.0\r\n",
      "astor==0.7.1\r\n",
      "beautifulsoup4==4.6.0\r\n",
      "bleach==2.1.2\r\n",
      "boto==2.47.0\r\n",
      "boto3==1.5.36\r\n",
      "botocore==1.8.50\r\n",
      "bz2file==0.98\r\n",
      "certifi==2018.1.18\r\n",
      "cffi==1.11.4\r\n",
      "chardet==3.0.4\r\n",
      "clyent==1.2.2\r\n",
      "costcla==0.5\r\n",
      "cryptography==2.1.4\r\n",
      "cycler==0.10.0\r\n",
      "cymem==1.31.2\r\n",
      "cytoolz==0.8.2\r\n",
      "decorator==4.2.1\r\n",
      "dicecore==1.13\r\n",
      "dill==0.2.8.2\r\n",
      "docutils==0.14\r\n",
      "entrypoints==0.2.3\r\n",
      "ftfy==4.4.3\r\n",
      "gast==0.2.0\r\n",
      "gensim==0.13.4\r\n",
      "grpcio==1.14.0\r\n",
      "h5py==2.7.0\r\n",
      "hdbscan==0.8.12\r\n",
      "html5lib==1.0.1\r\n",
      "idna==2.6\r\n",
      "ipykernel==4.8.2\r\n",
      "ipython==6.2.1\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.1.2\r\n",
      "jedi==0.11.1\r\n",
      "Jinja2==2.10\r\n",
      "jmespath==0.9.3\r\n",
      "joblib==0.9.4\r\n",
      "jsonschema==2.6.0\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client==5.2.2\r\n",
      "jupyter-console==5.2.0\r\n",
      "jupyter-core==4.4.0\r\n",
      "jupyterlab==0.31.8\r\n",
      "jupyterlab-launcher==0.10.5\r\n",
      "Keras==1.2.2\r\n",
      "Keras-Applications==1.0.4\r\n",
      "Keras-Preprocessing==1.0.2\r\n",
      "Markdown==2.6.11\r\n",
      "MarkupSafe==1.0\r\n",
      "matplotlib==2.0.0\r\n",
      "mistune==0.8.3\r\n",
      "murmurhash==0.26.4\r\n",
      "nb-anacondacloud==1.4.0\r\n",
      "nb-conda==2.2.1\r\n",
      "nb-conda-kernels==2.1.0\r\n",
      "nbconvert==5.3.1\r\n",
      "nbformat==4.4.0\r\n",
      "nbpresent==3.0.2\r\n",
      "nltk==3.2.2\r\n",
      "nose==1.3.7\r\n",
      "notebook==5.4.0\r\n",
      "numpy==1.14.1\r\n",
      "pandas==0.22.0\r\n",
      "pandocfilters==1.4.2\r\n",
      "parso==0.1.1\r\n",
      "pathlib==1.0.1\r\n",
      "pexpect==4.4.0\r\n",
      "pickleshare==0.7.4\r\n",
      "Pillow==5.0.0\r\n",
      "plac==0.9.6\r\n",
      "preshed==1.0.0\r\n",
      "prompt-toolkit==1.0.15\r\n",
      "protobuf==3.6.0\r\n",
      "ptyprocess==0.5.2\r\n",
      "pycparser==2.18\r\n",
      "pyea==0.2\r\n",
      "Pygments==2.2.0\r\n",
      "pymongo==3.4.0\r\n",
      "pyOpenSSL==17.5.0\r\n",
      "pyparsing==2.2.0\r\n",
      "PySocks==1.6.7\r\n",
      "python-dateutil==2.6.1\r\n",
      "pytz==2018.3\r\n",
      "PyYAML==3.12\r\n",
      "pyzmq==17.0.0\r\n",
      "qtconsole==4.3.1\r\n",
      "regex==2017.4.5\r\n",
      "requests==2.14.2\r\n",
      "s3transfer==0.1.13\r\n",
      "scikit-learn==0.18.1\r\n",
      "scikit-optimize==0.3\r\n",
      "scipy==1.0.0\r\n",
      "seaborn==0.7.1\r\n",
      "Send2Trash==1.5.0\r\n",
      "simplegeneric==0.8.1\r\n",
      "six==1.11.0\r\n",
      "smart-open==1.5.6\r\n",
      "spacy==1.8.2\r\n",
      "tensorboard==1.9.0\r\n",
      "tensorflow==1.9.0\r\n",
      "termcolor==1.1.0\r\n",
      "terminado==0.8.1\r\n",
      "testpath==0.3.1\r\n",
      "Theano==1.0.2\r\n",
      "thinc==6.5.2\r\n",
      "toolz==0.9.0\r\n",
      "tornado==4.5.3\r\n",
      "tqdm==4.19.5\r\n",
      "traitlets==4.3.2\r\n",
      "ujson==1.35\r\n",
      "urllib3==1.22\r\n",
      "wcwidth==0.1.7\r\n",
      "webencodings==0.5.1\r\n",
      "Werkzeug==0.14.1\r\n",
      "widgetsnbextension==3.1.4\r\n",
      "wordcloud==1.3.1\r\n",
      "wrapt==1.10.11\r\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "#DATASET = \"CoralBleaching\"\n",
    "DATASET = \"SkinCancer\"\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "training_pickled = settings.data_directory + DATASET + \"/Thesis_Dataset/training.pl\"\n",
    "\n",
    "# PREDICTIONS FOLDERS\n",
    "anaphor_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/\"\n",
    "\n",
    "# USE the \"-Fixed\" ones, these are much worse\n",
    "#anaphor_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-Anaphora_Tags-Binary/\"\n",
    "\n",
    "tag_predictions_folder = root_folder + \"Predictions/Bi-LSTM_fixed/\"\n",
    "#tag_predictions_folder = root_folder + \"Predictions/Bi-LSTM-4-SEARN/\"\n",
    "merged_predictions_folder = root_folder + \"Predictions/CoRef/MergedTags/\"\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid\n"
     ]
    }
   ],
   "source": [
    "# ensure paths exist\n",
    "pathlib.Path(merged_predictions_folder).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "assert dir_exists(merged_predictions_folder)\n",
    "print(\"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid\n"
     ]
    }
   ],
   "source": [
    "assert dir_exists(anaphor_predictions_folder)\n",
    "print(\"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/Bi-LSTM_fixed/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid\n"
     ]
    }
   ],
   "source": [
    "assert dir_exists(tag_predictions_folder)\n",
    "print(\"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tagged essays\n",
    "def load_tagged_essays(folder, pattern):\n",
    "    files = find_files(folder, pattern)\n",
    "    # multiple runs with different hidden layer sizes?\n",
    "    if len(files) > 2:        \n",
    "        for f in files:\n",
    "            print(f)\n",
    "    assert len(files) == 2, \"Wrong number of tagged files:\" + str(len(files))\n",
    "    for f in files:\n",
    "        assert \"_train_\" in f or \"_test_\" in f, \"Wrong files matched\"\n",
    "    train_tagged_fname = [f for file in files if \"_train_\" in f][0]\n",
    "    test_tagged_fname = list(set(files).difference([train_tagged_fname]))[0]\n",
    "    print(\"Train: {fname}\".format(fname=train_tagged_fname))\n",
    "    print(\"Test:  {fname}\".format(fname=test_tagged_fname))\n",
    "\n",
    "    # NOTE - is this throws an error, upgrade to dill 2.8.2. Version 2.6 had a bug in it\n",
    "    with open(train_tagged_fname, \"rb\") as f:\n",
    "        tagged_essays_train = dill.load(f)\n",
    "    with open(test_tagged_fname, \"rb\") as f:\n",
    "        tagged_essays_test  = dill.load(f)\n",
    "    return (tagged_essays_train, tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-1_use_pretrained_embedding-True.dill\n",
      "Test:  /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/Bi-LSTM-4-Anaphora_Tags-Binary-Fixed/essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-1_use_pretrained_embedding-True.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for optimal vd params - see http://localhost:8888/notebooks/Mongo%20Queries/Query%20Model%20Hyper%20Parameter%20Tuning%20Results-Anaphora%20Tagger.ipynb#\n",
    "# NOTE that 1 layer is optimal for SC, 2 for CB\n",
    "#pattern = \"essays_.*_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "pattern = \"essays_.*_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-1_use_pretrained_embedding-True.dill\"\n",
    "ana_tagged_tr, ana_tagged_test = load_tagged_essays(anaphor_predictions_folder, pattern=pattern)\n",
    "len(ana_tagged_tr), len(ana_tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/Bi-LSTM_fixed/essays_train_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\n",
      "Test:  /Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/Bi-LSTM_fixed/essays_test_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load concept code (cc) tagged essays\n",
    "# see mongo collection - metrics_codes.STORE_RESULTS_SC_TAGGING_VD_RNN_MOST_COMMON_TAG\n",
    "pattern = \"essays_.*_bi_directional-True_hidden_size-256_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.dill\"\n",
    "cc_tagged_tr, cc_tagged_test = load_tagged_essays(tag_predictions_folder, pattern=pattern)\n",
    "len(cc_tagged_tr), len(cc_tagged_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Essays (Untagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 files found\n",
      "870 essays processed\n",
      "218 files found\n",
      "218 essays processed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do I need to do this? good for validation below, but not needed otherwise\n",
    "# with open(training_pickled, \"rb+\") as f:\n",
    "#     untagged_essays_train = pickle.load(f)\n",
    "\n",
    "untagged_essays_train = load_process_essays(**config)\n",
    "\n",
    "test_config = get_config(test_folder)\n",
    "untagged_essays_test = load_process_essays(**test_config)\n",
    "\n",
    "len(untagged_essays_train), len(untagged_essays_test) # 902, 226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'folder': '/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Training/',\n",
       " 'include_normal': False,\n",
       " 'include_vague': True,\n",
       " 'lower_case': True,\n",
       " 'min_df': 2,\n",
       " 'min_sentence_length': 3,\n",
       " 'remove_infrequent': False,\n",
       " 'remove_punctuation': False,\n",
       " 'remove_stop_words': False,\n",
       " 'replace_nums': True,\n",
       " 'spelling_correct': True,\n",
       " 'stem': False,\n",
       " 'window_size': 9}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate all essay sets are equal and the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_the_same(essay_sets):\n",
    "    unique_fnames = [] # list of sets of str (fnames)\n",
    "    for essay_collection in essay_sets:\n",
    "        names = set()\n",
    "        for e in essay_collection:\n",
    "            names.add(e.name)\n",
    "        unique_fnames.append(names)\n",
    "    for a in unique_fnames:\n",
    "        print(len(a))\n",
    "        for b in unique_fnames:\n",
    "            assert len(a) == len(b), \"lens don't match\"\n",
    "            assert a == b, \"don't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870\n",
      "870\n",
      "870\n"
     ]
    }
   ],
   "source": [
    "names_the_same([ana_tagged_tr, cc_tagged_tr, untagged_essays_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "218\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "names_the_same([ana_tagged_test, cc_tagged_test, untagged_essays_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essays_2_hash_map(essays):\n",
    "    lu = {}\n",
    "    for e in essays:\n",
    "        lu[e.name] = e\n",
    "    return lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks the number of words and sentences are the same for 2 sets of essays\n",
    "def validate_tagged_essays(essays_a, essays_b, tags_should_match=True):\n",
    "    # make sure obj is not the same\n",
    "    assert essays_a != essays_b\n",
    "    print(\"Validating\", len(essays_a), \"essays\")\n",
    "    assert len(essays_a) == len(essays_b), \"Lens don't match\"\n",
    "    \n",
    "    a_hmap = essays_2_hash_map(essays_a)\n",
    "    b_hmap = essays_2_hash_map(essays_b)\n",
    "    \n",
    "    # same essays?\n",
    "    assert a_hmap.keys() == b_hmap.keys()\n",
    "    intersect = set(a_hmap.keys()).intersection(b_hmap.keys())\n",
    "    assert len(intersect) == len(a_hmap.keys())\n",
    "    assert len(a_hmap.keys()) > 1    \n",
    "    assert len(a_hmap.keys()) == len(b_hmap.keys())\n",
    "    \n",
    "    word_misses = 0\n",
    "    \n",
    "    for key, a_essay in a_hmap.items():\n",
    "        b_essay = b_hmap[key]\n",
    "        # assert NOT the same obj ref\n",
    "        assert a_essay != b_essay\n",
    "        assert len(a_essay.sentences) == len(b_essay.sentences)\n",
    "        assert len(a_essay.sentences) > 0\n",
    "        assert len(b_essay.sentences) > 0\n",
    "        for i in range(len(a_essay.sentences)):\n",
    "            a_sent = a_essay.sentences[i]\n",
    "            b_sent = b_essay.sentences[i]\n",
    "            # the same lists?\n",
    "            #assert a_sent == b_sent\n",
    "            assert len(a_sent) == len(b_sent)\n",
    "            if not len(a_sent) == len(b_sent):\n",
    "                print(key, \"\\tsent-ix:\", i, \"lens\", len(a_sent), len(b_sent))\n",
    "            for wd_ix, (a_wd, a_tags) in enumerate(a_sent):\n",
    "                b_wd, b_tags = b_sent[wd_ix]\n",
    "                if a_wd != b_wd:\n",
    "                    word_misses+=1\n",
    "                assert a_wd   == b_wd,   \\\n",
    "                    \"Words don't match: '{a}' - '{b}', Esssay: {essay} Sent Ix: {i}\".format(\n",
    "                            a=a_wd, b=b_wd, essay=key, i=i)\n",
    "                \n",
    "                # SH - Make conditional, as untagged essays contain new anaphora tags\n",
    "                if tags_should_match:                \n",
    "                    assert a_tags == b_tags, \\\n",
    "                        \"Tags don't match: '{a}' - '{b}', Esssay: {essay} Sent Ix: {i}\".format(\n",
    "                            a=str(a_tags), b=str(b_tags), essay=key, i=i)\n",
    "                else:\n",
    "                    intersectn = a_tags.intersection(b_tags)\n",
    "                    # smaller set should match intersection i.e. be a subset of larger one\n",
    "                    # will only differ due to new anaphora tags\n",
    "                    if len(b_tags) <= len(a_tags):\n",
    "                        assert intersectn == b_tags\n",
    "                    else:\n",
    "                        assert intersectn == a_tags\n",
    "                        \n",
    "    if word_misses:\n",
    "        print(\"Word miss-matches: \", word_misses)\n",
    "    print(\"Validation Passed\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 870 essays\n",
      "Validation Passed\n",
      "Validating 870 essays\n",
      "Validation Passed\n",
      "Validating 218 essays\n",
      "Validation Passed\n",
      "Validating 218 essays\n",
      "Validation Passed\n"
     ]
    }
   ],
   "source": [
    "validate_tagged_essays(untagged_essays_train, ana_tagged_tr, tags_should_match=False)\n",
    "validate_tagged_essays(cc_tagged_tr, ana_tagged_tr)\n",
    "validate_tagged_essays(untagged_essays_test, ana_tagged_test, tags_should_match=False)\n",
    "validate_tagged_essays(cc_tagged_test, ana_tagged_test, tags_should_match=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Two Sets of Tagged Essays\n",
    "* The anaphora essays were not tagged with concept codes, and vice versa, so need to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 218)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tagged_essays(untagged, tagged_ana, tagged_cc):\n",
    "    untagged_hmap = essays_2_hash_map(untagged)\n",
    "    ana_hmap = essays_2_hash_map(tagged_ana)\n",
    "    cc_hmap = essays_2_hash_map(tagged_cc)\n",
    "    \n",
    "    assert ana_hmap.keys() == cc_hmap.keys()\n",
    "    assert len(ana_hmap.keys()) == len(cc_hmap.keys())\n",
    "    \n",
    "    for key, cc_essay in cc_hmap.items():\n",
    "        ana_essay = ana_hmap[key]\n",
    "        untag_essay = untagged_hmap[key]\n",
    "        assert len(ana_essay.sentences) == len(cc_essay.sentences)\n",
    "        untag_essay.ana_tagged_sentences  = ana_essay.pred_tagged_sentences\n",
    "        untag_essay.pred_tagged_sentences =  cc_essay.pred_tagged_sentences\n",
    "        for i in range(len(ana_essay.sentences)):\n",
    "            ana_sent = ana_essay.sentences[i]\n",
    "            ana_ptags = ana_essay.pred_tagged_sentences[i]\n",
    "            cc_sent = ana_essay.sentences[i]\n",
    "            cc_ptags = cc_essay.pred_tagged_sentences[i]\n",
    "            assert len(ana_sent) == len(cc_sent)\n",
    "            assert len(cc_sent) == len(cc_ptags)\n",
    "    return untagged\n",
    "\n",
    "# Train\n",
    "merged_essays_tr   = merge_tagged_essays(\n",
    "    untagged=untagged_essays_train, \n",
    "    tagged_ana=ana_tagged_tr,   \n",
    "    tagged_cc=cc_tagged_tr)\n",
    "\n",
    "# Test\n",
    "merged_essays_test = merge_tagged_essays(\n",
    "    untagged=untagged_essays_test,\n",
    "    tagged_ana=ana_tagged_test, \n",
    "    tagged_cc=cc_tagged_test)\n",
    "len(merged_essays_tr), len(merged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Compute Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 19, 19)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = untagged_essays_train[0]\n",
    "len(e.sentences),len(e.pred_tagged_sentences),len(e.ana_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = untagged_essays_test[0]\n",
    "len(e.sentences),len(e.pred_tagged_sentences),len(e.ana_tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_tally(tally):\n",
    "    total = sum(tally.values())\n",
    "    norm_tally = {}\n",
    "    for tag, freq in tally.items():\n",
    "        norm_tally[tag] = freq/total\n",
    "    return norm_tally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '11', '12', '50']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tally = defaultdict(int)\n",
    "for e in untagged_essays_train:\n",
    "    for sent in e.sentences:\n",
    "        for wd, tags in sent:\n",
    "            for t in tags:\n",
    "                tally[t] +=1\n",
    "\n",
    "lst_all_tags = list(tally.keys())\n",
    "regular_tags = sorted(set((t for t in lst_all_tags if t[0].isdigit())), key = lambda s: int(s.replace('b','')))\n",
    "assert EMPTY_TAG not in regular_tags, \"Empty tag in list of regular tags\"\n",
    "regular_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Label Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 0.14999496661185865),\n",
       " ('11', 0.01171101640884534),\n",
       " ('12', 0.010838562464346835),\n",
       " ('2', 0.1533505587060837),\n",
       " ('3', 0.09439280561054998),\n",
       " ('4', 0.07409147344048858),\n",
       " ('5', 0.13341834166638702),\n",
       " ('50', 0.2954263279755713),\n",
       " ('6', 0.07677594711586859)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_tally = dict([k,v] for k,v in tally.items() if k in set(regular_tags))\n",
    "sorted(norm_tally(cc_tally).items(), key = lambda tpl: tpl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 0.15222876366694701),\n",
       " ('11', 0.00963694981777404),\n",
       " ('12', 0.009882253994953743),\n",
       " ('2', 0.1559784132324082),\n",
       " ('3', 0.09668488926268573),\n",
       " ('4', 0.0787426408746846),\n",
       " ('5', 0.13589851415755536),\n",
       " ('50', 0.2989206616204093),\n",
       " ('6', 0.062026913372582)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptag_tally = defaultdict(int)\n",
    "for e in merged_essays_tr:   \n",
    "    for sent in e.pred_tagged_sentences:\n",
    "        for tag in sent:\n",
    "            ptag_tally[tag] +=1\n",
    "            \n",
    "norm_ptag_tally = norm_tally(dict([(k,v) for k,v in ptag_tally.items() if k != EMPTY_TAG]))\n",
    "sorted(norm_ptag_tally.items(), key = lambda tpl: tpl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Empty', '50', '1', '2', '12', '3', '5', '4', '6', '11']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ptag_tally.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Metrics on Concept Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.815067</td>\n",
       "      <td>0.826888</td>\n",
       "      <td>0.803579</td>\n",
       "      <td>0.988795</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>0.746795</td>\n",
       "      <td>0.847273</td>\n",
       "      <td>0.667622</td>\n",
       "      <td>0.998914</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.641322</td>\n",
       "      <td>0.687943</td>\n",
       "      <td>0.600619</td>\n",
       "      <td>0.998508</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.791043</td>\n",
       "      <td>0.801618</td>\n",
       "      <td>0.780744</td>\n",
       "      <td>0.987042</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0.808686</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.800924</td>\n",
       "      <td>0.992672</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.777553</td>\n",
       "      <td>0.770806</td>\n",
       "      <td>0.784420</td>\n",
       "      <td>0.993188</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.854596</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.844064</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.868005</td>\n",
       "      <td>0.881946</td>\n",
       "      <td>0.854498</td>\n",
       "      <td>0.984272</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.758502</td>\n",
       "      <td>0.869492</td>\n",
       "      <td>0.672640</td>\n",
       "      <td>0.993263</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.822463</td>\n",
       "      <td>0.840692</td>\n",
       "      <td>0.805007</td>\n",
       "      <td>0.992089</td>\n",
       "      <td>1309239.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  f1_score  precision    recall  accuracy  data_points\n",
       "0          1  0.815067   0.826888  0.803579  0.988795     145471.0\n",
       "7         11  0.746795   0.847273  0.667622  0.998914     145471.0\n",
       "3         12  0.641322   0.687943  0.600619  0.998508     145471.0\n",
       "8          2  0.791043   0.801618  0.780744  0.987042     145471.0\n",
       "6          3  0.808686   0.816600  0.800924  0.992672     145471.0\n",
       "5          4  0.777553   0.770806  0.784420  0.993188     145471.0\n",
       "4          5  0.854596   0.865395  0.844064  0.992150     145471.0\n",
       "1         50  0.868005   0.881946  0.854498  0.984272     145471.0\n",
       "2          6  0.758502   0.869492  0.672640  0.993263     145471.0\n",
       "13  MICRO_F1  0.822463   0.840692  0.805007  0.992089    1309239.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from results_procesor import metrics_to_df\n",
    "import numpy as np\n",
    "METRICS_COLS = [\"code\",\"f1_score\", \"precision\", \"recall\", \"accuracy\", \"data_points\"]\n",
    "\n",
    "reg_tr_metrics = ResultsProcessor.compute_mean_metrics_from_tagged_essays(cc_tagged_tr, regular_tags)\n",
    "m_df = metrics_to_df(reg_tr_metrics)\n",
    "m_df = m_df[np.isin(m_df[\"code\"], regular_tags + [__MICRO_F1__])][METRICS_COLS]\n",
    "m_df.sort_values(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.867039</td>\n",
       "      <td>0.807492</td>\n",
       "      <td>0.991413</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.998927</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.646707</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0.865702</td>\n",
       "      <td>0.856851</td>\n",
       "      <td>0.874739</td>\n",
       "      <td>0.992656</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0.843869</td>\n",
       "      <td>0.849693</td>\n",
       "      <td>0.838124</td>\n",
       "      <td>0.994209</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>0.772031</td>\n",
       "      <td>0.787109</td>\n",
       "      <td>0.757519</td>\n",
       "      <td>0.993277</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.863286</td>\n",
       "      <td>0.827696</td>\n",
       "      <td>0.902074</td>\n",
       "      <td>0.992995</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>0.875325</td>\n",
       "      <td>0.888142</td>\n",
       "      <td>0.862873</td>\n",
       "      <td>0.985114</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0.746060</td>\n",
       "      <td>0.800752</td>\n",
       "      <td>0.698361</td>\n",
       "      <td>0.991808</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.840573</td>\n",
       "      <td>0.851273</td>\n",
       "      <td>0.830139</td>\n",
       "      <td>0.993192</td>\n",
       "      <td>318618.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        code  f1_score  precision    recall  accuracy  data_points\n",
       "0          1  0.836207   0.867039  0.807492  0.991413      35402.0\n",
       "7         11  0.641509   0.850000  0.515152  0.998927      35402.0\n",
       "3         12  0.646707   0.683544  0.613636  0.998333      35402.0\n",
       "8          2  0.865702   0.856851  0.874739  0.992656      35402.0\n",
       "6          3  0.843869   0.849693  0.838124  0.994209      35402.0\n",
       "5          4  0.772031   0.787109  0.757519  0.993277      35402.0\n",
       "4          5  0.863286   0.827696  0.902074  0.992995      35402.0\n",
       "1         50  0.875325   0.888142  0.862873  0.985114      35402.0\n",
       "2          6  0.746060   0.800752  0.698361  0.991808      35402.0\n",
       "13  MICRO_F1  0.840573   0.851273  0.830139  0.993192     318618.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_test_metrics = ResultsProcessor.compute_mean_metrics_from_tagged_essays(cc_tagged_test, regular_tags)\n",
    "m_df = metrics_to_df(reg_test_metrics)\n",
    "m_df = m_df[np.isin(m_df[\"code\"], regular_tags + [__MICRO_F1__])][METRICS_COLS]\n",
    "m_df.sort_values(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Metrics on Ana Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ana_tags = [ANAPHORA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.350575</td>\n",
       "      <td>0.552036</td>\n",
       "      <td>0.256842</td>\n",
       "      <td>0.996893</td>\n",
       "      <td>145471.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code  f1_score  precision    recall  accuracy  data_points\n",
       "5  MICRO_F1  0.350575   0.552036  0.256842  0.996893     145471.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_tr_metrics = ResultsProcessor.compute_mean_metrics_from_tagged_essays(ana_tagged_tr, ana_tags)\n",
    "m_df = metrics_to_df(reg_tr_metrics)\n",
    "m_df = m_df[np.isin(m_df[\"code\"], regular_tags + [__MICRO_F1__])][METRICS_COLS]\n",
    "m_df.sort_values(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>data_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MICRO_F1</td>\n",
       "      <td>0.47343</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.996921</td>\n",
       "      <td>35402.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code  f1_score  precision    recall  accuracy  data_points\n",
       "5  MICRO_F1   0.47343   0.505155  0.445455  0.996921      35402.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_test_metrics = ResultsProcessor.compute_mean_metrics_from_tagged_essays(ana_tagged_test, ana_tags)\n",
    "m_df = metrics_to_df(reg_test_metrics)\n",
    "m_df = m_df[np.isin(m_df[\"code\"], regular_tags + [__MICRO_F1__])][METRICS_COLS]\n",
    "m_df.sort_values(\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist Merged Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/SkinCancer/Thesis_Dataset/Predictions/CoRef/MergedTags/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_predictions_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{folder}merged_essays_train.dill\".format(folder=merged_predictions_folder), \"wb+\") as f:\n",
    "    dill.dump(merged_essays_tr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"{folder}merged_essays_test.dill\".format(folder=merged_predictions_folder), \"wb+\") as f:\n",
    "    dill.dump(merged_essays_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Metrics for All Tagged Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_params_from_name(fname):\n",
    "    prms = {}\n",
    "    fname = f.split(\"/\")[-1]\n",
    "    params = fname.replace(\".dill\",\"\")\n",
    "    current = []\n",
    "    for p in params.split(\"_\")[2:]: # skip essays_test\n",
    "        if \"-\" in p:\n",
    "            end, val = p.split(\"-\")\n",
    "            current.append(end)\n",
    "            prms[\" \".join(current)] = val\n",
    "            current = []\n",
    "        else:\n",
    "            current.append(p)\n",
    "    return prms\n",
    "\n",
    "cols = ['f1_score', 'accuracy', 'precision','recall', 'bi directional', 'data_points', \n",
    "          'hidden size', 'merge mode', 'num rnns', 'num_codes', 'use pretrained embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>bi directional</th>\n",
       "      <th>data_points</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>hidden size</th>\n",
       "      <th>merge mode</th>\n",
       "      <th>num rnns</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>use pretrained embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.997062</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>256</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997232</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>0.494845</td>\n",
       "      <td>128</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.996921</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>0.473430</td>\n",
       "      <td>256</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997204</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>0.464865</td>\n",
       "      <td>128</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.390909</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.996921</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>0.423280</td>\n",
       "      <td>64</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.997062</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>64</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy bi directional  data_points  f1_score hidden size merge mode  \\\n",
       "3  0.997062           True      35402.0  0.514019         256        sum   \n",
       "1  0.997232           True      35402.0  0.494845         128        sum   \n",
       "2  0.996921           True      35402.0  0.473430         256        sum   \n",
       "0  0.997204           True      35402.0  0.464865         128        sum   \n",
       "4  0.996921           True      35402.0  0.423280          64        sum   \n",
       "5  0.997062           True      35402.0  0.350000          64        sum   \n",
       "\n",
       "  num rnns  num_codes  precision    recall use pretrained embedding  \n",
       "3        2      110.0   0.528846  0.500000                     True  \n",
       "1        2      110.0   0.571429  0.436364                     True  \n",
       "2        1      110.0   0.505155  0.445455                     True  \n",
       "0        1      110.0   0.573333  0.390909                     True  \n",
       "4        1      110.0   0.506329  0.363636                     True  \n",
       "5        2      110.0   0.560000  0.254545                     True  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"f1_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>bi directional</th>\n",
       "      <th>data_points</th>\n",
       "      <th>hidden size</th>\n",
       "      <th>merge mode</th>\n",
       "      <th>num rnns</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>use pretrained embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.350575</td>\n",
       "      <td>0.996893</td>\n",
       "      <td>0.552036</td>\n",
       "      <td>0.256842</td>\n",
       "      <td>True</td>\n",
       "      <td>145471.0</td>\n",
       "      <td>256</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>475.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.338139</td>\n",
       "      <td>0.996529</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.271579</td>\n",
       "      <td>True</td>\n",
       "      <td>145471.0</td>\n",
       "      <td>256</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>475.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.319508</td>\n",
       "      <td>0.996955</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.218947</td>\n",
       "      <td>True</td>\n",
       "      <td>145471.0</td>\n",
       "      <td>128</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>475.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.996824</td>\n",
       "      <td>0.564356</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>True</td>\n",
       "      <td>145471.0</td>\n",
       "      <td>128</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>475.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.143357</td>\n",
       "      <td>0.996632</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.086316</td>\n",
       "      <td>True</td>\n",
       "      <td>145471.0</td>\n",
       "      <td>64</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>475.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.996735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>145471.0</td>\n",
       "      <td>64</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>475.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score  accuracy  precision    recall bi directional  data_points  \\\n",
       "2  0.350575  0.996893   0.552036  0.256842           True     145471.0   \n",
       "3  0.338139  0.996529   0.447917  0.271579           True     145471.0   \n",
       "1  0.319508  0.996955   0.590909  0.218947           True     145471.0   \n",
       "0  0.197917  0.996824   0.564356  0.120000           True     145471.0   \n",
       "5  0.143357  0.996632   0.422680  0.086316           True     145471.0   \n",
       "4  0.000000  0.996735   0.000000  0.000000           True     145471.0   \n",
       "\n",
       "  hidden size merge mode num rnns  num_codes use pretrained embedding  \n",
       "2         256        sum        1      475.0                     True  \n",
       "3         256        sum        2      475.0                     True  \n",
       "1         128        sum        2      475.0                     True  \n",
       "0         128        sum        1      475.0                     True  \n",
       "5          64        sum        2      475.0                     True  \n",
       "4          64        sum        1      475.0                     True  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"essays_train_bi_directional-True_hidden_size.*.dill\"\n",
    "files = find_files(anaphor_predictions_folder, pattern)\n",
    "\n",
    "rows = []\n",
    "for f in files:\n",
    "    params = get_params_from_name(f)\n",
    "    with open(f, \"rb+\") as fin:\n",
    "        essays = dill.load(fin)\n",
    "    metrics = ResultsProcessor.compute_mean_metrics_from_tagged_essays(essays, ana_tags)\n",
    "    params.update(metrics[\"MICRO_F1\"])\n",
    "    rows.append(params)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.sort_values(\"f1_score\", ascending=False)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>bi directional</th>\n",
       "      <th>data_points</th>\n",
       "      <th>hidden size</th>\n",
       "      <th>merge mode</th>\n",
       "      <th>num rnns</th>\n",
       "      <th>num_codes</th>\n",
       "      <th>use pretrained embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.997062</td>\n",
       "      <td>0.528846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>256</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.494845</td>\n",
       "      <td>0.997232</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.436364</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>128</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.473430</td>\n",
       "      <td>0.996921</td>\n",
       "      <td>0.505155</td>\n",
       "      <td>0.445455</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>256</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.464865</td>\n",
       "      <td>0.997204</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.390909</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>128</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423280</td>\n",
       "      <td>0.996921</td>\n",
       "      <td>0.506329</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>64</td>\n",
       "      <td>sum</td>\n",
       "      <td>1</td>\n",
       "      <td>110.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.997062</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>True</td>\n",
       "      <td>35402.0</td>\n",
       "      <td>64</td>\n",
       "      <td>sum</td>\n",
       "      <td>2</td>\n",
       "      <td>110.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1_score  accuracy  precision    recall bi directional  data_points  \\\n",
       "3  0.514019  0.997062   0.528846  0.500000           True      35402.0   \n",
       "1  0.494845  0.997232   0.571429  0.436364           True      35402.0   \n",
       "2  0.473430  0.996921   0.505155  0.445455           True      35402.0   \n",
       "0  0.464865  0.997204   0.573333  0.390909           True      35402.0   \n",
       "4  0.423280  0.996921   0.506329  0.363636           True      35402.0   \n",
       "5  0.350000  0.997062   0.560000  0.254545           True      35402.0   \n",
       "\n",
       "  hidden size merge mode num rnns  num_codes use pretrained embedding  \n",
       "3         256        sum        2      110.0                     True  \n",
       "1         128        sum        2      110.0                     True  \n",
       "2         256        sum        1      110.0                     True  \n",
       "0         128        sum        1      110.0                     True  \n",
       "4          64        sum        1      110.0                     True  \n",
       "5          64        sum        2      110.0                     True  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"essays_test_bi_directional-True_hidden_size.*.dill\"\n",
    "files = find_files(anaphor_predictions_folder, pattern)\n",
    "\n",
    "rows = []\n",
    "for f in files:\n",
    "    params = get_params_from_name(f)\n",
    "    with open(f, \"rb+\") as fin:\n",
    "        essays = dill.load(fin)\n",
    "    metrics = ResultsProcessor.compute_mean_metrics_from_tagged_essays(essays, ana_tags)\n",
    "    params.update(metrics[\"MICRO_F1\"])\n",
    "    rows.append(params)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.sort_values(\"f1_score\", ascending=False)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
