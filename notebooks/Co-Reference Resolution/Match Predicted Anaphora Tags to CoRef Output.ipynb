{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "from CoRefHelper import parse_stanfordnlp_tagged_essays\n",
    "from FindFiles import find_files\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "\"\"\" Begin Settings \"\"\"\n",
    "DATASET = \"CoralBleaching\"\n",
    "PARTITION = \"Test\" # Training | Test\n",
    "SCAN_LENGTH = 3\n",
    "\"\"\" END Settings \"\"\"\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "merged_predictions_folder = root_folder + \"Predictions/CoRef/MergedTags/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_folder = coref_root + PARTITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Merged Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 training essays loaded from:\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/CoRef/MergedTags/merged_essays_test.dill\n"
     ]
    }
   ],
   "source": [
    "##override this so we don't replace INFREQUENT words\n",
    "#config[\"min_df\"] = 0\n",
    "\n",
    "if PARTITION.lower() == \"training\":\n",
    "    merged_essays_fname =  \"merged_essays_train.dill\"\n",
    "elif PARTITION.lower() == \"test\":\n",
    "    merged_essays_fname = \"merged_essays_test.dill\"\n",
    "else:\n",
    "    raise Exception(\"Invalid partition: \" + PARTITION)\n",
    "\n",
    "merged_essays_fname = merged_predictions_folder + merged_essays_fname\n",
    "with open(merged_essays_fname, \"rb+\") as f:\n",
    "    tagged_essays = dill.load(f)\n",
    "\n",
    "# map parsed essays to essay name\n",
    "essay2tagged = {}\n",
    "for e in tagged_essays:\n",
    "    essay2tagged[e.name.split(\".\")[0]] = e\n",
    "\n",
    "print(\"{0} training essays loaded from:\\n{1}\".format(len(tagged_essays), merged_essays_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CoRef Parsed Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 co-ref tagged files loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load CoRef Parsed Essays\n",
    "coref_files = find_files(coref_folder, \".*\\.tagged\")\n",
    "print(\"{0} co-ref tagged files loaded\".format(len(coref_files)))\n",
    "assert len(coref_files) == len(tagged_essays)\n",
    "\n",
    "essay2coref_tagged = parse_stanfordnlp_tagged_essays(coref_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Same Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATE THE SAME SET OF ESSAYS\n",
    "assert essay2tagged.keys() == essay2coref_tagged.keys()\n",
    "intersect = set(essay2tagged.keys()).intersection(essay2coref_tagged.keys())\n",
    "assert len(intersect) == len(essay2tagged.keys())\n",
    "assert len(essay2tagged.keys()) > 1\n",
    "assert len(essay2tagged.keys()) == len(essay2coref_tagged.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on CoRef Datastructure\n",
    "- Dictionary of esssays, keyed by name\n",
    "- Each essay is a list of sentences\n",
    "- Each sentence is a list of words\n",
    "- words are mapped to a tag dict: Dict[str, Set[str]]]\n",
    "  - tag dict - contains\n",
    "    - NER tag (most are O - none)\n",
    "    - POS tag\n",
    "    - If a Co-Reference such as an anaphor (mostly pronouns)\n",
    "      - COREF_PHRASE - phrase referred to by coref\n",
    "      - COREF_REF - Id of referenced phrase\n",
    "    - else if it is a phrase that is referenced:\n",
    "      - COREF_ID - id of the co-reference, referenced in the COREF_REF tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "COREF_PHRASE = \"COREF_PHRASE\"\n",
    "COREF_ID     = \"COREF_ID\"\n",
    "COREF_REF    = \"COREF_REF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EBA1415_AEKD_4_CB_ES-05574'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_coref_essay_sentence(essay2coref_tagged):\n",
    "    for ename, list_sent in list(essay2coref_tagged.items()):\n",
    "        for ix, sent in enumerate(list_sent):\n",
    "            found_id = False\n",
    "            found_ref = False\n",
    "            for wd, tag_dict in sent:\n",
    "                if COREF_ID in tag_dict:\n",
    "                    found_id = True\n",
    "                if COREF_REF in tag_dict:\n",
    "                    found_ref = True\n",
    "            if found_id and found_ref:\n",
    "                return ename\n",
    "    return None\n",
    "\n",
    "ename = find_coref_essay_sentence(essay2coref_tagged)\n",
    "ename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBA1415_AEKD_4_CB_ES-05574\n",
      "\n",
      "well                 \n",
      "based                \n",
      "on                   \n",
      "what                 \n",
      "i                    \n",
      "read                 \n",
      "the                  {'COREF_ID': {'3'}}\n",
      "corals               {'COREF_ID': {'3'}}\n",
      "are                  \n",
      "loosing              \n",
      "their                {'COREF_REF': {'3'}, 'COREF_PHRASE': {'the_corals'}}\n",
      "colors               \n",
      ",                    \n",
      "coral                {'COREF_REF': {'4'}, 'COREF_PHRASE': {'the_coral_bleaching'}}\n",
      "bleaching            {'COREF_REF': {'4'}, 'COREF_PHRASE': {'the_coral_bleaching'}}\n",
      "are                  \n",
      "a                    \n",
      "serious              \n",
      "problem              \n",
      "with                 \n",
      "a                    \n",
      "serious              \n",
      "impact               \n",
      "on                   \n",
      "the                  {'COREF_ID': {'1'}}\n",
      "worlds               {'COREF_ID': {'1'}}\n",
      "coral                {'COREF_ID': {'1'}}\n",
      "reefs                {'COREF_ID': {'1'}}\n",
      ".                    \n",
      "********************************************************************************\n",
      "this                 {'COREF_REF': {'3'}, 'COREF_PHRASE': {'the_corals'}}\n",
      "is                   \n",
      "a                    \n",
      "serious              \n",
      "problem              \n",
      "because              \n",
      "the                  {'COREF_ID': {'4'}}\n",
      "coral                {'COREF_ID': {'4'}}\n",
      "bleaching            {'COREF_ID': {'4'}}\n",
      "is                   \n",
      "most                 \n",
      "noticeable           \n",
      "in                   \n",
      "the                  {'COREF_ID': {'5'}}\n",
      "pacific              {'COREF_ID': {'5'}}\n",
      "ocean                {'COREF_ID': {'5'}}\n",
      ",                    \n",
      "the                  {'COREF_REF': {'5'}, 'COREF_PHRASE': {'the_pacific_ocean'}}\n",
      "ocean                {'COREF_REF': {'5'}, 'COREF_PHRASE': {'the_pacific_ocean'}}\n",
      "covers               \n",
      "about                \n",
      "0                    \n",
      "0                    \n",
      "of                   \n",
      "the                  \n",
      "entire               \n",
      "globe                \n",
      ",                    \n",
      "some                 \n",
      "corals               \n",
      "are                  \n",
      "sensitive            \n",
      "to                   \n",
      "how                  \n",
      "salty                \n",
      "the                  \n",
      "water                \n",
      "us                   \n",
      ".                    \n",
      "********************************************************************************\n",
      "a                    {'COREF_ID': {'2'}}\n",
      "massive              {'COREF_ID': {'2'}}\n",
      "coral                {'COREF_ID': {'2'}}\n",
      "bleaching            {'COREF_ID': {'2'}}\n",
      "event                {'COREF_ID': {'2'}}\n",
      "in                   {'COREF_ID': {'2'}}\n",
      "0000                 {'COREF_ID': {'2'}}\n",
      "is                   \n",
      "one                  \n",
      "of                   \n",
      "the                  \n",
      "worst                \n",
      "ever                 \n",
      "obscure              \n",
      ",                    \n",
      "the                  {'COREF_REF': {'2'}, 'COREF_PHRASE': {'a_massive_coral_bleaching_event_in_0000'}}\n",
      "event                {'COREF_REF': {'2'}, 'COREF_PHRASE': {'a_massive_coral_bleaching_event_in_0000'}}\n",
      "resulted             \n",
      "in                   \n",
      "the                  \n",
      "death                \n",
      "of                   \n",
      "00                   \n",
      "%                    \n",
      "of                   \n",
      "the                  \n",
      "worlds               \n",
      "coral                \n",
      "reefs                \n",
      ",                    \n",
      "thus                 \n",
      "very                 \n",
      "serious              \n",
      "it                   {'COREF_REF': {'2'}, 'COREF_PHRASE': {'a_massive_coral_bleaching_event_in_0000'}}\n",
      "killed               \n",
      "a                    \n",
      "lot                  \n",
      "of                   \n",
      "coral                \n",
      "reefs                \n",
      ".                    \n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(ename)\n",
    "print()\n",
    "for sent in essay2coref_tagged[ename]: #[:matching_ix+1]:\n",
    "    for wd, tag_dict in sent:\n",
    "        copy = dict([(k,v) for k,v in tag_dict.items() if k in {COREF_ID, COREF_REF, COREF_PHRASE}])\n",
    "        print(wd.ljust(20), copy if copy else \"\")\n",
    "    print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match CoRef Tagged to Consolidated Tagged Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tagged_words_to_word_ixs(tagged_essay):\n",
    "\n",
    "    wd2tags = []\n",
    "    taggedwd2sentixs = {}\n",
    "    for sent_ix, sent in enumerate(tagged_essay.sentences):\n",
    "        for wd_ix, (wd, tags) in enumerate(sent):\n",
    "            taggedwd2sentixs[len(wd2tags)] = (sent_ix, wd_ix)\n",
    "            if wd == \"\\'\\'\":\n",
    "                wd = \"\\\"\"\n",
    "            wd2tags.append((wd, tags))\n",
    "    return wd2tags, taggedwd2sentixs\n",
    "\n",
    "\n",
    "def replace_underscore(mention):\n",
    "    return set(map(lambda s: s.replace(\"_\",\" \"), mention))\n",
    "\n",
    "def map_mentions_to_word_ixs(coref_essay):\n",
    "    #TODO - fix this, it assume one mention per word, but we can have multiple\n",
    "    wds2coref = []\n",
    "    mentions = []\n",
    "    for sent_ix, sent in enumerate(coref_essay):\n",
    "        current_mentions = set()\n",
    "        mention_ixs = set()\n",
    "        for wd_ix, (wd, tag_dict) in enumerate(sent):\n",
    "            wds2coref.append((wd, tag_dict))\n",
    "            if COREF_PHRASE not in tag_dict:\n",
    "                if len(current_mentions) > 0:\n",
    "                    mentions.append((current_mentions, mention_ixs))\n",
    "                current_mentions = set()\n",
    "                mention_ixs = set()\n",
    "            else:\n",
    "                phrases = replace_underscore(tag_dict[COREF_PHRASE])\n",
    "                if phrases != current_mentions and len(current_mentions) > 0:\n",
    "                    mentions.append((current_mentions, mention_ixs))\n",
    "                    current_mentions = set()\n",
    "                    mention_ixs = set()\n",
    "                current_mentions = phrases\n",
    "                mention_ixs.add(len(wds2coref) - 1)\n",
    "        if len(current_mentions) > 0:\n",
    "            mentions.append((current_mentions, mention_ixs))\n",
    "    return wds2coref, mentions\n",
    "\n",
    "def map_words_between_essays(wd2_tags, wds2coref):\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    ix_tagd, ix_coref = 0, 0\n",
    "    ixtagd_2_ixcoref = {}\n",
    "    ixcoref_2_ixtagd = {}\n",
    "    \n",
    "    while ix_tagd < (len(wd2tags) - 1) and ix_coref < (len(wds2coref) - 1):\n",
    "        wd_tagd, atags = wd2tags[ix_tagd]\n",
    "        wd_coref, btag_dict = wds2coref[ix_coref]\n",
    "\n",
    "        if wd_tagd == wd_coref or wd_tagd == \"cannot\" and wd_coref == \"can\":\n",
    "            ixtagd_2_ixcoref[ix_tagd]  = ix_coref\n",
    "            ixcoref_2_ixtagd[ix_coref] = ix_tagd\n",
    "            ix_tagd  += 1\n",
    "            ix_coref += 1\n",
    "        else:\n",
    "            # look ahead in wds2 for item that matches next a\n",
    "            found_match = False\n",
    "            for offseta, (aa, atags) in enumerate(wd2tags[ix_tagd: ix_tagd + 1 + SCAN_LENGTH]):\n",
    "                for offsetb, (bb, bb_tag_dict) in enumerate(wds2coref[ix_coref:ix_coref + 1 + SCAN_LENGTH]):\n",
    "                    if aa == bb:\n",
    "                        if offseta == offsetb:\n",
    "                            for i in range(ix_tagd, ix_tagd + offseta):\n",
    "                                if i not in ixtagd_2_ixcoref:\n",
    "                                    ixtagd_2_ixcoref[i] = i\n",
    "\n",
    "                        ix_tagd  = ix_tagd + offseta\n",
    "                        ix_coref = ix_coref + offsetb\n",
    "                        ixtagd_2_ixcoref[ix_tagd] = ix_coref\n",
    "                        ixcoref_2_ixtagd[ix_coref] = ix_tagd\n",
    "                        found_match = True\n",
    "                        break\n",
    "                if found_match:\n",
    "                    break\n",
    "            if not found_match:\n",
    "                errors.append((ename, wd_tagd, wd_coref, ix_tagd, ix_coref))\n",
    "                break\n",
    "    return ixtagd_2_ixcoref, ixcoref_2_ixtagd, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_essay  = essay2coref_tagged[ename]\n",
    "tagged_essay = essay2tagged[ename]\n",
    "\n",
    "wd2tags, taggedwd2sentixs = map_tagged_words_to_word_ixs(tagged_essay)\n",
    "wds2coref, mentions = map_mentions_to_word_ixs(coref_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtagd_2_ixcoref, ixcoref_2_ixtagd, errors = map_words_between_essays(wd2tags, wds2coref)\n",
    "if errors:\n",
    "    # Print errors\n",
    "    for ename, wd_tagd, wd_coref, ix_tagd, ix_coref in errors:\n",
    "        failed_cnt += 1\n",
    "        print(\"Failed: \" + ename, wd_tagd, wd_coref, ix_tagd, ix_coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ixtagd_2_ixcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ixtagd_2_ixcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 109)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_wd2_tags), len(taggedwd2sentixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wds_coref, mentions = map_mentions_to_word_ixs(coref_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109, 6)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wds_coref), len(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'the corals'}, {10}),\n",
       " ({'the coral bleaching'}, {13, 14}),\n",
       " ({'the corals'}, {29}),\n",
       " ({'the pacific ocean'}, {46, 47}),\n",
       " ({'a massive coral bleaching event in 0000'}, {83, 84}),\n",
       " ({'a massive coral bleaching event in 0000'}, {101})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
