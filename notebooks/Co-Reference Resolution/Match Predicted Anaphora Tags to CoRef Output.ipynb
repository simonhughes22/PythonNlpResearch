{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "from CoRefHelper import parse_stanfordnlp_tagged_essays\n",
    "from FindFiles import find_files\n",
    "from Settings import Settings\n",
    "from collections import defaultdict\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "\"\"\" Begin Settings \"\"\"\n",
    "DATASET = \"CoralBleaching\"\n",
    "PARTITION = \"Test\" # Training | Test\n",
    "SCAN_LENGTH = 3\n",
    "\"\"\" END Settings \"\"\"\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + DATASET + \"/Thesis_Dataset/\"\n",
    "merged_predictions_folder = root_folder + \"Predictions/CoRef/MergedTags/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_folder = coref_root + PARTITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Merged Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 training essays loaded from:\n",
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Predictions/CoRef/MergedTags/merged_essays_test.dill\n"
     ]
    }
   ],
   "source": [
    "##override this so we don't replace INFREQUENT words\n",
    "#config[\"min_df\"] = 0\n",
    "\n",
    "if PARTITION.lower() == \"training\":\n",
    "    merged_essays_fname =  \"merged_essays_train.dill\"\n",
    "elif PARTITION.lower() == \"test\":\n",
    "    merged_essays_fname = \"merged_essays_test.dill\"\n",
    "else:\n",
    "    raise Exception(\"Invalid partition: \" + PARTITION)\n",
    "\n",
    "merged_essays_fname = merged_predictions_folder + merged_essays_fname\n",
    "with open(merged_essays_fname, \"rb+\") as f:\n",
    "    tagged_essays = dill.load(f)\n",
    "\n",
    "# map parsed essays to essay name\n",
    "essay2tagged = {}\n",
    "for e in tagged_essays:\n",
    "    essay2tagged[e.name.split(\".\")[0]] = e\n",
    "\n",
    "print(\"{0} training essays loaded from:\\n{1}\".format(len(tagged_essays), merged_essays_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CoRef Parsed Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 co-ref tagged files loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load CoRef Parsed Essays\n",
    "coref_files = find_files(coref_folder, \".*\\.tagged\")\n",
    "print(\"{0} co-ref tagged files loaded\".format(len(coref_files)))\n",
    "assert len(coref_files) == len(tagged_essays)\n",
    "\n",
    "essay2coref_tagged = parse_stanfordnlp_tagged_essays(coref_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Same Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATE THE SAME SET OF ESSAYS\n",
    "assert essay2tagged.keys() == essay2coref_tagged.keys()\n",
    "intersect = set(essay2tagged.keys()).intersection(essay2coref_tagged.keys())\n",
    "assert len(intersect) == len(essay2tagged.keys())\n",
    "assert len(essay2tagged.keys()) > 1\n",
    "assert len(essay2tagged.keys()) == len(essay2coref_tagged.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on CoRef Datastructure\n",
    "- Dictionary of esssays, keyed by name\n",
    "- Each essay is a list of sentences\n",
    "- Each sentence is a list of words\n",
    "- words are mapped to a tag dict: Dict[str, Set[str]]]\n",
    "  - tag dict - contains\n",
    "    - NER tag (most are O - none)\n",
    "    - POS tag\n",
    "    - If a Co-Reference such as an anaphor (mostly pronouns)\n",
    "      - COREF_PHRASE - phrase referred to by coref\n",
    "      - COREF_REF - Id of referenced phrase\n",
    "    - else if it is a phrase that is referenced:\n",
    "      - COREF_ID - id of the co-reference, referenced in the COREF_REF tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "COREF_PHRASE = \"COREF_PHRASE\"\n",
    "COREF_ID     = \"COREF_ID\"\n",
    "COREF_REF    = \"COREF_REF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EBA1415_AEKD_4_CB_ES-05574'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_coref_essay_sentence(essay2coref_tagged):\n",
    "    for ename, list_sent in list(essay2coref_tagged.items()):\n",
    "        for ix, sent in enumerate(list_sent):\n",
    "            found_id = False\n",
    "            found_ref = False\n",
    "            for wd, tag_dict in sent:\n",
    "                if COREF_ID in tag_dict:\n",
    "                    found_id = True\n",
    "                if COREF_REF in tag_dict:\n",
    "                    found_ref = True\n",
    "            if found_id and found_ref:\n",
    "                return ename\n",
    "    return None\n",
    "\n",
    "ename = find_coref_essay_sentence(essay2coref_tagged)\n",
    "ename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBA1415_AEKD_4_CB_ES-05574\n",
      "\n",
      "0   well                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "1   based                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "2   on                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "3   what                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "4   i                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "5   read                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "6   the                  {'COREF_ID': {'3'}, 'COREF_REF': set()}\n",
      "7   corals               {'COREF_ID': {'3'}, 'COREF_REF': set()}\n",
      "8   are                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "9   loosing              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "10  their                {'COREF_REF': {'3'}, 'COREF_PHRASE': {'the_corals'}, 'COREF_ID': set()}\n",
      "11  colors               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "12  ,                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "13  coral                {'COREF_REF': {'4'}, 'COREF_PHRASE': {'the_coral_bleaching'}, 'COREF_ID': set()}\n",
      "14  bleaching            {'COREF_REF': {'4'}, 'COREF_PHRASE': {'the_coral_bleaching'}, 'COREF_ID': set()}\n",
      "15  are                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "16  a                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "17  serious              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "18  problem              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "19  with                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "20  a                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "21  serious              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "22  impact               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "23  on                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "24  the                  {'COREF_ID': {'1'}, 'COREF_REF': set()}\n",
      "25  worlds               {'COREF_ID': {'1'}, 'COREF_REF': set()}\n",
      "26  coral                {'COREF_ID': {'1'}, 'COREF_REF': set()}\n",
      "27  reefs                {'COREF_ID': {'1'}, 'COREF_REF': set()}\n",
      "28  .                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "********************************************************************************\n",
      "29  this                 {'COREF_REF': {'3'}, 'COREF_PHRASE': {'the_corals'}, 'COREF_ID': set()}\n",
      "30  is                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "31  a                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "32  serious              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "33  problem              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "34  because              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "35  the                  {'COREF_ID': {'4'}, 'COREF_REF': set()}\n",
      "36  coral                {'COREF_ID': {'4'}, 'COREF_REF': set()}\n",
      "37  bleaching            {'COREF_ID': {'4'}, 'COREF_REF': set()}\n",
      "38  is                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "39  most                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "40  noticeable           {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "41  in                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "42  the                  {'COREF_ID': {'5'}, 'COREF_REF': set()}\n",
      "43  pacific              {'COREF_ID': {'5'}, 'COREF_REF': set()}\n",
      "44  ocean                {'COREF_ID': {'5'}, 'COREF_REF': set()}\n",
      "45  ,                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "46  the                  {'COREF_REF': {'5'}, 'COREF_PHRASE': {'the_pacific_ocean'}, 'COREF_ID': set()}\n",
      "47  ocean                {'COREF_REF': {'5'}, 'COREF_PHRASE': {'the_pacific_ocean'}, 'COREF_ID': set()}\n",
      "48  covers               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "49  about                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "50  0                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "51  0                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "52  of                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "53  the                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "54  entire               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "55  globe                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "56  ,                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "57  some                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "58  corals               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "59  are                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "60  sensitive            {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "61  to                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "62  how                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "63  salty                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "64  the                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "65  water                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "66  us                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "67  .                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "********************************************************************************\n",
      "68  a                    {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "69  massive              {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "70  coral                {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "71  bleaching            {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "72  event                {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "73  in                   {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "74  0000                 {'COREF_ID': {'2'}, 'COREF_REF': set()}\n",
      "75  is                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "76  one                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "77  of                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "78  the                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "79  worst                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "80  ever                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "81  obscure              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "82  ,                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "83  the                  {'COREF_REF': {'2'}, 'COREF_PHRASE': {'a_massive_coral_bleaching_event_in_0000'}, 'COREF_ID': set()}\n",
      "84  event                {'COREF_REF': {'2'}, 'COREF_PHRASE': {'a_massive_coral_bleaching_event_in_0000'}, 'COREF_ID': set()}\n",
      "85  resulted             {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "86  in                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "87  the                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "88  death                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "89  of                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "90  00                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "91  %                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "92  of                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "93  the                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "94  worlds               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "95  coral                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "96  reefs                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "97  ,                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "98  thus                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "99  very                 {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "100 serious              {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "101 it                   {'COREF_REF': {'2'}, 'COREF_PHRASE': {'a_massive_coral_bleaching_event_in_0000'}, 'COREF_ID': set()}\n",
      "102 killed               {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "103 a                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "104 lot                  {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "105 of                   {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "106 coral                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "107 reefs                {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "108 .                    {'COREF_REF': set(), 'COREF_ID': set()}\n",
      "********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(ename)\n",
    "print()\n",
    "wd_ix = -1\n",
    "for sent in essay2coref_tagged[ename]: #[:matching_ix+1]:\n",
    "    for wd, tag_dict in sent:\n",
    "        wd_ix+=1\n",
    "        copy = dict([(k,v) for k,v in tag_dict.items() if k in {COREF_ID, COREF_REF, COREF_PHRASE}])\n",
    "        print(str(wd_ix).ljust(3), wd.ljust(20), copy if copy else \"\")\n",
    "    print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match CoRef Tagged to Consolidated Tagged Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tagged_words_to_word_ixs(tagged_essay):\n",
    "\n",
    "    tagged_wds = []\n",
    "    taggedwd2sentixs = {}\n",
    "    for sent_ix, sent in enumerate(tagged_essay.sentences):\n",
    "        for wd_ix, (wd, tags) in enumerate(sent):\n",
    "            taggedwd2sentixs[len(tagged_wds)] = (sent_ix, wd_ix)\n",
    "            if wd == \"\\'\\'\":\n",
    "                wd = \"\\\"\"\n",
    "            tagged_wds.append(wd)\n",
    "    return tagged_wds, taggedwd2sentixs\n",
    "\n",
    "def replace_underscore(mention):\n",
    "    return set(map(lambda s: s.replace(\"_\",\" \"), mention))\n",
    "\n",
    "def map_mentions_to_word_ixs(coref_essay, keys):\n",
    "    #TODO - fix this, it assume one mention per word, but we can have multiple\n",
    "    coref_wd2_tags = []\n",
    "    coref_ids_2_wd_ixs = defaultdict(list) # maps a coref id to a list of set of ixs\n",
    "    for sent_ix, sent in enumerate(coref_essay):\n",
    "        current_coref_ids = set()\n",
    "        for wd_ix, (wd, tag_dict) in enumerate(sent):\n",
    "            coref_wd2_tags.append((wd,tag_dict))\n",
    "\n",
    "            wd_coref_ids = set()\n",
    "            for k in keys:\n",
    "                wd_coref_ids.update(tag_dict[k])\n",
    "\n",
    "            for cref_id in wd_coref_ids:\n",
    "                prev_ixs = coref_ids_2_wd_ixs[cref_id]\n",
    "                # continuation of existing sequence\n",
    "                wd_essay_ix = len(coref_wd2_tags)-1\n",
    "                if cref_id in current_coref_ids:\n",
    "                    if len(prev_ixs) == 0:\n",
    "                        prev_ixs.append(set())\n",
    "                    prev_ixs[-1].add(wd_essay_ix)\n",
    "                else:\n",
    "                    # else create a new set and add it\n",
    "                    prev_ixs.append({wd_essay_ix})\n",
    "\n",
    "            current_coref_ids = wd_coref_ids\n",
    "    return coref_wd2_tags, coref_ids_2_wd_ixs\n",
    "\n",
    "def map_words_between_essays(tagged_wds, coref_wd2_tags):\n",
    "    errors = []\n",
    "\n",
    "    ix_tagd, ix_coref = 0, 0\n",
    "    ixtagd_2_ixcoref = {}\n",
    "    ixcoref_2_ixtagd = {}\n",
    "\n",
    "    while ix_tagd < (len(tagged_wds) - 1) and ix_coref < (len(coref_wd2_tags) - 1):\n",
    "        wd_tagd = tagged_wds[ix_tagd]\n",
    "        wd_coref, btag_dict = coref_wd2_tags[ix_coref]\n",
    "\n",
    "        if wd_tagd == wd_coref or wd_tagd == \"cannot\" and wd_coref == \"can\":\n",
    "            ixtagd_2_ixcoref[ix_tagd] = ix_coref\n",
    "            ixcoref_2_ixtagd[ix_coref] = ix_tagd\n",
    "            ix_tagd += 1\n",
    "            ix_coref += 1\n",
    "        else:\n",
    "            # look ahead in wds2 for item that matches next a\n",
    "            found_match = False\n",
    "            for offseta, aa in enumerate(tagged_wds[ix_tagd: ix_tagd + 1 + SCAN_LENGTH]):\n",
    "                for offsetb, (bb, _) in enumerate(coref_wd2_tags[ix_coref:ix_coref + 1 + SCAN_LENGTH]):\n",
    "                    if aa == bb:\n",
    "                        if offseta == offsetb:\n",
    "                            for i in range(ix_tagd, ix_tagd + offseta):\n",
    "                                if i not in ixtagd_2_ixcoref:\n",
    "                                    ixtagd_2_ixcoref[i] = i\n",
    "\n",
    "                        ix_tagd = ix_tagd + offseta\n",
    "                        ix_coref = ix_coref + offsetb\n",
    "                        ixtagd_2_ixcoref[ix_tagd] = ix_coref\n",
    "                        ixcoref_2_ixtagd[ix_coref] = ix_tagd\n",
    "                        found_match = True\n",
    "                        break\n",
    "                if found_match:\n",
    "                    break\n",
    "            if not found_match:\n",
    "                errors.append((ename, wd_tagd, wd_coref, ix_tagd, ix_coref))\n",
    "                break\n",
    "    return ixtagd_2_ixcoref, ixcoref_2_ixtagd, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_essays = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_essay  = essay2coref_tagged[ename]\n",
    "tagged_essay = essay2tagged[ename]\n",
    "\n",
    "tagged_wds, taggedwd2sentixs = map_tagged_words_to_word_ixs(tagged_essay)\n",
    "coref_wd2_tags, coref_ids_2_wd_ixs = map_mentions_to_word_ixs(coref_essay, keys={COREF_ID, COREF_REF})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtagd_2_ixcoref, ixcoref_2_ixtagd, errors = map_words_between_essays(tagged_wds, coref_wd2_tags)\n",
    "if errors:\n",
    "    # Print errors\n",
    "    for ename, wd_tagd, wd_coref, ix_tagd, ix_coref in errors:\n",
    "        failed_cnt += 1\n",
    "        print(\"Failed: \" + ename, wd_tagd, wd_coref, ix_tagd, ix_coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_wdix_2_corefids = defaultdict(set)\n",
    "for coref_id, list_wd_ix_seq in coref_ids_2_wd_ixs.items():\n",
    "    for wd_ixs in list_wd_ix_seq:\n",
    "\n",
    "        first_ix = min(wd_ixs)\n",
    "        is_fuzzy = False\n",
    "        if first_ix not in ixcoref_2_ixtagd:\n",
    "            while first_ix > 0 and first_ix not in ixcoref_2_ixtagd:\n",
    "                first_ix -= 1\n",
    "            if first_ix not in ixcoref_2_ixtagd:\n",
    "                e_first_wd_ix = 0\n",
    "            # one past last matching index\n",
    "            else:\n",
    "                e_first_wd_ix = min(len(tagged_wds) - 1, ixcoref_2_ixtagd[first_ix] + 1)\n",
    "            is_fuzzy = True\n",
    "        else:\n",
    "            e_first_wd_ix = ixcoref_2_ixtagd[first_ix]\n",
    "\n",
    "        last_ix = max(wd_ixs)\n",
    "        if last_ix not in ixcoref_2_ixtagd:\n",
    "            while last_ix < len(coref_wd2_tags) and last_ix not in ixcoref_2_ixtagd:\n",
    "                last_ix += 1\n",
    "            if last_ix not in ixcoref_2_ixtagd:\n",
    "                e_last_wd_ix = len(tagged_wds) - 1\n",
    "            else:\n",
    "                e_last_wd_ix = max(0, ixcoref_2_ixtagd[last_ix] - 1)\n",
    "            is_fuzzy = True\n",
    "        else:\n",
    "            e_last_wd_ix = ixcoref_2_ixtagd[last_ix]\n",
    "\n",
    "        for e_wd_ix in range(e_first_wd_ix, e_last_wd_ix + 1):\n",
    "            sent_ix, sent_wd_ix = taggedwd2sentixs[e_wd_ix]\n",
    "            sentence = tagged_essay.sentences[sent_ix]\n",
    "            wd, tags = sentence[sent_wd_ix]\n",
    "\n",
    "            sent_wdix_2_corefids[(sent_ix, sent_wd_ix)].add(coref_id)\n",
    "\n",
    "predicted_corefids_sentences = []\n",
    "for sent_ix, sent in enumerate(tagged_essay.sentences):\n",
    "    changed_ix = -1\n",
    "\n",
    "    predicted_coref_ids_wds = []\n",
    "    predicted_corefids_sentences.append(predicted_coref_ids_wds)\n",
    "\n",
    "    for wd_ix in range(len((sent))):\n",
    "        wd_coref_ids = sent_wdix_2_corefids[(sent_ix, wd_ix)]\n",
    "        predicted_coref_ids_wds.append(wd_coref_ids)\n",
    "\n",
    "tagged_essay.predicted_corefids = predicted_corefids_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': [{24, 25, 26, 27}],\n",
      " '2': [{68, 69, 70, 71, 72, 73, 74}, {83, 84}, {101}],\n",
      " '3': [{6, 7}, {10}, {29}],\n",
      " '4': [{13, 14}, {35, 36, 37}],\n",
      " '5': [{42, 43, 44}, {46, 47}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dict(coref_ids_2_wd_ixs.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_essays.append(tagged_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well                 set()\n",
      "based                set()\n",
      "on                   set()\n",
      "what                 set()\n",
      "i                    set()\n",
      "read                 set()\n",
      "the                  {'3'}\n",
      "corals               {'3'}\n",
      "are                  set()\n",
      "INFREQUENT           set()\n",
      "their                {'3'}\n",
      "colors               set()\n",
      ",                    set()\n",
      "coral                {'4'}\n",
      "bleaching            {'4'}\n",
      "are                  set()\n",
      "a                    set()\n",
      "serious              set()\n",
      "problem              set()\n",
      "with                 set()\n",
      "a                    set()\n",
      "serious              set()\n",
      "impact               set()\n",
      "on                   set()\n",
      "the                  {'1'}\n",
      "worlds               {'1'}\n",
      "coral                {'1'}\n",
      "reefs                {'1'}\n",
      ".                    set()\n",
      "this                 {'3'}\n",
      "is                   set()\n",
      "a                    set()\n",
      "serious              set()\n",
      "problem              set()\n",
      "because              set()\n",
      "the                  {'4'}\n",
      "coral                {'4'}\n",
      "bleaching            {'4'}\n",
      "is                   set()\n",
      "most                 set()\n",
      "noticeable           set()\n",
      "in                   set()\n",
      "the                  {'5'}\n",
      "pacific              {'5'}\n",
      "ocean                {'5'}\n",
      ",                    set()\n",
      "the                  {'5'}\n",
      "ocean                {'5'}\n",
      "covers               set()\n",
      "about                set()\n",
      "0                    set()\n",
      "0                    set()\n",
      "of                   set()\n",
      "the                  set()\n",
      "entire               set()\n",
      "globe                set()\n",
      ",                    set()\n",
      "some                 set()\n",
      "corals               set()\n",
      "are                  set()\n",
      "sensitive            set()\n",
      "to                   set()\n",
      "how                  set()\n",
      "salty                set()\n",
      "the                  set()\n",
      "water                set()\n",
      "us                   set()\n",
      ".                    set()\n",
      "a                    {'2'}\n",
      "massive              {'2'}\n",
      "coral                {'2'}\n",
      "bleaching            {'2'}\n",
      "event                {'2'}\n",
      "in                   {'2'}\n",
      "0000                 {'2'}\n",
      "is                   set()\n",
      "one                  set()\n",
      "of                   set()\n",
      "the                  set()\n",
      "worst                set()\n",
      "ever                 set()\n",
      "INFREQUENT           set()\n",
      ",                    set()\n",
      "the                  {'2'}\n",
      "event                {'2'}\n",
      "resulted             set()\n",
      "in                   set()\n",
      "the                  set()\n",
      "death                set()\n",
      "of                   set()\n",
      "00                   set()\n",
      "%                    set()\n",
      "of                   set()\n",
      "the                  set()\n",
      "worlds               set()\n",
      "coral                set()\n",
      "reefs                set()\n",
      ",                    set()\n",
      "thus                 set()\n",
      "very                 set()\n",
      "serious              set()\n",
      "it                   {'2'}\n",
      "killed               set()\n",
      "a                    set()\n",
      "lot                  set()\n",
      "of                   set()\n",
      "coral                set()\n",
      "reefs                set()\n",
      ".                    set()\n"
     ]
    }
   ],
   "source": [
    "for sentix, sent in enumerate(tagged_essay.sentences):\n",
    "    \n",
    "    corefid_sent = tagged_essay.predicted_corefids[sentix]\n",
    "    for wd_ix in range(len(sent)):\n",
    "        wd = sent[wd_ix][0]\n",
    "        \n",
    "        cref_ids = corefid_sent[wd_ix]\n",
    "        print(wd.ljust(20), cref_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
