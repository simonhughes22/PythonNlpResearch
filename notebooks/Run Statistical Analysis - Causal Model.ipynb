{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE - Use Python 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import pymongo\n",
    "import dill\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from window_based_tagger_config import get_config\n",
    "from FindFiles import find_files\n",
    "from DirUtils import dir_exists\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Settings import Settings\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = mcnemar([[1,2],[3,1]], exact=True)\n",
    "# result.statistic, result.pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/Predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_str(filters, files, exclude=False):\n",
    "    if type(filters) != list:\n",
    "        filters = [filters]\n",
    "    flt = files\n",
    "    for fltr in filters:\n",
    "        files = [f for f in files if (fltr in f) != exclude]\n",
    "    return  files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1202\n",
      "163\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(FOLDER)\n",
    "print(len(all_files))\n",
    "files = list(all_files)\n",
    "# files = filter_by_str(\"_TAGGING_\", files, exclude=True)\n",
    "files = filter_by_str(\"_FINAL_RUN_\", files, exclude=True) # this is the sentence CREL parser (sentence level)\n",
    "files = filter_by_str(\"_TAGGING_\", files, exclude=True) # this is the sentence CREL parser (sentence level)\n",
    "files = filter_by_str(\"_STACKED_\", files, exclude=True) # this is the sentence CREL parser (sentence level)\n",
    "files = filter_by_str(\"_PA_\", files, exclude=True) # this is the sentence CREL parser (sentence level)\n",
    "# files = filter_by_str(\"_CR_\", files) \n",
    "files = filter_by_str(\"TEST_\", files)\n",
    "files = filter_by_str(\"_VD_\", files)\n",
    "print(len(files))\n",
    "files = filter_by_str(\"2019\", files, exclude=True)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENT_TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FIXED_VD_PREDS__.dill',\n",
       " 'SENT_TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FIXED_VD_YS_.dill',\n",
       " 'SENT_TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM_VD_2_PREDS_.dill',\n",
       " 'SENT_TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM_VD_2_YS_.dill',\n",
       " 'TEST_CB_STR_PCPTRN_RE-RANKER_VD_PREDS.dill',\n",
       " 'TEST_CB_STR_PCPTRN_RE-RANKER_VD_YS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_PREDS__.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_YS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_VD_PREDS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_VD_YS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_PREDS__.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_YS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_YS__.dill',\n",
       " 'TEST_SC_STR_PCPTRN_RE-RANKER_VD_2_PREDS__.dill',\n",
       " 'TEST_SC_STR_PCPTRN_RE-RANKER_VD_2_YS_.dill']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_files = filter_by_str(\"_SC_\", files)\n",
    "cb_files = filter_by_str(\"_CB_\", files)\n",
    "len(cb_files), len(sc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_VD_YS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_VD_PREDS_.dill',\n",
       " 'SENT_TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FIXED_VD_YS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_PREDS__.dill',\n",
       " 'SENT_TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FIXED_VD_PREDS__.dill',\n",
       " 'TEST_CB_STR_PCPTRN_RE-RANKER_VD_YS_.dill',\n",
       " 'TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_YS_.dill',\n",
       " 'TEST_CB_STR_PCPTRN_RE-RANKER_VD_PREDS.dill']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENT_TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM_VD_2_PREDS_.dill',\n",
       " 'SENT_TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM_VD_2_YS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_YS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_YS__.dill',\n",
       " 'TEST_SC_STR_PCPTRN_RE-RANKER_VD_2_YS_.dill',\n",
       " 'TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2_VD_PREDS__.dill',\n",
       " 'TEST_SC_STR_PCPTRN_RE-RANKER_VD_2_PREDS__.dill']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "filters = OrderedDict({\n",
    "    \"SENT_PARSER\":  [\"SENT_\", \"_PARSER_\"],\n",
    "    \"ESSAY_PARSER\": \"SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM2\",\n",
    "    \"RE-RANKER\":    \"STR_PCPTRN_RE-RANKER\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def load_predictions(input_files):\n",
    "    algo2preds = dict()\n",
    "    pred_files = filter_by_str(\"_PREDS\", input_files)\n",
    "    for algo_name, fltr in filters.items():\n",
    "        f_files = filter_by_str(fltr, pred_files)\n",
    "        assert len(f_files) == 1, (algo_name,f_files)\n",
    "        fname = FOLDER + f_files[0]\n",
    "        with open(fname, \"rb+\") as f:\n",
    "            algo2preds[algo_name] = dill.load(f)\n",
    "    return algo2preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_preds = load_predictions(cb_files)\n",
    "sc_preds = load_predictions(sc_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FOLDER + \"SENT_TEST_CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_FIXED_VD_YS_.dill\", \"rb+\") as f:\n",
    "    cb_ysbytag = dill.load(f)\n",
    "    \n",
    "with open(FOLDER + \"SENT_TEST_CR_SC_SHIFT_REDUCE_PARSER_TEMPLATED_HYPER_PARAM_VD_2_YS_.dill\", \"rb+\") as f:\n",
    "    sc_ysbytag = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(predsbytag, k_filter=None):\n",
    "    if k_filter is None:\n",
    "        k_filter = set(predsbytag.keys())\n",
    "    all_p = []\n",
    "    \n",
    "    for k, vals in sorted(predsbytag.items(), key = lambda tpl: tpl[0]):\n",
    "        if k not in k_filter:\n",
    "            continue\n",
    "        all_p.extend(vals)\n",
    "    return all_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_p_value_binomial_test(ysbytag, predsbytaga, predsbytagb, alternative='two-sided'):\n",
    "#     assert len(ysbytag.keys()) == len(predsbytaga.keys()), (len(ysbytag.keys()),len(predsbytaga.keys())) \n",
    "#     assert len(ysbytag.keys()) == len(predsbytagb.keys()), (len(ysbytag.keys()),len(predsbytagb.keys())) \n",
    "\n",
    "    first = list(ysbytag.keys())[0]\n",
    "    assert len(ysbytag[first]) == len(predsbytaga[first])\n",
    "    assert len(ysbytag[first]) == len(predsbytagb[first])\n",
    "\n",
    "    lbls = set(ysbytag.keys())\n",
    "    lbls = lbls.intersection(predsbytaga.keys())\n",
    "    lbls = lbls.intersection(predsbytagb.keys())\n",
    "    \n",
    "    ys = get_all_preds(ysbytag, k_filter=lbls)\n",
    "    aas = get_all_preds(predsbytaga, k_filter=lbls)\n",
    "    bbs = get_all_preds(predsbytagb, k_filter=lbls)\n",
    "\n",
    "    assert len(ys) == len(aas) == len(bbs)\n",
    "\n",
    "    successes = defaultdict(int)\n",
    "    both_correct, both_wrong, a_correct_only, b_correct_only = 0,0,0,0\n",
    "    for y,a,b in zip(ys,aas,bbs):\n",
    "        if a == b:\n",
    "            if a == y:\n",
    "                both_correct +=1\n",
    "            else:\n",
    "                both_wrong += 1        \n",
    "        else: # a != b\n",
    "            if a == y:\n",
    "                successes[\"a\"] += 1\n",
    "                a_correct_only += 1\n",
    "            else:\n",
    "                successes[\"b\"] += 1\n",
    "                b_correct_only +=1\n",
    "\n",
    "\n",
    "    mcn_result = mcnemar([[both_correct, a_correct_only],[b_correct_only, both_wrong]], exact=True)\n",
    "#     p_value = stats.binom_test(successes[\"a\"], sum(successes.values()), p=0.5, alternative=alternative)\n",
    "    return mcn_result.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get Predicted Tags from Labelled Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(ysbytag, algo2preds, alternative=\"two-sided\", stats_only=False):\n",
    "    algo2metrics = {}\n",
    "    for algo, preds in algo2preds.items():\n",
    "        mean_metrics = ResultsProcessor.compute_mean_metrics(ysbytag, preds)\n",
    "        algo2metrics[algo] = mean_metrics[__MICRO_F1__]\n",
    "\n",
    "    matrix = dict()\n",
    "    for algo_name_a, predsbytaga in algo2preds.items():\n",
    "        for algo_name_b, predsbytagb in algo2preds.items():\n",
    "            if algo_name_a == algo_name_b:\n",
    "                continue\n",
    "            f1_a = algo2metrics[algo_name_a][\"f1_score\"]\n",
    "            f1_b = algo2metrics[algo_name_b][\"f1_score\"]\n",
    "            pval = compute_p_value_binomial_test(ysbytag, predsbytaga, predsbytagb, alternative)\n",
    "            if stats_only:\n",
    "                print(f\"{algo_name_a.ljust(20)} \\t{algo_name_b.ljust(20)}\\t {pval}\")\n",
    "            else:\n",
    "                print(f\"{algo_name_a.ljust(20)} {f1_a:.4f}\\t{algo_name_b.ljust(20)} {f1_b:.4f}\\t {pval}\")\n",
    "            matrix[(algo_name_a, algo_name_b)] = pval\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from mlxtend.evaluate import cochrans_q\n",
    "\n",
    "def run_cochrans_test(ysbytag, predsbyalgo):\n",
    "    all_preds = []\n",
    "    for a, preds in predsbyalgo.items():\n",
    "        p = get_all_preds(preds)\n",
    "        all_preds.append(np.asarray(p))\n",
    "    ground_truth = np.asarray(get_all_preds(ysbytag))\n",
    "    qvalue, pvalue = cochrans_q(ground_truth, *all_preds)\n",
    "    return qvalue, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.0, 0.0301973834223185)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cochrans_test(cb_ysbytag, cb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT_PARSER          0.7366\tESSAY_PARSER         0.7393\t 0.1453655497472726\n",
      "SENT_PARSER          0.7366\tRE-RANKER            0.7500\t 0.0021949089077805944\n",
      "\n",
      "ESSAY_PARSER         0.7393\tSENT_PARSER          0.7366\t 0.1453655497472726\n",
      "ESSAY_PARSER         0.7393\tRE-RANKER            0.7500\t 0.43699054908597207\n",
      "\n",
      "RE-RANKER            0.7500\tSENT_PARSER          0.7366\t 0.0021949089077805944\n",
      "RE-RANKER            0.7500\tESSAY_PARSER         0.7393\t 0.43699054908597207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#perceptron and CRF differ somewhat from prior run\n",
    "print_comparison(cb_ysbytag, cb_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.130232558139535, 0.12680354511244885)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cochrans_test(sc_ysbytag, sc_preds) # not surprising, results are virtually identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT_PARSER          0.8271\tESSAY_PARSER         0.8210\t 0.15188924140345142\n",
      "SENT_PARSER          0.8271\tRE-RANKER            0.8292\t 0.9049745264594615\n",
      "\n",
      "ESSAY_PARSER         0.8210\tSENT_PARSER          0.8271\t 0.15188924140345142\n",
      "ESSAY_PARSER         0.8210\tRE-RANKER            0.8292\t 0.12135284705943229\n",
      "\n",
      "RE-RANKER            0.8292\tSENT_PARSER          0.8271\t 0.9049745264594615\n",
      "RE-RANKER            0.8292\tESSAY_PARSER         0.8210\t 0.12135284705943229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN only differs\n",
    "print_comparison(sc_ysbytag, sc_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT_PARSER          \tESSAY_PARSER        \t 0.1453655497472726\n",
      "SENT_PARSER          \tRE-RANKER           \t 0.0021949089077805944\n",
      "\n",
      "ESSAY_PARSER         \tSENT_PARSER         \t 0.1453655497472726\n",
      "ESSAY_PARSER         \tRE-RANKER           \t 0.43699054908597207\n",
      "\n",
      "RE-RANKER            \tSENT_PARSER         \t 0.0021949089077805944\n",
      "RE-RANKER            \tESSAY_PARSER        \t 0.43699054908597207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_comparison(cb_ysbytag, cb_preds, stats_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENT_PARSER          \tESSAY_PARSER        \t 0.15188924140345142\n",
      "SENT_PARSER          \tRE-RANKER           \t 0.9049745264594615\n",
      "\n",
      "ESSAY_PARSER         \tSENT_PARSER         \t 0.15188924140345142\n",
      "ESSAY_PARSER         \tRE-RANKER           \t 0.12135284705943229\n",
      "\n",
      "RE-RANKER            \tSENT_PARSER         \t 0.9049745264594615\n",
      "RE-RANKER            \tESSAY_PARSER        \t 0.12135284705943229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_comparison(sc_ysbytag, sc_preds, stats_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:stats_test]",
   "language": "python",
   "name": "conda-env-stats_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
