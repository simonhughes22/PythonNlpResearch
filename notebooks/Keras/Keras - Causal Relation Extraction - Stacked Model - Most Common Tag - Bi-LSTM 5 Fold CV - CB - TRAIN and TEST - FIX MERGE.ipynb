{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on this code: https://github.com/codekansas/keras-language-modeling/blob/master/keras_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check mongo is running\n",
    "import pymongo\n",
    "client = pymongo.MongoClient()\n",
    "db = client.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - To Get this working:\n",
    "\n",
    "* Install CUDA and associated libraries, setup path\n",
    "* Install bleeding edge theano (from src)\n",
    "* Make sure the THEANO_FLAGS are set correctly via the environment var, or via the ~/.theanorc file\n",
    "* Install and compile bleeding edge Keras (from src)\n",
    "* `export KERAS_BACKEND=theano`\n",
    "* `export KERAS_IMAGE_DIM_ORDERING='th'`\n",
    "* `sh <project_root>/shell_scipts/setup_environment.sh` to install additional dependencies\n",
    "* **DO NOT SET UNROLL=True** when creating RNN's - causes max recursion issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouble-Shooting\n",
    "\n",
    "* You may need to clean the theano cache. To do so thoroughly, run this command from the shell:\n",
    " * `theano-cache purge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import dill\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, TimeDistributedDense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from Metrics import rpf1\n",
    "from load_data import load_process_essays\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "#from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from window_based_tagger_config import get_config\n",
    "from IdGenerator import IdGenerator as idGen\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from Rpfa import micro_rpfa\n",
    "from collections import defaultdict\n",
    "\n",
    "import Settings\n",
    "import logging\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Pre-Process Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "WARNING - No db name specified - should be either 'metrics_causal' or 'metrics'. Defaulting to 'metrics' \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from CrossValidation import cross_validation\n",
    "from BrattEssay import load_bratt_essays\n",
    "from load_data import load_process_essays\n",
    "from collections import defaultdict\n",
    "from IterableFP import flatten\n",
    "from Settings import Settings\n",
    "from Settings import Settings\n",
    "\n",
    "CV_FOLDS = 5\n",
    "DEV_SPLIT = 0.1\n",
    "\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "training_pickled = settings.data_directory + \"CoralBleaching/Thesis_Dataset/training.pl\"\n",
    "models_folder = root_folder + \"Models/Bi-LSTM_Stacked/\"\n",
    "cv_folder = root_folder + \"CV_Data_Pickled/\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "processor = ResultsProcessor(dbname=\"metrics_causal2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/training.pl\n"
     ]
    }
   ],
   "source": [
    "print(training_pickled)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "config = get_config(training_folder)\n",
    "tagged_essays_tmp = load_process_essays(**config)\n",
    "\n",
    "with open(training_pickled, \"wb+\") as f:\n",
    "    pickle.dump(tagged_essays_tmp, f)\n",
    "del tagged_essays_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(training_pickled, \"rb+\") as f:\n",
    "    tagged_essays = pickle.load(f)\n",
    "len(tagged_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 files found\n",
      "226 essays processed\n"
     ]
    }
   ],
   "source": [
    "test_config = get_config(test_folder)\n",
    "tagged_essays_test = load_process_essays(**test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2019-04-17 17:13:08.914499\n"
     ]
    }
   ],
   "source": [
    "import datetime, logging\n",
    "print(\"Started at: \" + str(datetime.datetime.now()))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "shuffle(tagged_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1677, 13, 86)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = defaultdict(int)\n",
    "unique_words = set()\n",
    "\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "# This caused some discrepancies with the other models. I believe this is here to prevent errors\n",
    "# with some of the later code, but unfortunately it potentially breaks the micro-metrics\n",
    "for essay in tagged_essays_test:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            unique_words.add(word)\n",
    "            for tag in tags:\n",
    "                tag_freq[tag] += 1\n",
    "\n",
    "EMPTY_TAG = \"Empty\"\n",
    "regular_tags = list((t for t in tag_freq.keys() if t[0].isdigit()))\n",
    "cr_tags = list((t for t in tag_freq.keys() if ( \"->\" in t) and \n",
    "                not \"Anaphor\" in t and \n",
    "                not \"other\" in t and \n",
    "                not \"rhetorical\" in t and\n",
    "                not \"factor\" in t and \n",
    "                1==1\n",
    "               ))\n",
    "\n",
    "vtags = set(regular_tags)\n",
    "vtags.add(EMPTY_TAG)\n",
    "\n",
    "cr_vtags = set(cr_tags)\n",
    "cr_vtags.add(EMPTY_TAG)\n",
    "\n",
    "len(unique_words), len(regular_tags), len(cr_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '11', '12', '13', '14', '2', '3', '4', '5', '50', '5b', '6', '7']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(regular_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '5b',\n",
       " '6',\n",
       " '7',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Causer:1->Result:11',\n",
       " 'Causer:1->Result:13',\n",
       " 'Causer:1->Result:14',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:1->Result:3',\n",
       " 'Causer:1->Result:4',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:1->Result:7',\n",
       " 'Causer:11->Result:11',\n",
       " 'Causer:11->Result:12',\n",
       " 'Causer:11->Result:13',\n",
       " 'Causer:11->Result:14',\n",
       " 'Causer:11->Result:3',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:11->Result:6',\n",
       " 'Causer:12->Result:11',\n",
       " 'Causer:12->Result:13',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:12->Result:5b',\n",
       " 'Causer:12->Result:7',\n",
       " 'Causer:13->Result:11',\n",
       " 'Causer:13->Result:12',\n",
       " 'Causer:13->Result:14',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:13->Result:5',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:13->Result:6',\n",
       " 'Causer:13->Result:7',\n",
       " 'Causer:14->Result:50',\n",
       " 'Causer:14->Result:6',\n",
       " 'Causer:14->Result:7',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:2->Result:3',\n",
       " 'Causer:2->Result:50',\n",
       " 'Causer:2->Result:6',\n",
       " 'Causer:3->Result:1',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:3->Result:14',\n",
       " 'Causer:3->Result:2',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:3->Result:50',\n",
       " 'Causer:3->Result:5b',\n",
       " 'Causer:3->Result:6',\n",
       " 'Causer:3->Result:7',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:4->Result:13',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:4->Result:3',\n",
       " 'Causer:4->Result:5',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:4->Result:5b',\n",
       " 'Causer:4->Result:6',\n",
       " 'Causer:4->Result:7',\n",
       " 'Causer:5->Result:11',\n",
       " 'Causer:5->Result:13',\n",
       " 'Causer:5->Result:14',\n",
       " 'Causer:5->Result:3',\n",
       " 'Causer:5->Result:4',\n",
       " 'Causer:5->Result:50',\n",
       " 'Causer:5->Result:5b',\n",
       " 'Causer:5->Result:7',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:50->Result:3',\n",
       " 'Causer:50->Result:50',\n",
       " 'Causer:50->Result:7',\n",
       " 'Causer:5b->Result:14',\n",
       " 'Causer:5b->Result:5',\n",
       " 'Causer:5b->Result:50',\n",
       " 'Causer:5b->Result:7',\n",
       " 'Causer:6->Result:14',\n",
       " 'Causer:6->Result:5',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:6->Result:5b',\n",
       " 'Causer:6->Result:7',\n",
       " 'Causer:7->Result:1',\n",
       " 'Causer:7->Result:13',\n",
       " 'Causer:7->Result:14',\n",
       " 'Causer:7->Result:4',\n",
       " 'Causer:7->Result:5',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:7->Result:5b',\n",
       " 'Empty']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(cr_vtags))\n",
    "sorted(cr_vtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Essays into Training Data (Word Ids)\n",
    "\n",
    "* Computes `xs`, `ys`, `ys_bytag` and `seq_lens`\n",
    "* `ys_bytag` includes **all tags** and does **not** focus only on the most common tag\n",
    "* `ys` only includes the most common tag (so we can use cross entropy)\n",
    "* `seq_lens` is without the start and end tags included (so we have to map back and forth to maintain mappings)\n",
    "* `ys_bytag` also excludes the START and END tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Max Sequence Length, Generate All Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix2tag = {}\n",
    "for ix, t in enumerate(vtags):\n",
    "    ix2tag[ix] = t\n",
    "\n",
    "ix2crtag = {}\n",
    "for ix, t in enumerate(cr_vtags):\n",
    "    ix2crtag[ix] = t\n",
    "    \n",
    "generator = idGen(seed=1) # important as we zero pad sequences\n",
    "\n",
    "maxlen = 0\n",
    "for essay in tagged_essays:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "        \n",
    "for essay in tagged_essays_test:\n",
    "    for sentence in essay.sentences:\n",
    "        for word, tags in sentence:\n",
    "            id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "        maxlen = max(maxlen, len(sentence) + 2)\n",
    "\n",
    "def ids2tags(ids):\n",
    "    return [generator.get_key(j) for j in ids]\n",
    "\n",
    "def lbls2tags(ixs):\n",
    "    return [ix2tag[ix] for ix in ixs]\n",
    "        \n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "START = \"<start>\"\n",
    "END   = \"<end>\"\n",
    "\n",
    "def get_training_data(tessays):\n",
    "    # outputs\n",
    "    xs = []\n",
    "    ys = []\n",
    "    ys_bytag_concept_sent = defaultdict(list)\n",
    "    ys_bytag_cr_sent = defaultdict(list)\n",
    "    seq_lens = []\n",
    "\n",
    "    # cut texts after this number of words (among top max_features most common words)\n",
    "    for essay in tessays:\n",
    "        for sentence in essay.sentences:\n",
    "            row = []\n",
    "            y_found = False\n",
    "            y_seq = []\n",
    "            unique_tags = set() # get all unique tags in sentence\n",
    "            for word, tags in [(START, set())] + sentence + [(END, set())]:\n",
    "                id = generator.get_id(word) #starts at 0, but 0 used to pad sequences\n",
    "                row.append(id)\n",
    "                                \n",
    "                # Make sure to include Causer:<num> and Result:<num> tags, as we do for the parser model\n",
    "                missing_tags = [t.replace(\"Causer:\",\"\").replace(\"Result:\",\"\") \n",
    "                                    for t in tags\n",
    "                                ]\n",
    "                # Filter to just concept codes that were missing\n",
    "                missing_tags = set([t for t in missing_tags if t[0].isdigit() and \"->\" not in t])\n",
    "                new_tags = tags.union(missing_tags)\n",
    "                #if missing_tags:\n",
    "                #    diff = missing_tags - tags\n",
    "                #    if diff:\n",
    "                #        print(diff, tags)\n",
    "\n",
    "                tags = new_tags\n",
    "                \n",
    "                unique_tags.update(tags)\n",
    "                               \n",
    "                # remove unwanted tags, filter to concept tags\n",
    "                concept_tags = vtags.intersection(tags)\n",
    "\n",
    "                # encode ys with most common tag only\n",
    "                if len(concept_tags) > 1:\n",
    "                    most_common = max(concept_tags, key=lambda t: tag_freq[t])\n",
    "                    concept_tags = set([most_common])\n",
    "                if len(concept_tags) == 0:\n",
    "                    concept_tags.add(EMPTY_TAG)\n",
    "\n",
    "                one_hot = []\n",
    "                for t in vtags:\n",
    "                    if t in concept_tags:\n",
    "                        one_hot.append(1)\n",
    "                    else:\n",
    "                        one_hot.append(0)\n",
    "                y_seq.append(one_hot)\n",
    "                #end for each word\n",
    "            \n",
    "            # sentence level tags\n",
    "            for tag in vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_concept_sent[tag].append(1)\n",
    "                else:\n",
    "                    ys_bytag_concept_sent[tag].append(0)\n",
    "            \n",
    "            for tag in cr_vtags:\n",
    "                if tag in unique_tags:\n",
    "                    ys_bytag_cr_sent[tag].append(1)\n",
    "                else:\n",
    "                    ys_bytag_cr_sent[tag].append(0)\n",
    "                \n",
    "            seq_lens.append(len(row)-2)\n",
    "            ys.append(y_seq)\n",
    "            xs.append(row)\n",
    "    \n",
    "    xs = sequence.pad_sequences(xs, maxlen=maxlen)\n",
    "    ys = sequence.pad_sequences(ys, maxlen=maxlen)\n",
    "    assert xs.shape[0] == ys.shape[0], \"Sequences should have the same number of rows\"\n",
    "    assert xs.shape[1] == ys.shape[1] == maxlen, \"Sequences should have the same lengths\"\n",
    "    return xs, ys, ys_bytag_concept_sent, ys_bytag_cr_sent, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def collapse_results(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = pred_ys[1:-1]\n",
    "        for pred_tag in pred_ys:\n",
    "            pred_ys_by_tag[pred_tag].append(1)\n",
    "            # for all other tags, a 0\n",
    "            for tag in(vtags - set([EMPTY_TAG, pred_tag])):\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "def collapse_results_sentence_level(seq_lens, preds):\n",
    "    assert len(seq_lens) == preds.shape[0], \"Axis 1 size does not align\"\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(len(seq_lens)):\n",
    "        row_ixs = preds[i,:]\n",
    "        len_of_sequence = seq_lens[i] + 2\n",
    "        # sequences are padded from the left, take the preds from the end of the seq\n",
    "        pred_ys = [ix2tag[j] for j in row_ixs[-len_of_sequence:]]\n",
    "        # skip the start and end label\n",
    "        pred_ys = set(pred_ys[1:-1])\n",
    "        for tag in vtags:\n",
    "            if tag == EMPTY_TAG:\n",
    "                continue\n",
    "            if tag in pred_ys:\n",
    "                pred_ys_by_tag[tag].append(1)\n",
    "            else:\n",
    "                pred_ys_by_tag[tag].append(0)\n",
    "        if EMPTY_TAG in pred_ys_by_tag:\n",
    "            del pred_ys_by_tag[EMPTY_TAG]\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dev_split(lst, dev_split, randomize=True):\n",
    "    # random shuffle\n",
    "    if randomize:\n",
    "        shuffle(lst)\n",
    "    num_training = int((1.0 - dev_split) * len(lst))\n",
    "    return lst[:num_training], lst[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.82 s, sys: 579 ms, total: 7.4 s\n",
      "Wall time: 7.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this name for a different function later\n",
    "from CrossValidation import cross_validation as cv\n",
    "\n",
    "folds = cv(tagged_essays, CV_FOLDS)\n",
    "fold2training_data = {}\n",
    "fold2dev_data = {}\n",
    "fold2test_data = {}\n",
    "\n",
    "for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "    # further split into train and dev test\n",
    "    essays_train, essays_dev = train_dev_split(essays_TD, DEV_SPLIT)\n",
    "    fold2training_data[i] = get_training_data(essays_train)\n",
    "    fold2dev_data[i]      = get_training_data(essays_dev)\n",
    "    # Test Data\n",
    "    fold2test_data[i]     = get_training_data(essays_VD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(cv_folder + \"td.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2training_data, f)\n",
    "\n",
    "with open(cv_folder + \"devd.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2dev_data, f)\n",
    "\n",
    "with open(cv_folder + \"vd.dill\", \"wb\") as f:\n",
    "    dill.dump(fold2test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure the generator is incremented on the test data too\n",
    "_,_,_,_,_, = get_training_data(tagged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove 100 Dim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see /Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/DeepLearning/WordVectors/pickle_glove_embedding.py\n",
    "# for creating pre-filtered embeddings file\n",
    "import pickle, os\n",
    "from numpy.linalg import norm\n",
    "\n",
    "embeddings_file = \"/Users/simon.hughes/data/word_embeddings/glove.6B/cb_dict_glove.6B.100d.txt\"\n",
    "# read data file\n",
    "with open(embeddings_file, \"rb+\") as f:\n",
    "    cb_emb_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 1677 2.5 %\n"
     ]
    }
   ],
   "source": [
    "missed = set()\n",
    "for wd in unique_words:\n",
    "    if wd not in cb_emb_index:\n",
    "        missed.add(wd)\n",
    "print(len(missed), len(unique_words), 100.0 * round(len(missed)/  len(unique_words),4), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = list(cb_emb_index.values())[0].shape[0]\n",
    "\n",
    "def get_embedding_matrix(words, idgenerator, max_features, init='uniform', unit_length=False):\n",
    "    embedding_dim = list(cb_emb_index.values())[0].shape[0]\n",
    "    # initialize with a uniform distribution\n",
    "    if init == 'uniform':\n",
    "        # NOTE: the max norms for these is quite low relative to the embeddings\n",
    "        embedding_matrix = np.random.uniform(low=-0.05, high=0.05,size=(max_features, embedding_dim))\n",
    "    elif init =='zeros':\n",
    "        embedding_matrix = np.zeros(shape=(max_features, embedding_dim), dtype=np.float32)\n",
    "    elif init == 'normal':\n",
    "        embedding_matrix = np.random.normal(mean, sd, size=(max_features, embedding_dim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown init type\")\n",
    "    for word in words:\n",
    "        i = idgenerator.get_id(word)\n",
    "        embedding_vector = cb_emb_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    if unit_length:\n",
    "        norms = np.linalg.norm(embedding_matrix, axis=1,keepdims=True)\n",
    "        # remove 0 norms to prevent divide by zero\n",
    "        norms[norms == 0.0] = 1.0\n",
    "        embedding_matrix = embedding_matrix / norms\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict_classes(xs, batch_size=batch_size, verbose=0)   \n",
    "    pred_ys_by_tag = collapse_results_sentence_level(seq_len, preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pivot_predictions_to_dict(preds):\n",
    "    pred_ys_by_tag = defaultdict(list)\n",
    "    for i in range(preds.shape[0]):\n",
    "        row = preds[i]\n",
    "        for tag_ix, pred in enumerate(row):\n",
    "            tag = ix2crtag[tag_ix]\n",
    "            pred_ys_by_tag[tag].append(pred)\n",
    "    return pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_predictions_sent_level(model, xs, ys_by_tag, seq_len):\n",
    "    preds = model.predict(xs, batch_size=batch_size, verbose=0)\n",
    "    preds = np.where(preds >= 0.5, 1, 0)\n",
    "    pred_ys_by_tag = pivot_predictions_to_dict(preds)\n",
    "    class2metrics = ResultsProcessor.compute_metrics(ys_by_tag, pred_ys_by_tag)\n",
    "    micro_metrics = micro_rpfa(class2metrics.values())\n",
    "    return micro_metrics, pred_ys_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2019-04-17 17:13:26.777555', '20190417_171326_777581')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from datetime import datetime\n",
    "\n",
    "def get_ts():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def get_file_ts():\n",
    "    return datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "\n",
    "embedding_size = EMBEDDING_DIM\n",
    "hidden_size    = 128\n",
    "out_size = len(vtags)\n",
    "batch_size = 128\n",
    "merge_mode = \"sum\"\n",
    "\n",
    "get_ts(), get_file_ts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Bi-Directional LSTM With Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features=len(generator.get_ids())+2 #Need plus one maybe due to masking of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/Google Drive/Phd/Data/CoralBleaching/Thesis_Dataset/Models/Bi-LSTM_Stacked/fold_ix-0_bi_directional-True_hidden_size-128_merge_mode-sum_num_rnns-2_use_pretrained_embedding-True.h5'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_file_name(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    lcls = locals()\n",
    "    s = \"\"\n",
    "    for k, val in sorted(lcls.items(), key = lambda tpl: (0,tpl[0]) if tpl[0] == 'fold_ix' else (1,tpl[0])):\n",
    "        s += \"{key}-{val}_\".format(key=k, val=str(val))\n",
    "    return models_folder + s[:-1] + \".h5\"\n",
    "\n",
    "get_file_name(0, True, True, 2, merge_mode, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_fold(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "\n",
    "    if use_pretrained_embedding:\n",
    "        embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform', unit_length=False)\n",
    "        embedding_layer = Embedding(max_features,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=True,\n",
    "                                mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    else:\n",
    "        embedding_layer = Embedding(max_features, embedding_size, input_length=maxlen, trainable=True, mask_zero=True)\n",
    "\n",
    "    if bi_directional:\n",
    "        rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "    else:\n",
    "        rnn_layer_fact = lambda : GRU(hidden_size, return_sequences=True, consume_less=\"cpu\")\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "    \n",
    "    X_train, y_train, train_ys_bytag_con_sent, train_ys_by_tag_cr_sent, seq_len_train = fold2training_data[fold_ix]\n",
    "    X_dev,   y_dev,   dev_ys_bytag_con_sent,   dev_ys_by_tag_cr_sent,   seq_len_dev   = fold2dev_data[fold_ix]\n",
    "    X_test,  y_test,  test_ys_bytag_con_sent,  test_ys_by_tag_cr_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "    #for i in range(2):\n",
    "        #print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1 # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_bytag_con_sent, seq_len_dev)\n",
    "\n",
    "        #print(micro_metrics)\n",
    "        #print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Fold[{ix}] - Best F1 Score={f1}\".format(ix=fold_ix, f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Param Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fold2model = {}\n",
    "    for i in range(CV_FOLDS):\n",
    "        model = evaluate_fold(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        fname = get_file_name(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        model.save(fname)    \n",
    "        fold2model[i] = model\n",
    "    return fold2model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a quick run to validate it is working"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i = 0\n",
    "for use_pretrained_embedding in [True]:\n",
    "    for bi_directional in [False]:\n",
    "        for num_rnns in [1]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [16]:\n",
    "\n",
    "                    i += 1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    fold2model = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    #print(\"MicroF1={micro_f1}\".format(micro_f1=micro_f1))\n",
    "                    print(get_ts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Params 2019-04-17 17:13:26.987480 - Embeddings=True, Bi-Direct=True Num_Rnns=2 Hidden_Size=256\n",
      "WARNING:tensorflow:From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1029: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 17:13:27,775 : WARNING : From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1029: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:993: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 17:13:28,837 : WARNING : From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:993: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1108: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-17 17:13:28,960 : WARNING : From /Users/simon.hughes/anaconda3/envs/phd_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1108: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold[0] - Best F1 Score=0.8968609865470851\n",
      "Fold[1] - Best F1 Score=0.9035262807717898\n",
      "Fold[2] - Best F1 Score=0.9092188599577763\n",
      "Fold[3] - Best F1 Score=0.8945783132530121\n",
      "Fold[4] - Best F1 Score=0.8967874231032127\n",
      "2019-04-17 20:03:35.225293\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "i = 0\n",
    "for use_pretrained_embedding in [True]:\n",
    "    for bi_directional in [True]:\n",
    "        for num_rnns in [2]:\n",
    "            for merge_mode in [\"sum\"]:\n",
    "                for hidden_size in [256]:\n",
    "\n",
    "                    i += 1\n",
    "                    print(\"[{i}] Params {ts} - Embeddings={use_pretrained_embedding}, Bi-Direct={bi_directional} Num_Rnns={num_rnns} Hidden_Size={hidden_size}\"\\\n",
    "                          .format(i=i, ts=get_ts(), use_pretrained_embedding=use_pretrained_embedding, bi_directional=bi_directional, num_rnns=num_rnns, hidden_size=hidden_size))\n",
    "                    fold2model = cross_validation(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "                    #print(\"MicroF1={micro_f1}\".format(micro_f1=micro_f1))\n",
    "                    print(get_ts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_model(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    fname = get_file_name(fold_ix, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "    return keras.models.load_model(fname)\n",
    "\n",
    "def load_models(use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "    models_by_fold = {}\n",
    "    for i in range(CV_FOLDS):\n",
    "        model = load_model(i, use_pretrained_embedding, bi_directional, num_rnns, merge_mode, hidden_size)\n",
    "        models_by_fold[i] = model\n",
    "    return models_by_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568/1568 [==============================] - 18s    \n",
      "1605/1605 [==============================] - 18s    \n",
      "1685/1685 [==============================] - 19s    \n",
      "1699/1699 [==============================] - 20s    \n",
      "1735/1735 [==============================] - 20s    \n"
     ]
    }
   ],
   "source": [
    "predicts_by_fold = {}\n",
    "for fold_ix in range(CV_FOLDS):\n",
    "    X_test,  y_test,  test_ys_bytag_con_sent,  test_ys_by_tag_cr_sent,  seq_len_test  = fold2test_data[fold_ix]\n",
    "    model = fold2model[fold_ix]\n",
    "    probs = model.predict_proba(X_test)\n",
    "    predicts_by_fold[fold_ix] = probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicts_by_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test Data Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_test(num_rnns, merge_mode, hidden_size):\n",
    "    embedding_matrix = get_embedding_matrix(unique_words, generator, max_features, init='uniform', unit_length=False)\n",
    "    embedding_layer = Embedding(max_features,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True) # If false, initialize unfound words with all 0's\n",
    "    rnn_layer_fact = lambda : Bidirectional(GRU(hidden_size, return_sequences=True, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    for i in range(num_rnns):\n",
    "        model.add(rnn_layer_fact())\n",
    "\n",
    "    model.add(TimeDistributedDense(out_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "    \n",
    "    essays_train, essays_dev = train_dev_split(tagged_essays, DEV_SPLIT)\n",
    "    X_train, y_train,  train_ys_bytag_con_sent,  train_ys_by_tag_cr_sent,  seq_len_train = get_training_data(essays_train)\n",
    "    X_dev,   y_dev,    dev_ys_bytag_con_sent,    dev_ys_by_tag_cr_sent,    seq_len_dev   = get_training_data(essays_dev)\n",
    "\n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 3\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions(model, X_dev, dev_ys_bytag_con_sent, seq_len_dev)\n",
    "\n",
    "        print(micro_metrics)\n",
    "        print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Fold[{ix}] - Best F1 Score={f1}\".format(ix=fold_ix, f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-17 20:05:17.365457: Epoch=0\n",
      "Recall: 0.8711, Precision: 0.8562, F1: 0.8636, Accuracy: 0.9787, Codes:   923\n",
      "\n",
      "2019-04-17 20:09:26.157137: Epoch=1\n",
      "Recall: 0.9079, Precision: 0.8877, F1: 0.8977, Accuracy: 0.9840, Codes:   923\n",
      "\n",
      "2019-04-17 20:13:35.303918: Epoch=2\n",
      "Recall: 0.8992, Precision: 0.8954, F1: 0.8973, Accuracy: 0.9840, Codes:   923\n",
      "\n",
      "2019-04-17 20:17:29.421119: Epoch=3\n",
      "Recall: 0.8971, Precision: 0.8865, F1: 0.8918, Accuracy: 0.9831, Codes:   923\n",
      "\n",
      "2019-04-17 20:21:23.326997: Epoch=4\n",
      "Recall: 0.9068, Precision: 0.8961, F1: 0.9015, Accuracy: 0.9846, Codes:   923\n",
      "\n",
      "2019-04-17 20:25:16.936736: Epoch=5\n",
      "Recall: 0.8960, Precision: 0.9148, F1: 0.9053, Accuracy: 0.9855, Codes:   923\n",
      "\n",
      "2019-04-17 20:29:05.306674: Epoch=6\n",
      "Recall: 0.9328, Precision: 0.8706, F1: 0.9006, Accuracy: 0.9840, Codes:   923\n",
      "\n",
      "2019-04-17 20:32:59.957264: Epoch=7\n",
      "Recall: 0.9036, Precision: 0.9055, F1: 0.9046, Accuracy: 0.9852, Codes:   923\n",
      "\n",
      "2019-04-17 20:36:58.162676: Epoch=8\n",
      "Recall: 0.9177, Precision: 0.8906, F1: 0.9039, Accuracy: 0.9849, Codes:   923\n",
      "\n",
      "Fold[4] - Best F1 Score=0.9053092501368363\n"
     ]
    }
   ],
   "source": [
    "test_model = evaluate_test(2, \"sum\", 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1918/1918 [==============================] - 23s    \n"
     ]
    }
   ],
   "source": [
    "X_test,  y_test,   test_ys_bytag_con_sent,   test_ys_by_tag_cr_sent,   seq_len_test = get_training_data(tagged_essays_test)\n",
    "test_probs = test_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1918, 141, 14), 1918)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_probs.shape, len(test_ys_by_tag_cr_sent['Causer:4->Result:3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train and Test Data For Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stacked_feat_from_probs(probs, max_feats, min_feats, average_feats, binary_feats, combo_feats):\n",
    "    xs = []\n",
    "    for i in range(len(probs)):\n",
    "        preds = probs[i,:]\n",
    "        max_preds = np.max(preds, axis=0)\n",
    "        min_preds = np.max(preds, axis=0)\n",
    "        mean_preds = np.mean(preds, axis=0)\n",
    "        \n",
    "        predicted_ixs = set(np.argwhere(max_preds > 0.5).flatten())\n",
    "        binary = [0] * len(max_preds)\n",
    "        for ix in predicted_ixs:\n",
    "            binary[ix] = 1\n",
    "        \n",
    "        x = []\n",
    "        if binary_feats:\n",
    "            x += binary\n",
    "        if max_feats:\n",
    "            x += max_preds.tolist()\n",
    "        if min_feats:            \n",
    "            x += min_preds.tolist()\n",
    "        if average_feats:\n",
    "            x += mean_preds.tolist()\n",
    "\n",
    "        # combination tags\n",
    "        if combo_feats:\n",
    "            ixs = ix2tag.keys()\n",
    "            for a in ixs:\n",
    "                for b in ixs:\n",
    "                    if b < a:\n",
    "                        if a in predicted_ixs and b in predicted_ixs:\n",
    "                            x.append(1)\n",
    "                        else:\n",
    "                            x.append(0)\n",
    "        xs.append(x)\n",
    "    return np.asarray(xs)\n",
    "\n",
    "def get_stacked_feats_by_fold(fold_ix, predicts_by_fold, max_feats, min_feats, average_feats, binary_feats, combo_feats):\n",
    "    probs = predicts_by_fold[fold_ix]\n",
    "    xs = get_stacked_feat_from_probs(probs, max_feats, min_feats, average_feats, binary_feats, combo_feats)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "stacked_feats_by_code = {}\n",
    "for fold_ix in range(CV_FOLDS):\n",
    "    stacked_feats_by_code[fold_ix] = get_stacked_feats_by_fold(fold_ix, predicts_by_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop Thru Each Fold, Merge the Xs and Ys from the Other Folds as TD, and then Use Fold as VD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordtagginghelper import merge_dictionaries\n",
    "\n",
    "def generate_stacked_features(max_feats, min_feats, average_feats, binary_feats, combo_feats):\n",
    "    stacked_feats_by_code = {}\n",
    "    for fold_ix in range(CV_FOLDS):\n",
    "        stacked_feats_by_code[fold_ix] = get_stacked_feats_by_fold(fold_ix, predicts_by_fold, max_feats, min_feats, average_feats, binary_feats, combo_feats)\n",
    "    \n",
    "    td_xs_by_fold = {}\n",
    "    vd_xs_by_fold = {}\n",
    "\n",
    "    td_ys_by_fold = {}\n",
    "    vd_ys_by_fold = {}\n",
    "    for vd_ix in range(CV_FOLDS):\n",
    "        td = []\n",
    "\n",
    "        td_ys = defaultdict(list)\n",
    "        vd_ys = defaultdict(list)\n",
    "        for td_ix in range(CV_FOLDS):\n",
    "            if td_ix == vd_ix:\n",
    "                continue\n",
    "            xs = stacked_feats_by_code[td_ix]\n",
    "            td.append(xs)\n",
    "\n",
    "            _, _, _, td_ys_by_tag_cr_sent, _ = fold2test_data[td_ix]\n",
    "            merge_dictionaries(td_ys_by_tag_cr_sent, td_ys)\n",
    "\n",
    "        vd_xs_by_fold[vd_ix] = stacked_feats_by_code[vd_ix]\n",
    "        td_xs_by_fold[vd_ix] = np.vstack(td)\n",
    "\n",
    "        del td_ys[EMPTY_TAG]\n",
    "        td_ys_by_fold[vd_ix] = td_ys\n",
    "\n",
    "        _, _, _, vd_ys_by_tag_cr_sent, _ = fold2test_data[vd_ix]\n",
    "        # make a copy (so can delete EMPTY tag)\n",
    "        merge_dictionaries(vd_ys_by_tag_cr_sent, vd_ys)\n",
    "        \n",
    "        del vd_ys[EMPTY_TAG]\n",
    "        assert EMPTY_TAG not in td_ys\n",
    "        assert EMPTY_TAG not in vd_ys\n",
    "        \n",
    "        vd_ys_by_fold[vd_ix] = vd_ys\n",
    "    return td_xs_by_fold, td_ys_by_fold, vd_xs_by_fold, vd_ys_by_fold"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Validate the sizes of the arrays match expected\n",
    "for i in range(CV_FOLDS):\n",
    "    print(i)\n",
    "    print(td_xs_by_fold[i].shape)\n",
    "    print(vd_xs_by_fold[i].shape)\n",
    "    print(\"\")\n",
    "    key = \"Causer:1->Result:50\"\n",
    "    print(len(td_ys_by_fold[i].keys()))\n",
    "    print(len(vd_ys_by_fold[i].keys()))\n",
    "    print(len(td_ys_by_fold[i][key]))\n",
    "    print(len(vd_ys_by_fold[i][key]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Stacked Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordtagginghelper import train_classifier_per_code, test_classifier_per_code\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_stacked_classifier(dual, penalty, C, max_feats, min_feats, average_feats, binary_feats, combo_feats):\n",
    "    # capture param values\n",
    "    fn_args  = dict(locals())\n",
    "    \n",
    "    td_xs_by_fold, td_ys_by_fold, vd_xs_by_fold, vd_ys_by_fold = generate_stacked_features(max_feats, min_feats, average_feats, binary_feats, combo_feats)\n",
    "    \n",
    "    fn_create_sent_cls  = lambda : LogisticRegression(dual=dual, C=C, penalty=penalty)\n",
    "\n",
    "    cv_wd_td_ys_by_tag, cv_wd_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_wd_vd_ys_by_tag, cv_wd_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    num_feats = []\n",
    "    for i in range(CV_FOLDS):\n",
    "        sent_td_xs = td_xs_by_fold[i]\n",
    "        sent_vd_xs = vd_xs_by_fold[i]\n",
    "        \n",
    "        num_feats.append(sent_vd_xs.shape[1])\n",
    "\n",
    "        sent_td_ys_bycode = td_ys_by_fold[i]\n",
    "        sent_vd_ys_bycode = vd_ys_by_fold[i]\n",
    "\n",
    "        tags = sent_td_ys_bycode.keys()\n",
    "\n",
    "        tag2sent_classifier = train_classifier_per_code(sent_td_xs, sent_td_ys_bycode , fn_create_sent_cls, tags, verbose=False)\n",
    "        td_sent_predictions_by_code \\\n",
    "            = test_classifier_per_code(sent_td_xs, tag2sent_classifier, tags )\n",
    "\n",
    "        vd_sent_predictions_by_code \\\n",
    "            = test_classifier_per_code(sent_vd_xs, tag2sent_classifier, tags )\n",
    "\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(td_sent_predictions_by_code, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(vd_sent_predictions_by_code, cv_sent_vd_predictions_by_tag)\n",
    "\n",
    "    sent_algo = str(fn_create_sent_cls())\n",
    "\n",
    "    CB_SENT_TD, CB_SENT_VD = \"CR_CB_STACKED_TD_CNT_FEATS\", \"CR_CB_STACKED_VD_CNT_FEATS\"\n",
    "    parameters = dict(config)\n",
    "    parameters[\"extractors\"] = []\n",
    "    parameters[\"num_feats_MEAN\"] = np.mean(num_feats)\n",
    "    # merge in function args\n",
    "    parameters.update(fn_args)\n",
    "\n",
    "    sent_td_objectid = processor.persist_results(CB_SENT_TD, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, parameters, sent_algo)\n",
    "    sent_vd_objectid = processor.persist_results(CB_SENT_VD, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, parameters, sent_algo)\n",
    "    \n",
    "    avg_f1 = float(processor.get_metric(CB_SENT_VD, sent_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "    return avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Optimal Stacked Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-03 21:52:10.308026 1 MICRO: F1: 0.692014 Max:True Min:True Avg:True Binary:True Combo:True\n",
      "2017-12-03 21:52:28.890105 2 MICRO: F1: 0.687338 Max:True Min:True Avg:True Binary:True Combo:False\n",
      "2017-12-03 21:52:48.725480 3 MICRO: F1: 0.690863 Max:True Min:True Avg:True Binary:False Combo:True\n",
      "2017-12-03 21:53:02.934574 4 MICRO: F1: 0.675737 Max:True Min:True Avg:True Binary:False Combo:False\n",
      "2017-12-03 21:53:21.054911 5 MICRO: F1: 0.691425 Max:True Min:True Avg:False Binary:True Combo:True\n",
      "2017-12-03 21:53:34.447422 6 MICRO: F1: 0.684661 Max:True Min:True Avg:False Binary:True Combo:False\n",
      "2017-12-03 21:53:49.842310 7 MICRO: F1: 0.690598 Max:True Min:True Avg:False Binary:False Combo:True\n",
      "2017-12-03 21:54:00.833707 8 MICRO: F1: 0.674115 Max:True Min:True Avg:False Binary:False Combo:False\n",
      "2017-12-03 21:54:17.367821 9 MICRO: F1: 0.691680 Max:True Min:False Avg:True Binary:True Combo:True\n",
      "2017-12-03 21:54:29.332734 10 MICRO: F1: 0.684993 Max:True Min:False Avg:True Binary:True Combo:False\n",
      "2017-12-03 21:54:43.425245 11 MICRO: F1: 0.690968 Max:True Min:False Avg:True Binary:False Combo:True\n",
      "2017-12-03 21:54:53.224844 12 MICRO: F1: 0.660352 Max:True Min:False Avg:True Binary:False Combo:False\n",
      "2017-12-03 21:55:07.149298 13 MICRO: F1: 0.690230 Max:True Min:False Avg:False Binary:True Combo:True\n",
      "2017-12-03 21:55:17.158379 14 MICRO: F1: 0.682563 Max:True Min:False Avg:False Binary:True Combo:False\n",
      "2017-12-03 21:55:28.881368 15 MICRO: F1: 0.690044 Max:True Min:False Avg:False Binary:False Combo:True\n",
      "2017-12-03 21:55:37.156528 16 MICRO: F1: 0.660081 Max:True Min:False Avg:False Binary:False Combo:False\n",
      "2017-12-03 21:55:53.875764 17 MICRO: F1: 0.691680 Max:False Min:True Avg:True Binary:True Combo:True\n",
      "2017-12-03 21:56:05.843949 18 MICRO: F1: 0.685120 Max:False Min:True Avg:True Binary:True Combo:False\n",
      "2017-12-03 21:56:19.903338 19 MICRO: F1: 0.690968 Max:False Min:True Avg:True Binary:False Combo:True\n",
      "2017-12-03 21:56:29.755280 20 MICRO: F1: 0.660352 Max:False Min:True Avg:True Binary:False Combo:False\n",
      "2017-12-03 21:56:43.689409 21 MICRO: F1: 0.690230 Max:False Min:True Avg:False Binary:True Combo:True\n",
      "2017-12-03 21:56:53.716800 22 MICRO: F1: 0.682563 Max:False Min:True Avg:False Binary:True Combo:False\n",
      "2017-12-03 21:57:05.497152 23 MICRO: F1: 0.690044 Max:False Min:True Avg:False Binary:False Combo:True\n",
      "2017-12-03 21:57:13.719482 24 MICRO: F1: 0.660081 Max:False Min:True Avg:False Binary:False Combo:False\n",
      "2017-12-03 21:57:26.450308 25 MICRO: F1: 0.692855 Max:False Min:False Avg:True Binary:True Combo:True\n",
      "2017-12-03 21:57:35.562056 26 MICRO: F1: 0.673042 Max:False Min:False Avg:True Binary:True Combo:False\n",
      "2017-12-03 21:57:46.191528 27 MICRO: F1: 0.688782 Max:False Min:False Avg:True Binary:False Combo:True\n",
      "2017-12-03 21:57:53.507357 28 MICRO: F1: 0.001987 Max:False Min:False Avg:True Binary:False Combo:False\n",
      "2017-12-03 21:58:04.438748 29 MICRO: F1: 0.693566 Max:False Min:False Avg:False Binary:True Combo:True\n",
      "2017-12-03 21:58:11.928596 30 MICRO: F1: 0.676082 Max:False Min:False Avg:False Binary:True Combo:False\n",
      "2017-12-03 21:58:21.234667 31 MICRO: F1: 0.689877 Max:False Min:False Avg:False Binary:False Combo:True\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for max_feats in [True, False]:\n",
    "    for min_feats in [True, False]:\n",
    "        for average_feats in [True, False]:\n",
    "            for binary_feats in [True,False]:\n",
    "                for combo_feats in [True,False]:\n",
    "                    if not any([max_feats, min_feats, average_feats, binary_feats, combo_feats]):\n",
    "                        continue\n",
    "                        \n",
    "                    counter +=1\n",
    "                    micro_f1 = train_stacked_classifier(dual=True, penalty='l2', C=1.0, \\\n",
    "                                                      max_feats=max_feats, min_feats=min_feats, average_feats=average_feats,\\\n",
    "                                                      binary_feats=binary_feats, combo_feats=combo_feats)\n",
    "                    print(\"{ts} {counter} MICRO: F1: {f1:.6f} Max:{max} Min:{min} Avg:{average} Binary:{binary} Combo:{combo}\".format(\\\n",
    "                        ts=get_ts(), counter=counter, f1=micro_f1, max=max_feats, min=min_feats, \\\n",
    "                        average=average_feats, binary=binary_feats, combo=combo_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best 2017-12-03 21:58:04.438748 29 MICRO: F1: 0.693566 Max:False Min:False Avg:False Binary:True Combo:True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Features are Binary and Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2857142857142857, 0.7142857142857143)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_feats = 105\n",
    "max_feats = 147\n",
    "\n",
    "(max_feats-optimal_feats)/max_feats, (optimal_feats)/max_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 MICRO: F1: 0.621378 dual: True penalty: l2 C:  0.1\n",
      "2 MICRO: F1:  0.68806 dual: True penalty: l2 C:  0.5\n",
      "3 MICRO: F1: 0.693566 dual: True penalty: l2 C:  1.0\n",
      "4 MICRO: F1: 0.690153 dual: True penalty: l2 C:  5.0\n",
      "5 MICRO: F1: 0.688191 dual: True penalty: l2 C: 10.0\n",
      "6 MICRO: F1: 0.647016 dual: True penalty: l2 C:100.0\n",
      "7 MICRO: F1: 0.633465 dual: False penalty: l1 C:  0.1\n",
      "8 MICRO: F1: 0.692996 dual: False penalty: l1 C:  0.5\n",
      "9 MICRO: F1: 0.694611 dual: False penalty: l1 C:  1.0\n",
      "10 MICRO: F1: 0.685934 dual: False penalty: l1 C:  5.0\n",
      "11 MICRO: F1: 0.682689 dual: False penalty: l1 C: 10.0\n",
      "12 MICRO: F1: 0.678819 dual: False penalty: l1 C:100.0\n",
      "13 MICRO: F1: 0.621378 dual: False penalty: l2 C:  0.1\n",
      "14 MICRO: F1:  0.68806 dual: False penalty: l2 C:  0.5\n",
      "15 MICRO: F1: 0.693566 dual: False penalty: l2 C:  1.0\n",
      "16 MICRO: F1: 0.690153 dual: False penalty: l2 C:  5.0\n",
      "17 MICRO: F1:  0.68853 dual: False penalty: l2 C: 10.0\n",
      "18 MICRO: F1: 0.681699 dual: False penalty: l2 C:100.0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for dual in [True, False]:\n",
    "    for penalty in [\"l1\", \"l2\"]:\n",
    "        # dual only support l2\n",
    "        if dual and penalty != \"l2\":\n",
    "            continue\n",
    "        for C in [0.1, 0.5, 1.0, 5.0, 10.0, 100.0]:\n",
    "            counter += 1\n",
    "            micro_f1 = train_stacked_classifier(dual=dual, penalty=penalty, C=C, \\\n",
    "                                                      max_feats=False, min_feats=False, average_feats=False,\\\n",
    "                                                      binary_feats=True, combo_feats=True)                \n",
    "            print(\"%i MICRO: F1: %s dual: %s penalty: %s C:%s\"\n",
    "                   % (counter, str(round(micro_f1, 6)).rjust(8), str(dual), str(penalty), str(round(C, 3)).rjust(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best - 9 MICRO: F1: 0.694611 dual: False penalty: l1 C:  1.0\n",
    "# need to adjust code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Test Metric Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8292, 141, 14), 8292)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = []\n",
    "td_ys = defaultdict(list)\n",
    "for i in range(CV_FOLDS):\n",
    "    tmp_xs = predicts_by_fold[i]\n",
    "    td.append(tmp_xs)\n",
    "\n",
    "    _, _, _, td_ys_by_tag_cr_sent, _ = fold2test_data[i]\n",
    "    merge_dictionaries(td_ys_by_tag_cr_sent, td_ys)\n",
    "\n",
    "xs = np.vstack(td)\n",
    "# ensure the same number of rows\n",
    "xs.shape, len(list(td_ys.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8292, 105)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_feats_train = get_stacked_feat_from_probs(xs, max_feats=False, min_feats=False, average_feats=False, binary_feats=True, combo_feats=True)\n",
    "xs_feats_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1918, 105)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_feats_test = get_stacked_feat_from_probs(test_probs, max_feats=False, min_feats=False, average_feats=False, binary_feats=True, combo_feats=True)\n",
    "xs_feats_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimal from earlier\n",
    "fn_create_sent_cls = lambda: LogisticRegression(dual=False, C=1.0, penalty='l1')\n",
    "tag2sent_classifier = train_classifier_per_code(xs_feats_train, td_ys , fn_create_sent_cls, cr_vtags, verbose=False)\n",
    "train_sent_predictions_by_code \\\n",
    "    = test_classifier_per_code(xs_feats_train, tag2sent_classifier, cr_vtags )\n",
    "\n",
    "test_sent_predictions_by_code \\\n",
    "    = test_classifier_per_code(xs_feats_test, tag2sent_classifier, cr_vtags )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CB_SENT_TD, CB_SENT_VD = \"TEST_CR_CB_STACKED_TD\", \"TEST_CR_CB_STACKED_VD\"\n",
    "parameters = dict(config)\n",
    "parameters[\"extractors\"] = []\n",
    "# merge in function args\n",
    "sent_algo = \"stacked\"\n",
    "parameters.update({'dual': True, 'C':0.5, 'penalty':'l2', \n",
    "                   'max_feats': True, 'min_feats': False, 'average_feats': True, \n",
    "                   'binary_feats': False, 'combo_feats': True})\n",
    "\n",
    "sent_td_objectid = processor.persist_results(CB_SENT_TD, td_ys,                  train_sent_predictions_by_code, parameters, sent_algo)\n",
    "sent_vd_objectid = processor.persist_results(CB_SENT_VD, test_ys_by_tag_cr_sent, test_sent_predictions_by_code,  parameters, sent_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raise Exception(\"Stop here, don't run rest of notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Train an RNN on Sequential Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cr_vtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xs_for_fold(fold_ix):\n",
    "    tmp_xs = predicts_by_fold[fold_ix]\n",
    "    _,_,_,_, seq_len_test  = fold2test_data[fold_ix]\n",
    "\n",
    "    xs = []\n",
    "    for i in range(len(seq_len_test)):\n",
    "        x = tmp_xs[i].copy()\n",
    "        seq_len = seq_len_test[i]\n",
    "        #mask zeros\n",
    "        x[:-seq_len,:] = 0.0\n",
    "        xs.append(x)\n",
    "    return np.asarray(xs)\n",
    "\n",
    "def get_seq_lens_for_fold(fold_ix):\n",
    "    _,_,_,_, seq_len_test  = fold2test_data[fold_ix]\n",
    "    return seq_len_test\n",
    "\n",
    "def get_ys_for_fold(fold_ix):\n",
    "    _, _, _, td_ys_by_tag_cr_sent, _ = fold2test_data[fold_ix]\n",
    "    num_classes = len(td_ys_by_tag_cr_sent.keys())\n",
    "    n_rows = len(td_ys_by_tag_cr_sent[list(td_ys_by_tag_cr_sent.keys())[0]])\n",
    "    ys = []\n",
    "    for i in range(n_rows):\n",
    "        y = np.zeros((num_classes))\n",
    "        for tag_ix in range(num_classes):            \n",
    "            tag = ix2crtag[tag_ix]\n",
    "            lbl = td_ys_by_tag_cr_sent[tag][i]\n",
    "            y[tag_ix] = lbl\n",
    "        ys.append(y)\n",
    "    return np.asarray(ys)\n",
    "\n",
    "def get_ys_for_fold(fold_ix):\n",
    "    _, _, _, td_ys_by_tag_cr_sent, _ = fold2test_data[fold_ix]\n",
    "    num_classes = len(td_ys_by_tag_cr_sent.keys())\n",
    "    n_rows = len(td_ys_by_tag_cr_sent[list(td_ys_by_tag_cr_sent.keys())[0]])\n",
    "    ys = []\n",
    "    for i in range(n_rows):\n",
    "        y = np.zeros((num_classes))\n",
    "        for tag_ix in range(num_classes):            \n",
    "            tag = ix2crtag[tag_ix]\n",
    "            lbl = td_ys_by_tag_cr_sent[tag][i]\n",
    "            y[tag_ix] = lbl\n",
    "        ys.append(y)\n",
    "    return np.asarray(ys)\n",
    "\n",
    "def split_dict(ys_by_tag, split):\n",
    "    n_rows = len(ys_by_tag[list(ys_by_tag.keys())[0]])\n",
    "    num_training = int((1.0 - split) * n_rows)\n",
    "    \n",
    "    a, b = defaultdict(list), defaultdict(list)\n",
    "    for tag, lst in ys_by_tag.items():\n",
    "        a[tag], b[tag] = lst[:num_training], lst[num_training:]\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(CV_FOLDS):\n",
    "    xs = get_xs_for_fold(0)\n",
    "    ys = get_ys_for_fold(0)\n",
    "    assert ys.shape[0] == len(xs)\n",
    "    # + 1 because of the empty tag\n",
    "    assert ys.shape[1] == len(cr_tags) + 1, \"%i,%i\" % (ys.shape[1] , len(cr_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dev_split_array(a, dev_split):\n",
    "    num_training = int((1.0 - dev_split) * a.shape[0])\n",
    "    return a[:num_training], a[num_training:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td_xs_by_fold, td_ys_by_fold, vd_xs_by_fold, vd_ys_by_fold = generate_stacked_features(False, False, False, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_td_xs_by_fold = {}\n",
    "seq_dev_xs_by_fold = {}\n",
    "seq_vd_xs_by_fold = {}\n",
    "\n",
    "seq_td_ys_by_fold = {}\n",
    "seq_dev_ys_by_fold = {}\n",
    "seq_vd_ys_by_fold = {}\n",
    "\n",
    "seq_len_td_by_fold = {}\n",
    "seq_len_dev_by_fold = {}\n",
    "seq_len_vd_by_fold = {}\n",
    "\n",
    "td_ys_by_tag_fold = {}\n",
    "dev_ys_by_tag_fold = {}\n",
    "vd_ys_by_tag_fold = {}\n",
    "\n",
    "for vd_ix in range(CV_FOLDS):\n",
    "    \n",
    "    td_xs = []\n",
    "    td_ys = []\n",
    "    vd_ys = []\n",
    "    td_seq_lens = []\n",
    "    for td_ix in range(CV_FOLDS):\n",
    "        if td_ix == vd_ix:\n",
    "            continue\n",
    "        \n",
    "        xs = get_xs_for_fold(td_ix)\n",
    "        td_xs.append(xs)\n",
    "        \n",
    "        ys = get_ys_for_fold(td_ix)\n",
    "        td_ys.append(ys)\n",
    "        \n",
    "        seq_lens = get_seq_lens_for_fold(td_ix)\n",
    "        td_seq_lens.extend(seq_lens)\n",
    "        #TODO - keep EMPTY?\n",
    "\n",
    "    td_xs = np.vstack(td_xs)\n",
    "    td_ys = np.vstack(td_ys)\n",
    "    \n",
    "    seq_td_xs_by_fold[vd_ix], seq_dev_xs_by_fold[vd_ix] = train_dev_split_array(td_xs, DEV_SPLIT)\n",
    "    seq_vd_xs_by_fold[vd_ix] = get_xs_for_fold(vd_ix)\n",
    "    \n",
    "    seq_td_ys_by_fold[vd_ix], seq_dev_ys_by_fold[vd_ix] = train_dev_split_array(td_ys, DEV_SPLIT)\n",
    "    seq_vd_ys_by_fold[vd_ix] = get_ys_for_fold(vd_ix)\n",
    "    \n",
    "    td_ys_by_tag_fold[vd_ix], dev_ys_by_tag_fold[vd_ix] = split_dict(td_ys_by_fold[vd_ix], DEV_SPLIT)\n",
    "    vd_ys_by_tag_fold[vd_ix] = vd_ys_by_fold[vd_ix]\n",
    "    \n",
    "    seq_len_td_by_fold[vd_ix], seq_len_dev_by_fold[vd_ix] = train_dev_split(td_seq_lens, DEV_SPLIT, randomize=False)\n",
    "    seq_len_vd_by_fold[vd_ix] = get_seq_lens_for_fold(vd_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge_mode is Bi-Directional only\n",
    "def evaluate_stacked_fold(fold_ix, bi_directional, num_rnns, merge_mode, hidden_size):\n",
    "\n",
    "    num_outputs = len(cr_vtags)    \n",
    "    input_shape=(maxlen, len(vtags))\n",
    "    \n",
    "    model = Sequential()\n",
    "    for i in range(num_rnns):\n",
    "        return_sequences = i < (num_rnns-1)\n",
    "        if i == 0:\n",
    "            if bi_directional:\n",
    "                rnn_layer = Bidirectional(GRU(hidden_size, return_sequences=return_sequences, consume_less=\"cpu\"), input_shape=input_shape, merge_mode=merge_mode)\n",
    "            else:\n",
    "                rnn_layer = GRU(hidden_size, input_shape=input_shape, return_sequences=return_sequences, consume_less=\"cpu\")\n",
    "        else: # no need for input_size            \n",
    "            if bi_directional:\n",
    "                rnn_layer = Bidirectional(GRU(hidden_size, return_sequences=return_sequences, consume_less=\"cpu\"), merge_mode=merge_mode)\n",
    "            else:\n",
    "                rnn_layer = GRU(hidden_size, return_sequences=return_sequences, consume_less=\"cpu\")\n",
    "        model.add(rnn_layer)\n",
    "\n",
    "    model.add(Dense(num_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # get training data, split into train and development sets\n",
    "    X_train, y_train = seq_td_xs_by_fold[fold_ix],  seq_td_ys_by_fold[fold_ix]    \n",
    "    X_dev, y_dev     = seq_dev_xs_by_fold[fold_ix], seq_dev_ys_by_fold[fold_ix] \n",
    "    X_test,  y_test  = seq_vd_xs_by_fold[fold_ix],  seq_vd_ys_by_fold[fold_ix]\n",
    "\n",
    "    #model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3, validation_split=0.0, verbose=0)\n",
    "    #return model\n",
    "    \n",
    "    # init loop vars\n",
    "    f1_scores = [-1]\n",
    "    num_since_best_score = 0\n",
    "    patience = 10\n",
    "    best_weights = None\n",
    "\n",
    "    for i in range(30):\n",
    "        print(\"{ts}: Epoch={epoch}\".format(ts=get_ts(), epoch=i))\n",
    "        epochs = 1 # epochs per training instance\n",
    "        results = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs, validation_split=0.0, verbose=0)\n",
    "        micro_metrics,_ = score_predictions_sent_level(model, X_dev, dev_ys_by_tag_fold[fold_ix], seq_len_dev_by_fold[fold_ix])\n",
    "\n",
    "        print(micro_metrics)\n",
    "        print()\n",
    "\n",
    "        f1_score = micro_metrics.f1_score\n",
    "        best_f1_score = max(f1_scores)\n",
    "        if f1_score <= best_f1_score:\n",
    "            num_since_best_score += 1\n",
    "        else: # score improved\n",
    "            num_since_best_score = 0\n",
    "            best_weights = model.get_weights()\n",
    "\n",
    "        f1_scores.append(f1_score)\n",
    "        if num_since_best_score >= patience:\n",
    "            #print(\"Too long since an improvement, stopping\")\n",
    "            break\n",
    "    \n",
    "    print(\"Fold[{ix}] - Best F1 Score={f1}\".format(ix=fold_ix, f1=best_f1_score))\n",
    "    \n",
    "    # load best weights\n",
    "    model.set_weights(best_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# works with a very small number of hidden nodes\n",
    "mdl = evaluate_stacked_fold(0, bi_directional=True, num_rnns=2, merge_mode=\"sum\", hidden_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
