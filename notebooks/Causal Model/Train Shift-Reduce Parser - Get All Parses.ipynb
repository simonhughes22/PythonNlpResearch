{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "cr_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cr_tags = set(cr_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_essay_level(\n",
    "        collection_prefix: str,\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        extractor_fn_names_lst: List[str],\n",
    "        cost_function_name: str,\n",
    "        beta: float,\n",
    "        ngrams: int,\n",
    "        stemmed: bool,\n",
    "        max_epochs: int,\n",
    "        down_sample_rate=1.0) -> float:\n",
    "\n",
    "    if down_sample_rate < 1.0:\n",
    "        new_folds = []  # type: List[Tuple[Any, Any]]\n",
    "        for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "            essays_TD = essays_TD[:int(down_sample_rate * len(essays_TD))]\n",
    "            essays_VD = essays_VD[:int(down_sample_rate * len(essays_VD))]\n",
    "            new_folds.append((essays_TD, essays_VD))\n",
    "        folds = new_folds  # type: List[Tuple[Any, Any]]\n",
    "\n",
    "    serial_results = [\n",
    "        model_train_predict_essay_level(essays_TD, essays_VD, extractor_fn_names_lst, cost_function_name, ngrams, stemmed, beta, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    parser_models = []\n",
    "    for (model, num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        parser_models.append(model)\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    return parser_models, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "            \n",
    "def get_label_data_essay_level(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_essay = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        add_labels(unique_cr_tags, ys_bytag_essay)\n",
    "    return ys_bytag_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_micro_metrics(df):\n",
    "    return df[df.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"recall\", \"precision\"]]\n",
    "\n",
    "def predict_essay_level(parser, essays):\n",
    "    pred_ys_by_sent = defaultdict(list)\n",
    "    for essay_ix, essay in enumerate(essays):\n",
    "        unq_pre_relations = set()\n",
    "        for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "            predicted_tags = essay.pred_tagged_sentences[sent_ix]\n",
    "            pred_relations = parser.predict_sentence(taggged_sentence, predicted_tags)\n",
    "            unq_pre_relations.update(pred_relations)\n",
    "        # Store predictions for evaluation\n",
    "        add_labels(unq_pre_relations, pred_ys_by_sent)\n",
    "    return pred_ys_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Set, List\n",
    "\n",
    "from StructuredLearning.SEARN.stack import Stack\n",
    "from oracle import Oracle\n",
    "from shift_reduce_helper import *\n",
    "from shift_reduce_parser import ShiftReduceParser\n",
    "\n",
    "from Classifiers.StructuredLearning.SEARN.searn_parser import SearnModelTemplateFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseActionResult(object):\n",
    "    def __init__(self, action, relations, prob, cause2effects, effect2causers, oracle, tag_ix, ctx, parent_action, lr_action_probs):\n",
    "        self.action = action\n",
    "        self.relations = relations\n",
    "        self.prob = prob\n",
    "        self.cause2effects = cause2effects\n",
    "        self.effect2causers = effect2causers\n",
    "        self.oracle = oracle\n",
    "        self.current_tag_ix = tag_ix # store for reference / debugging\n",
    "        self.tag_ix = tag_ix\n",
    "        self.ctx = ctx\n",
    "        self.parent_action = parent_action\n",
    "        self.lr_action_probs = lr_action_probs\n",
    "\n",
    "        self.probs = [self.prob]\n",
    "        if parent_action is not None:\n",
    "            self.probs = parent_action.probs + self.probs\n",
    "        self.cum_prob = np.product(self.probs)\n",
    "        self.__execute__()\n",
    "\n",
    "    def __execute__(self):\n",
    "        buffer_tag_pair = self.ctx.pos_ptag_seq[self.tag_ix]\n",
    "        if not self.oracle.execute(self.action, self.oracle.tos(), buffer_tag_pair) or self.oracle.is_stack_empty():\n",
    "            # increment tag_ix\n",
    "            self.tag_ix += 1\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.tag_ix >= len(self.ctx.pos_ptag_seq)\n",
    "\n",
    "class ParseContext(object):\n",
    "    def __init__(self, pos_ptag_seq, tag2span, tag2words, words):\n",
    "        self.pos_ptag_seq = pos_ptag_seq\n",
    "        self.tag2span = tag2span\n",
    "        self.tag2words = tag2words\n",
    "        self.words = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearnModelBreadthFirst(SearnModelTemplateFeatures):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SearnModelBreadthFirst, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def build_parse_context(self, tagged_sentence, predicted_tags):\n",
    "        pos_ptag_seq, _, tag2span, all_predicted_rtags, _ = self.get_tags_relations_for(\n",
    "            tagged_sentence, predicted_tags, self.cr_tags)\n",
    "\n",
    "        if len(all_predicted_rtags) == 0:\n",
    "            return None\n",
    "\n",
    "        # tags without positional info\n",
    "        rtag_seq = [t for t, i in pos_ptag_seq if t[0].isdigit()]\n",
    "        # if not at least 2 concept codes, then can't parse\n",
    "        if len(rtag_seq) < 2:\n",
    "            return None\n",
    "\n",
    "        words = [wd for wd, tags in tagged_sentence]\n",
    "\n",
    "        tag2words = defaultdict(list)\n",
    "        for ix, tag_pair in enumerate(pos_ptag_seq):\n",
    "            bstart, bstop = tag2span[tag_pair]\n",
    "            tag2words[tag_pair] = self.ngram_extractor.extract(words[bstart:bstop + 1])  # type: List[str]\n",
    "\n",
    "        ctx = ParseContext(pos_ptag_seq=pos_ptag_seq, tag2span=tag2span, tag2words=tag2words, words=words)\n",
    "        return ctx\n",
    "\n",
    "    def generate_all_potential_parses_for_sentence(self, tagged_sentence, predicted_tags, top_n):\n",
    "\n",
    "        ctx = self.build_parse_context(tagged_sentence, predicted_tags)\n",
    "        if not ctx:\n",
    "            return []\n",
    "\n",
    "        terminal_actions = []\n",
    "        actions_queue = [None]\n",
    "        while True:\n",
    "            current_actions_queue = list(actions_queue)\n",
    "            actions_queue = []\n",
    "            for act in current_actions_queue:\n",
    "                if act and act.is_terminal():\n",
    "                    terminal_actions.append(act)\n",
    "                actions_queue.extend(self.get_next_actions(act, ctx))\n",
    "\n",
    "            if len(actions_queue) == 0:\n",
    "                break\n",
    "            # trim to top_n\n",
    "            actions_queue = sorted(actions_queue,   key=lambda act: -act.cum_prob)[:top_n]\n",
    "\n",
    "        terminal_actions = sorted(terminal_actions, key=lambda act: -act.cum_prob)\n",
    "        return terminal_actions[:top_n]\n",
    "\n",
    "    def get_next_actions(self, parse_action, ctx):\n",
    "        next_actions = []\n",
    "        if parse_action is None:\n",
    "            # Initialize stack, basic parser and oracle\n",
    "            oracle = self.create_oracle()\n",
    "            tag_ix = 0\n",
    "            cause2effects, effect2causers = defaultdict(set), defaultdict(set)\n",
    "        else:\n",
    "            if parse_action.is_terminal():\n",
    "                return []\n",
    "            oracle = parse_action.oracle\n",
    "            tag_ix = parse_action.tag_ix\n",
    "            cause2effects, effect2causers = parse_action.cause2effects, parse_action.effect2causers\n",
    "\n",
    "            if tag_ix >= len(ctx.pos_ptag_seq):\n",
    "                return next_actions\n",
    "\n",
    "        return self.get_parse_action_results(cause2effects, effect2causers, oracle, tag_ix, ctx, parse_action)\n",
    "\n",
    "    def get_parse_action_results(self, cause2effects, effect2causers, oracle, tag_ix, ctx, parent_action):\n",
    "        buffer_tag_pair = ctx.pos_ptag_seq[tag_ix]\n",
    "        buffer_tag = buffer_tag_pair[0]\n",
    "        bstart, bstop = ctx.tag2span[buffer_tag_pair]\n",
    "        remaining_buffer_tags = ctx.pos_ptag_seq[tag_ix:]\n",
    "\n",
    "        tos_tag_pair = oracle.tos()\n",
    "        tos_tag = tos_tag_pair[0]\n",
    "        # Returns -1,-1 if TOS is ROOT\n",
    "        if tos_tag == ROOT:\n",
    "            tstart, tstop = -1, -1\n",
    "        else:\n",
    "            tstart, tstop = ctx.tag2span[tos_tag_pair]\n",
    "        # Note that the end ix in tag2span is always the last index, not the last + 1\n",
    "        btwn_start, btwn_stop = min(tstop + 1, len(ctx.words)), max(0, bstart)\n",
    "        btwn_word_seq = ctx.words[btwn_start:btwn_stop]\n",
    "        distance = len(btwn_word_seq)\n",
    "        btwn_word_ngrams = self.ngram_extractor.extract(btwn_word_seq)  # type: List[str]\n",
    "        feats = self.feat_extractor.extract(stack_tags=oracle.parser.stack.contents(), buffer_tags=remaining_buffer_tags,\n",
    "                                            tag2word_seq=ctx.tag2words,\n",
    "                                            between_word_seq=btwn_word_ngrams, distance=distance,\n",
    "                                            cause2effects=cause2effects, effect2causers=effect2causers,\n",
    "                                            positive_val=self.positive_val)\n",
    "\n",
    "        action_probabilities = self.predict_parse_action_probabilities(feats=feats,\n",
    "                                           tos=tos_tag,\n",
    "                                           models=self.parser_models[-1],\n",
    "                                           vectorizer=self.parser_feature_vectorizers[-1])\n",
    "\n",
    "        parse_action_results = []\n",
    "        for action, prob in action_probabilities.items():\n",
    "            # Decide the direction of the causal relation\n",
    "            new_relations = set()\n",
    "            new_cause2effects = self.clone_default_dict(cause2effects)\n",
    "            new_effect2causers = self.clone_default_dict(effect2causers)\n",
    "\n",
    "            lr_action_probs = dict()\n",
    "            if action in [LARC, RARC]:\n",
    "                feats_copy = dict(feats)  # don't modify feats as we iterate through possibilities\n",
    "                cause_effect, effect_cause = self.update_feats_with_action(action, buffer_tag, feats_copy, tos_tag)\n",
    "                lr_action_probs = self.predict_parse_action_probabilities(feats=feats_copy,\n",
    "                                                     model=self.crel_models[-1],\n",
    "                                                     vectorizer=self.crel_feat_vectorizers[-1])\n",
    "\n",
    "                lr_action = max(lr_action_probs.keys(), key = lambda k: lr_action_probs[k])\n",
    "                new_relations = self.update_cause_effects(buffer_tag_pair,\n",
    "                                                          new_cause2effects, cause_effect,\n",
    "                                                          new_effect2causers, effect_cause,\n",
    "                                                          lr_action, tos_tag_pair)\n",
    "\n",
    "            parse_action_result = ParseActionResult(\n",
    "                action, new_relations, prob, new_cause2effects, new_effect2causers, oracle.clone(), tag_ix, ctx, parent_action, lr_action_probs)\n",
    "            parse_action_results.append(parse_action_result)\n",
    "        return parse_action_results\n",
    "\n",
    "    def update_cause_effects(self, buffer_tag_pair, cause2effects, cause_effect, effect2causers, effect_cause,\n",
    "                             lr_action, tos_tag_pair):\n",
    "        new_relations = set()\n",
    "        if lr_action == CAUSE_AND_EFFECT:\n",
    "            new_relations.add(cause_effect)\n",
    "            new_relations.add(effect_cause)\n",
    "\n",
    "            cause2effects[tos_tag_pair].add(buffer_tag_pair)\n",
    "            effect2causers[buffer_tag_pair].add(tos_tag_pair)\n",
    "\n",
    "            cause2effects[buffer_tag_pair].add(tos_tag_pair)\n",
    "            effect2causers[tos_tag_pair].add(buffer_tag_pair)\n",
    "\n",
    "        elif lr_action == CAUSE_EFFECT:\n",
    "            new_relations.add(cause_effect)\n",
    "\n",
    "            cause2effects[tos_tag_pair].add(buffer_tag_pair)\n",
    "            effect2causers[buffer_tag_pair].add(tos_tag_pair)\n",
    "\n",
    "        elif lr_action == EFFECT_CAUSE:\n",
    "            new_relations.add(effect_cause)\n",
    "\n",
    "            cause2effects[buffer_tag_pair].add(tos_tag_pair)\n",
    "            effect2causers[tos_tag_pair].add(buffer_tag_pair)\n",
    "\n",
    "        elif lr_action == REJECT:\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Invalid CREL type\")\n",
    "        return new_relations\n",
    "\n",
    "    def clone_default_dict(self, d):\n",
    "        new_dd = defaultdict(d.default_factory)\n",
    "        new_dd.update(d)\n",
    "        return new_dd\n",
    "\n",
    "    def create_oracle(self):\n",
    "        parser = ShiftReduceParser(Stack(verbose=False))\n",
    "        parser.stack.push((ROOT, 0))\n",
    "        # needs to be a tuple\n",
    "        return Oracle([], parser)\n",
    "\n",
    "    def predict_parse_action_probabilities(self, feats, tos, models, vectorizer):\n",
    "\n",
    "        xs = vectorizer.transform(feats)\n",
    "        prob_by_label = {}\n",
    "        for action in self.randomize_actions():\n",
    "            if not allowed_action(action, tos):\n",
    "                continue\n",
    "\n",
    "            prob_by_label[action] = models[action].predict_proba(xs)[0][-1]\n",
    "        return prob_by_label\n",
    "\n",
    "    def update_feats_with_action(self, action, buffer_tag, feats, tos_tag):\n",
    "        c_e_pair = (tos_tag, buffer_tag)\n",
    "        # Convert to a string Causer:{l}->Result:{r}\n",
    "        cause_effect = denormalize_cr(c_e_pair)\n",
    "        e_c_pair = (buffer_tag, tos_tag)\n",
    "        # Convert to a string Causer:{l}->Result:{r}\n",
    "        effect_cause = denormalize_cr(e_c_pair)\n",
    "        # Add additional features\n",
    "        # needs to be before predict below\n",
    "        crel_feats = self.crel_features(action, tos_tag, buffer_tag)\n",
    "        feats.update(crel_feats)\n",
    "        return cause_effect, effect_cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_predict_essay_level(essays_TD, essays_VD, extractor_names, cost_function_name, ngrams, stemmed, beta, max_epochs):\n",
    "    extractors = get_functions_by_name(extractor_names, all_extractor_fns)\n",
    "    # get single cost function\n",
    "    cost_fn = get_functions_by_name([cost_function_name], all_cost_functions)[0]\n",
    "    assert cost_fn is not None, \"Cost function look up failed\"\n",
    "    # Ensure all extractors located\n",
    "    assert len(extractors) == len(extractor_names), \"number of extractor functions does not match the number of names\"\n",
    "\n",
    "    template_feature_extractor = NonLocalTemplateFeatureExtractor(extractors=extractors)\n",
    "    if stemmed:\n",
    "        ngram_extractor = NgramExtractorStemmed(max_ngram_len=ngrams)\n",
    "    else:\n",
    "        ngram_extractor = NgramExtractor(max_ngram_len=ngrams)\n",
    "    parse_model = SearnModelBreadthFirst(feature_extractor=template_feature_extractor,\n",
    "                                             cost_function=cost_fn,\n",
    "                                             min_feature_freq=MIN_FEAT_FREQ,\n",
    "                                             ngram_extractor=ngram_extractor, cr_tags=cr_tags,\n",
    "                                             base_learner_fact=BASE_LEARNER_FACT,\n",
    "                                             beta=beta,\n",
    "                                             # log_fn=lambda s: print(s))\n",
    "                                             log_fn=lambda s: None)\n",
    "\n",
    "    parse_model.train(essays_TD, max_epochs=max_epochs)\n",
    "\n",
    "    num_feats = template_feature_extractor.num_features()\n",
    "\n",
    "    sent_td_ys_bycode = get_label_data_essay_level(essays_TD)\n",
    "    sent_vd_ys_bycode = get_label_data_essay_level(essays_VD)\n",
    "\n",
    "    sent_td_pred_ys_bycode = predict_essay_level(parse_model, essays_TD)\n",
    "    sent_vd_pred_ys_bycode = predict_essay_level(parse_model, essays_VD)\n",
    "\n",
    "    return parse_model, num_feats, sent_td_ys_bycode, sent_vd_ys_bycode, sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_WIDTH = 80\n",
    "\n",
    "# other settings\n",
    "DOWN_SAMPLE_RATE = 1.0  # For faster smoke testing the algorithm\n",
    "BASE_LEARNER_FACT = None\n",
    "COLLECTION_PREFIX = \"CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_MOST_RECENT_CODE\"\n",
    "\n",
    "# some of the other extractors aren't functional if the system isn't able to do a basic parse\n",
    "# so the base extractors are the MVP for getting to a basic parser, then additional 'meta' parse\n",
    "# features from all_extractors can be included\n",
    "base_extractors = [\n",
    "    single_words,\n",
    "    word_pairs,\n",
    "    three_words,\n",
    "    between_word_features\n",
    "]\n",
    "\n",
    "all_extractor_fns = base_extractors + [\n",
    "    word_distance,\n",
    "    valency,\n",
    "    unigrams,\n",
    "    third_order,\n",
    "    label_set,\n",
    "    size_features\n",
    "]\n",
    "\n",
    "all_cost_functions = [\n",
    "    micro_f1_cost,\n",
    "    micro_f1_cost_squared,\n",
    "    micro_f1_cost_plusone,\n",
    "    micro_f1_cost_plusepsilon,\n",
    "    binary_cost,\n",
    "    inverse_micro_f1_cost,\n",
    "    uniform_cost\n",
    "]\n",
    "\n",
    "all_extractor_fn_names = get_function_names(all_extractor_fns)\n",
    "base_extractor_fn_names = get_function_names(base_extractors)\n",
    "all_cost_fn_names = get_function_names(all_cost_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that these are different for Skin Cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = 1\n",
    "stemmed = True\n",
    "cost_function_name = micro_f1_cost_plusepsilon.__name__\n",
    "dual = True\n",
    "fit_intercept = True\n",
    "beta = 0.5\n",
    "max_epochs = 2\n",
    "C = 0.5\n",
    "penalty = \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note these also differ for SC dataset\n",
    "BASE_LEARNER_FACT = lambda : LogisticRegression(dual=dual, C=C, penalty=penalty, fit_intercept=fit_intercept)\n",
    "best_extractor_names = ['single_words', 'between_word_features', 'label_set',\n",
    "                                    'three_words', 'third_order', 'unigrams'] # type: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds     = [(pred_tagged_essays_train, pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essay Level Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_essay_level = evaluate_model_essay_level(\n",
    "    collection_prefix=COLLECTION_PREFIX,\n",
    "    folds=test_folds,\n",
    "    extractor_fn_names_lst=best_extractor_names,\n",
    "    cost_function_name=cost_function_name,\n",
    "    ngrams=ngrams,\n",
    "    beta=beta,\n",
    "    stemmed=stemmed,\n",
    "    down_sample_rate=DOWN_SAMPLE_RATE,\n",
    "    max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.986136</td>\n",
       "      <td>0.785121</td>\n",
       "      <td>0.758759</td>\n",
       "      <td>0.81338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.986136  0.785121  0.758759    0.81338"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, \\\n",
    "    cv_sent_vd_ys_by_tag = result_test_essay_level\n",
    "    \n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.986094</td>\n",
       "      <td>0.748239</td>\n",
       "      <td>0.776965</td>\n",
       "      <td>0.721562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.986094  0.748239  0.776965   0.721562"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, \\\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = result_test_essay_level\n",
    "    \n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_possible_crels(predicted_tags):\n",
    "    if len(predicted_tags) < 2:\n",
    "        return set()\n",
    "    predicted_tags = sorted(predicted_tags)\n",
    "    pred_crels = set()\n",
    "    for a,b in combinations(predicted_tags, 2):\n",
    "        pred_crels.add(\"Causer:{a}->Result:{b}\".format(a=a, b=b))\n",
    "        pred_crels.add(\"Causer:{b}->Result:{a}\".format(a=a, b=b))\n",
    "    return pred_crels\n",
    "\n",
    "def to_canonical_parse(crels):\n",
    "    return tuple(sorted(crels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_parse_action_probabilities() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-287-5a19f415ac22>\u001b[0m in \u001b[0;36mgenerate_all_potential_parses_for_sentence\u001b[0;34m(self, tagged_sentence, predicted_tags, top_n)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mterminal_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mactions_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_queue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-287-5a19f415ac22>\u001b[0m in \u001b[0;36mget_next_actions\u001b[0;34m(self, parse_action, ctx)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parse_action_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause2effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffect2causers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_parse_action_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause2effects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffect2causers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moracle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-287-5a19f415ac22>\u001b[0m in \u001b[0;36mget_parse_action_results\u001b[0;34m(self, cause2effects, effect2causers, oracle, tag_ix, ctx, parent_action)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 lr_action_probs = self.predict_parse_action_probabilities(feats=feats_copy,\n\u001b[1;32m    112\u001b[0m                                                      \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrel_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                                                      vectorizer=self.crel_feat_vectorizers[-1])\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mlr_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_action_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr_action_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_parse_action_probabilities() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = models[0]\n",
    "parses = []\n",
    "sent2parse = dict()\n",
    "for eix, essay in enumerate(pred_tagged_essays_test):\n",
    "    for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "        predicted_tags = essay.pred_tagged_sentences[sent_ix]\n",
    "        unique_cr_tags = set()\n",
    "        for word, tags in taggged_sentence:\n",
    "            unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        unq_ptags = set([p for p in predicted_tags if p != EMPTY])\n",
    "        possible_rels = get_possible_crels(unq_ptags)\n",
    "        best_parse = to_canonical_parse(possible_rels.intersection(unique_cr_tags))\n",
    "        gold_parse = to_canonical_parse(unique_cr_tags)\n",
    "        if len(best_parse) > 0:\n",
    "            pred_parses = model.generate_all_potential_parses_for_sentence(\n",
    "                tagged_sentence=taggged_sentence, predicted_tags=predicted_tags, \n",
    "                top_n=500)\n",
    "            parses.append((eix, sent_ix, pred_parses, best_parse, gold_parse))\n",
    "            sent2parse[(eix,sent_ix)] = (pred_parses, best_parse, gold_parse)\n",
    "            #print(\"parses:\", len(pred_parses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-8cc5f9749a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcrels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0meix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_cr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold_cr\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mparses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_cr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_cr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def get_relations(parse):\n",
    "    crels = []\n",
    "    p = parse\n",
    "    while p:\n",
    "        if p.relations:\n",
    "            crels.append((p.prob, p.relations))\n",
    "        p = p.parent_action\n",
    "    return crels\n",
    "\n",
    "eix, sent_ix, p, best_cr, gold_cr =  parses[9]\n",
    "print(best_cr)\n",
    "print(gold_cr)\n",
    "for pix, parse in enumerate(p):    \n",
    "    relations = get_relations(parse)\n",
    "    if relations:\n",
    "        print(pix, parse.cum_prob, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_relations(parse):\n",
    "    crels = set()\n",
    "    p = parse\n",
    "    while p:\n",
    "        if p.relations:\n",
    "            crels.update(p.relations)\n",
    "        p = p.parent_action\n",
    "    return crels\n",
    "\n",
    "ranks = []\n",
    "top_probs = []\n",
    "for (eix,sent_ix), (pred_parses, best_parse, gold_parse) in sent2parse.items():  \n",
    "    top_rank = -1\n",
    "    top_rank_prob = -1\n",
    "    for rank, parse in enumerate(pred_parses):\n",
    "#         print(rank, parse.cum_prob, parse)\n",
    "        pred_parse = to_canonical_parse(get_unique_relations(parse))\n",
    "        if pred_parse == best_parse:\n",
    "            top_rank = rank\n",
    "            top_rank_prob = parse.cum_prob\n",
    "            break\n",
    "    ranks.append(top_rank)\n",
    "    top_probs.append(top_rank_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([r for r in ranks if r != -1]) / len(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % in top_n\n",
    "len([r for r in ranks if r != -1 and r < 500]) / len(ranks)\n",
    "\n",
    "np.mean([r for r in ranks if r != -1 and r < 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = [p for p in top_probs if p != -1]\n",
    "np.mean(tp), np.max(tp), np.min(tp), np.percentile(tp,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_incl = []\n",
    "has_all_rels = []\n",
    "missing = []\n",
    "for (eix,sent_ix), (pred_parses, best_parse, gold_parse) in sent2parse.items():  \n",
    "    top_rank = -1\n",
    "    set_best_parse = set(best_parse)\n",
    "    all_preds = set()\n",
    "    \n",
    "    for rank, parse in enumerate(pred_parses):\n",
    "        set_pred_parse = get_unique_relations(parse)\n",
    "        all_preds.update(set_pred_parse)\n",
    "        if len(set_best_parse - set_pred_parse) == 0:\n",
    "            top_rank = rank\n",
    "            break\n",
    "    ranks_incl.append(top_rank)\n",
    "    if len(set_best_parse - all_preds) == 0:\n",
    "        has_all_rels.append(1)\n",
    "    else:\n",
    "        has_all_rels.append(0)\n",
    "        missing.append((eix, sent_ix, pred_parses, all_preds, best_parse, gold_parse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([r for r in ranks_incl if r != -1]) / len(ranks_incl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(has_all_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eix, sent_ix, pred_parses, all_preds, best_parse, gold_parse = missing[0]\n",
    "e = pred_tagged_essays_test[eix]\n",
    "set(e.pred_tagged_sentences[sent_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_actions(parse):\n",
    "    actions = []\n",
    "    p = parse\n",
    "    while p:\n",
    "        actions.append((p.action, p.prob, p.relations, p.lr_action_probs))\n",
    "        p = p.parent_action\n",
    "    return actions[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pred_parses:\n",
    "    print(get_parse_actions(p))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
