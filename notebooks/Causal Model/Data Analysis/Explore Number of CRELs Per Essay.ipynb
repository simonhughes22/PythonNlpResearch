{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "cm_folder = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/\"\n",
    "src_path = os.path.join(cm_folder, \"src\")\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from structured_perceptron import StructuredPerceptron\n",
    "from Settings import Settings\n",
    "\n",
    "from window_based_tagger_config import get_config\n",
    "from crel_helper import get_cr_tags\n",
    "from crel_processing import essay_to_crels_cv\n",
    "from evaluation import evaluate_model_essay_level, get_micro_metrics, metrics_to_df\n",
    "from feature_normalization import min_max_normalize_feats\n",
    "from function_helpers import get_function_names\n",
    "from results_procesor import ResultsProcessor\n",
    "from train_parser import essay_to_crels, create_extractor_functions\n",
    "from cost_functions import micro_f1_cost_plusepsilon\n",
    "from train_reranker import train_model, train_instance, get_essays_for_data, evaluate_ranker\n",
    "from searn_parser_breadth_first import SearnModelBreadthFirst\n",
    "from causal_model_features import CausalModelType\n",
    "from feature_extraction import get_features_from_probabilities\n",
    "from results_procesor import ResultsProcessor\n",
    "from filter_features import filter_feats\n",
    "\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from evaluation import add_cr_labels\n",
    "\n",
    "from random import shuffle\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Global settings\n",
    "settings = Settings()\n",
    "CAUSAL_MODEL_TYPE = CausalModelType.CORAL_BLEACHING\n",
    "# CAUSAL_MODEL_TYPE = CausalModelType.SKIN_CANCER\n",
    "root = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/Re-Ranker Final Scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "902 226\n"
     ]
    }
   ],
   "source": [
    "# Global settings\n",
    "if CAUSAL_MODEL_TYPE == CausalModelType.CORAL_BLEACHING:\n",
    "    root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "    training_folder = root_folder + \"Training\" + \"/\"\n",
    "    test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "    crels_folder = root + \"/crels/CB\"\n",
    "    coref_root = root_folder + \"CoReference/\"\n",
    "    coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "    # first and second were with initial_weight set to 1.0\n",
    "    # thrid is with set to 0.001\n",
    "\n",
    "    config = get_config(training_folder)\n",
    "    \n",
    "    train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(train_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "    test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(test_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "else: # SC\n",
    "\n",
    "    root_folder = settings.data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "    training_folder = root_folder + \"Training\" + \"/\"\n",
    "    test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "    crels_folder = root + \"/crels/SC\"\n",
    "    coref_root = root_folder + \"CoReference/\"\n",
    "    coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "    MONGO_COLLECTION = \"SC_STR_PCPTRN_RE-RANKER_HYPER_PARAM_TD\"\n",
    "    MONGO_TEST_COLLECTION = \"TEST_SC_STR_PCPTRN_RE-RANKER_TD\"\n",
    "\n",
    "    config = get_config(training_folder)\n",
    "    \n",
    "    train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(train_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "    test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(test_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "print(len(pred_tagged_essays_train), len(pred_tagged_essays_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "\n",
    "set_cr_tags = set(cr_tags)\n",
    "print(len(cr_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = pred_tagged_essays_train + pred_tagged_essays_test\n",
    "name2essay = {}\n",
    "for essay in all_essays:\n",
    "    name2essay[essay.name] = essay\n",
    "\n",
    "name2crels = essay_to_crels(all_essays, set_cr_tags)\n",
    "assert len(name2crels) == len(all_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.568946796959826"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_lens = []\n",
    "for name, crels in name2crels.items():\n",
    "#     print(len(crels))\n",
    "    if crels:\n",
    "        c_lens.append(len(crels))\n",
    "np.mean(c_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/Re-Ranker Final Scripts/crels/CB'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crels_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rerank(top_n):\n",
    "    rr_fname = \"xs_rerank_\" + str(top_n) + \".dill\"\n",
    "    with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "        xs_rerank = dill.load(f)\n",
    "\n",
    "    rr_fname = \"xs_rerank_test\" + str(top_n) + \".dill\"\n",
    "    with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "        xs_test_rerank = dill.load(f)\n",
    "    return xs_rerank, xs_test_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_crels_counts():    \n",
    "    lens = []\n",
    "    # Dict[str,Set[str]]\n",
    "    for ename, crels in name2crels.items():\n",
    "        lens.append(len(crels))\n",
    "    return lens\n",
    "\n",
    "def get_predicted_crels_counts(top_n):\n",
    "    # Td, Vd\n",
    "    a,b = load_rerank(top_n=top_n)\n",
    "    a.update(b)\n",
    "    \n",
    "    lens = []\n",
    "    # Dict[str,Dict[str,List[float]]]\n",
    "    for ename, dct in a.items():\n",
    "        lens.append(len(dct.keys()))\n",
    "    return lens\n",
    "\n",
    "topn2lens = {}\n",
    "for topn in [1,2,3,5,7,10]:\n",
    "    topn2lens[topn] = get_predicted_crels_counts(topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c c c }\n",
      "  \\toprule\n",
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean }  \\\\\n",
      "\\midrule\n",
      "Human Labels & 15 & 2.914 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>15</td>\n",
       "      <td>2.914007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Beam Size  Max.      Mean\n",
       "0         -    15  2.914007"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP = 4 # decimal points\n",
    "cols = [\"Beam Size\", \"Max.\", \"Mean\",         \n",
    "        #\"25%\",\n",
    "#         \"50%\", \"75%\", \n",
    "#         \"90%\", \n",
    "#         \"95%\", \n",
    "#         \"99%\"\n",
    "       ]\n",
    "\n",
    "def list2stats(l, n):\n",
    "    d = {\n",
    "        \"Beam Size\": n,\n",
    "        \"Mean\": np.mean(l),\n",
    "        \"Max.\": np.max(l),\n",
    "        \"Min.\": np.min(l),\n",
    "        \"25%\": np.percentile(l,25),\n",
    "        \"50%\": np.percentile(l,50),\n",
    "        \"75%\": np.percentile(l,75),\n",
    "        \"90%\": np.percentile(l,90),\n",
    "        \"95%\": np.percentile(l,95),\n",
    "        \"98%\": np.percentile(l,98),\n",
    "        \"99%\": np.percentile(l,99),\n",
    "    }\n",
    "    return d\n",
    "        \n",
    "def df2latex_table(df, columns=None, dp=3):\n",
    "    # print header\n",
    "    if columns is None:\n",
    "        columns = list(df.columns)\n",
    "    s_col_def = \"\\\\begin{tabular}{\" + (\"c \" * len(columns)) + \"}\\n\"\n",
    "    s_col_def += \"  \\\\toprule\\n\"\n",
    "    s = s_col_def\n",
    "    for col in columns:\n",
    "        s += \"\\\\textbf{{ {col} }} & \".format(col=col.replace(\"%\", \"\\\\%\"))\n",
    "    print(s[:-2] + \" \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    del s\n",
    "    \n",
    "    # print rows\n",
    "    for i,row in df.iterrows():\n",
    "        s = \"\"\n",
    "        for col in columns:\n",
    "            oval = row[col]\n",
    "            if type(oval) == str or oval is None:\n",
    "                val = \"Human Labels\"\n",
    "            else:\n",
    "                v = round(oval,dp)\n",
    "                ival = int(oval)\n",
    "                if v == ival:\n",
    "                    val = str(int(v))\n",
    "                else:\n",
    "                    fmt = \"{val:.\" + str(dp) + \"f}\"\n",
    "                    val = fmt.format(val=oval)\n",
    "            s += val + \" & \"\n",
    "        print(s[:-2].strip() + \" \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}}\")\n",
    "        \n",
    "l = get_actual_crels_counts()\n",
    "df = pd.DataFrame([list2stats(l, \"-\")])[cols]\n",
    "\n",
    "df2latex_table(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c c c }\n",
      "  \\toprule\n",
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean }  \\\\\n",
      "\\midrule\n",
      "Human Labels & 15 & 2.9140 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>15</td>\n",
       "      <td>2.914007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Beam Size  Max.      Mean\n",
       "0         -    15  2.914007"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = get_actual_crels_counts()\n",
    "df = pd.DataFrame([list2stats(l, \"-\")])[cols]\n",
    "\n",
    "df2latex_table(df, dp=DP)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c c c }\n",
      "  \\toprule\n",
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean }  \\\\\n",
      "\\midrule\n",
      "1 & 13 & 2.958 \\\\\n",
      "2 & 13 & 2.966 \\\\\n",
      "3 & 15 & 3.643 \\\\\n",
      "5 & 22 & 5.353 \\\\\n",
      "7 & 22 & 5.358 \\\\\n",
      "10 & 22 & 5.358 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2.966312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3.642730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5.352837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5.358156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5.358156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Beam Size  Max.      Mean\n",
       "0          1    13  2.958333\n",
       "1          2    13  2.966312\n",
       "2          3    15  3.642730\n",
       "3          5    22  5.352837\n",
       "4          7    22  5.358156\n",
       "5         10    22  5.358156"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts = []\n",
    "for n, l in topn2lens.items():    \n",
    "    dicts.append(list2stats(l, n))\n",
    "df_stats = pd.DataFrame(dicts)[cols]\n",
    "\n",
    "df2latex_table(df_stats)\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2latex_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences by Top N?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 3 4\n",
      "503 5 6\n",
      "551 2 3\n",
      "585 12 13\n",
      "806 5 6\n",
      "996 7 8\n",
      "..........\n"
     ]
    }
   ],
   "source": [
    "# Are the counts different?\n",
    "# CB: 5 and 10, yes, marginally. 7 and 10 - Nope\n",
    "# SC: 1-3 and 5 diff, 5, 7 and 10 are the same\n",
    "\n",
    "# for i, (a,b) in enumerate(zip(topn2lens[3],topn2lens[5])):\n",
    "#     if a != b:\n",
    "#         print(i,a,b)\n",
    "\n",
    "for i, (a,b) in enumerate(zip(topn2lens[5],topn2lens[7])):\n",
    "    if a != b:\n",
    "        print(i,a,b)\n",
    "print(\".\" * 10)\n",
    "for i, (a,b) in enumerate(zip(topn2lens[7],topn2lens[10])):\n",
    "    if a != b:\n",
    "        print(i,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(serverSelectionTimeoutMS=100, host=\"127.0.0.1\")\n",
    "db = client.metrics_causal_model_reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def query_params_for(db, collection):\n",
    "    project = {\n",
    "            \"params\": \"$parameters\",\n",
    "            \"micro_f1\": \"$MICRO_F1\",\n",
    "            \"asof\": \"$asof\",\n",
    "            \"_id\": 1\n",
    "        }\n",
    "    feats_pipeline = [{ \"$project\": project }]\n",
    "    return [row for row in db[collection].aggregate(feats_pipeline)]    \n",
    "\n",
    "def get_df(collection):\n",
    "    rows = query_params_for(db=client.metrics_causal_model_reranker, collection=collection)   \n",
    "    if len(rows) == 0:\n",
    "        return pd.DataFrame([])\n",
    "\n",
    "    results = []\n",
    "    for r in rows:\n",
    "        d = dict(r[\"params\"])\n",
    "        d.update(r[\"micro_f1\"])\n",
    "        d[\"asof\"] = str(r[\"asof\"])\n",
    "        results.append(d)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df[\"extractors\"] = df[\"extractors\"].apply(lambda l: \",\".join(l))\n",
    "    df = df.sort_values(by=\"f1_score\",ascending=False)\n",
    "    cols = [\"f1_score\", \"precision\", \"recall\", \"asof\",\"best_top_n\"]\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CoralBleaching'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAUSAL_MODEL_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>asof</th>\n",
       "      <th>best_top_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.741353</td>\n",
       "      <td>0.782873</td>\n",
       "      <td>0.704015</td>\n",
       "      <td>2019-06-24 20:43:02.777000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740970</td>\n",
       "      <td>0.787033</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2019-06-24 19:41:12.771000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.740081</td>\n",
       "      <td>0.787804</td>\n",
       "      <td>0.697810</td>\n",
       "      <td>2019-06-24 20:40:56.262000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.740054</td>\n",
       "      <td>0.785890</td>\n",
       "      <td>0.699270</td>\n",
       "      <td>2019-06-24 19:42:20.087000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.739954</td>\n",
       "      <td>0.786125</td>\n",
       "      <td>0.698905</td>\n",
       "      <td>2019-06-24 19:43:32.740000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1_score  precision    recall                        asof  best_top_n\n",
       "29  0.741353   0.782873  0.704015  2019-06-24 20:43:02.777000           1\n",
       "4   0.740970   0.787033  0.700000  2019-06-24 19:41:12.771000           2\n",
       "27  0.740081   0.787804  0.697810  2019-06-24 20:40:56.262000           1\n",
       "5   0.740054   0.785890  0.699270  2019-06-24 19:42:20.087000           2\n",
       "6   0.739954   0.786125  0.698905  2019-06-24 19:43:32.740000           2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if CAUSAL_MODEL_TYPE == CausalModelType.CORAL_BLEACHING:\n",
    "    collection = \"CB_STR_PCPTRN_RE-RANKER_HYPER_PARAM_VD\"\n",
    "elif CAUSAL_MODEL_TYPE == CausalModelType.SKIN_CANCER:\n",
    "    collection = \"SC_STR_PCPTRN_RE-RANKER_HYPER_PARAM_VD_3\"\n",
    "else:\n",
    "    raise Exception()\n",
    "    \n",
    "df = get_df(collection).copy(deep=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>Beam Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.741353</td>\n",
       "      <td>0.704015</td>\n",
       "      <td>0.782873</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740970</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.787033</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.739610</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.786272</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.739231</td>\n",
       "      <td>0.701460</td>\n",
       "      <td>0.781301</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.739139</td>\n",
       "      <td>0.698540</td>\n",
       "      <td>0.784748</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.738284</td>\n",
       "      <td>0.698540</td>\n",
       "      <td>0.782822</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    f1_score    recall  precision  Beam Size\n",
       "29  0.741353  0.704015   0.782873          1\n",
       "4   0.740970  0.700000   0.787033          2\n",
       "11  0.739610  0.698175   0.786272          3\n",
       "18  0.739231  0.701460   0.781301          5\n",
       "37  0.739139  0.698540   0.784748          7\n",
       "46  0.738284  0.698540   0.782822         10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_rows = []\n",
    "for n in sorted(df.best_top_n.unique()):\n",
    "    sub = df[df.best_top_n == n].head(1).copy(deep=True)\n",
    "    top_rows.append(sub)\n",
    "\n",
    "df_by_bs = pd.concat(top_rows)[[\"best_top_n\",\"f1_score\", \"recall\", \"precision\"]]\n",
    "df_by_bs[\"Beam Size\"] = df_by_bs[\"best_top_n\"]\n",
    "del df_by_bs[\"best_top_n\"]\n",
    "df_by_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{c c c c c c }\n",
      "  \\toprule\n",
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean } & \\textbf{ f1_score } & \\textbf{ recall } & \\textbf{ precision }  \\\\\n",
      "\\midrule\n",
      "1 & 13 & 2.9583 & 0.7414 & 0.7040 & 0.7829 \\\\\n",
      "2 & 13 & 2.9663 & 0.7410 & 0.7000 & 0.7870 \\\\\n",
      "3 & 15 & 3.6427 & 0.7396 & 0.6982 & 0.7863 \\\\\n",
      "5 & 22 & 5.3528 & 0.7392 & 0.7015 & 0.7813 \\\\\n",
      "7 & 22 & 5.3582 & 0.7391 & 0.6985 & 0.7847 \\\\\n",
      "10 & 22 & 5.3582 & 0.7383 & 0.6985 & 0.7828 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2.958333</td>\n",
       "      <td>0.741353</td>\n",
       "      <td>0.704015</td>\n",
       "      <td>0.782873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2.966312</td>\n",
       "      <td>0.740970</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.787033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3.642730</td>\n",
       "      <td>0.739610</td>\n",
       "      <td>0.698175</td>\n",
       "      <td>0.786272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5.352837</td>\n",
       "      <td>0.739231</td>\n",
       "      <td>0.701460</td>\n",
       "      <td>0.781301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>5.358156</td>\n",
       "      <td>0.739139</td>\n",
       "      <td>0.698540</td>\n",
       "      <td>0.784748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>5.358156</td>\n",
       "      <td>0.738284</td>\n",
       "      <td>0.698540</td>\n",
       "      <td>0.782822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Beam Size  Max.      Mean  f1_score    recall  precision\n",
       "0          1    13  2.958333  0.741353  0.704015   0.782873\n",
       "1          2    13  2.966312  0.740970  0.700000   0.787033\n",
       "2          3    15  3.642730  0.739610  0.698175   0.786272\n",
       "3          5    22  5.352837  0.739231  0.701460   0.781301\n",
       "4          7    22  5.358156  0.739139  0.698540   0.784748\n",
       "5         10    22  5.358156  0.738284  0.698540   0.782822"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.merge(df_stats, df_by_bs, on = \"Beam Size\")\n",
    "df2latex_table(merged, dp=4)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
