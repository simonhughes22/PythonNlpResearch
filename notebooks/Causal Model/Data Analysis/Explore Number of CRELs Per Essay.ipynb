{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "cm_folder = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/\"\n",
    "src_path = os.path.join(cm_folder, \"src\")\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from structured_perceptron import StructuredPerceptron\n",
    "from Settings import Settings\n",
    "\n",
    "from window_based_tagger_config import get_config\n",
    "from crel_helper import get_cr_tags\n",
    "from crel_processing import essay_to_crels_cv\n",
    "from evaluation import evaluate_model_essay_level, get_micro_metrics, metrics_to_df\n",
    "from feature_normalization import min_max_normalize_feats\n",
    "from function_helpers import get_function_names\n",
    "from results_procesor import ResultsProcessor\n",
    "from train_parser import essay_to_crels, create_extractor_functions\n",
    "from cost_functions import micro_f1_cost_plusepsilon\n",
    "from train_reranker import train_model, train_instance, get_essays_for_data, evaluate_ranker\n",
    "from searn_parser_breadth_first import SearnModelBreadthFirst\n",
    "from causal_model_features import CausalModelType\n",
    "from feature_extraction import get_features_from_probabilities\n",
    "from results_procesor import ResultsProcessor\n",
    "from filter_features import filter_feats\n",
    "\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from evaluation import add_cr_labels\n",
    "\n",
    "from random import shuffle\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/SXH1M01/GitHub/NlpResearch/\n",
      "Public Data: /Users/SXH1M01/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Global settings\n",
    "settings = Settings()\n",
    "# CAUSAL_MODEL_TYPE = CausalModelType.CORAL_BLEACHING\n",
    "CAUSAL_MODEL_TYPE = CausalModelType.SKIN_CANCER\n",
    "root = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/Re-Ranker Final Scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 218\n"
     ]
    }
   ],
   "source": [
    "# Global settings\n",
    "if CAUSAL_MODEL_TYPE == CausalModelType.CORAL_BLEACHING:\n",
    "    root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "    training_folder = root_folder + \"Training\" + \"/\"\n",
    "    test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "    crels_folder = root + \"/crels/CB\"\n",
    "    coref_root = root_folder + \"CoReference/\"\n",
    "    coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "    # first and second were with initial_weight set to 1.0\n",
    "    # thrid is with set to 0.001\n",
    "\n",
    "    config = get_config(training_folder)\n",
    "    \n",
    "    train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(train_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "    test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(test_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "else: # SC\n",
    "\n",
    "    root_folder = settings.data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "    training_folder = root_folder + \"Training\" + \"/\"\n",
    "    test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "    crels_folder = root + \"/crels/SC\"\n",
    "    coref_root = root_folder + \"CoReference/\"\n",
    "    coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "    MONGO_COLLECTION = \"SC_STR_PCPTRN_RE-RANKER_HYPER_PARAM_TD\"\n",
    "    MONGO_TEST_COLLECTION = \"TEST_SC_STR_PCPTRN_RE-RANKER_TD\"\n",
    "\n",
    "    config = get_config(training_folder)\n",
    "    \n",
    "    train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(train_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "    test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "    with open(test_fname, \"rb\") as f:\n",
    "        pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "print(len(pred_tagged_essays_train), len(pred_tagged_essays_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "\n",
    "set_cr_tags = set(cr_tags)\n",
    "print(len(cr_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = pred_tagged_essays_train + pred_tagged_essays_test\n",
    "name2essay = {}\n",
    "for essay in all_essays:\n",
    "    name2essay[essay.name] = essay\n",
    "\n",
    "name2crels = essay_to_crels(all_essays, set_cr_tags)\n",
    "assert len(name2crels) == len(all_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.99794026776519"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_lens = []\n",
    "for name, crels in name2crels.items():\n",
    "#     print(len(crels))\n",
    "    if crels:\n",
    "        c_lens.append(len(crels))\n",
    "np.mean(c_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/Re-Ranker Final Scripts/crels/SC'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crels_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rerank(top_n):\n",
    "    rr_fname = \"xs_rerank_\" + str(top_n) + \".dill\"\n",
    "    with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "        xs_rerank = dill.load(f)s\n",
    "\n",
    "    rr_fname = \"xs_rerank_test\" + str(top_n) + \".dill\"\n",
    "    with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "        xs_test_rerank = dill.load(f)\n",
    "    return xs_rerank, xs_test_rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_crels_counts():    \n",
    "    lens = []\n",
    "    # Dict[str,Set[str]]\n",
    "    for ename, crels in name2crels.items():\n",
    "        lens.append(len(crels))\n",
    "    return lens\n",
    "\n",
    "def get_predicted_crels_counts(top_n):\n",
    "    # Td, Vd\n",
    "    a,b = load_rerank(top_n=top_n)\n",
    "    a.update(b)\n",
    "    \n",
    "    lens = []\n",
    "    # Dict[str,Dict[str,List[float]]]\n",
    "    for ename, dct in a.items():\n",
    "        lens.append(len(dct.keys()))\n",
    "    return lens\n",
    "\n",
    "topn2lens = {}\n",
    "for topn in [1,2,3,5,7,10]:\n",
    "    topn2lens[topn] = get_predicted_crels_counts(topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean } & \\textbf{ 50\\% } & \\textbf{ 75\\% } & \\textbf{ 90\\% } & \\textbf{ 95\\% } & \\textbf{ 99\\% }  \\\\\n",
      "\\midrule\n",
      "Human Labels & 15 & 4.460 & 4 & 6 & 8 & 10 & 12 \\\\\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>15</td>\n",
       "      <td>4.460478</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Beam Size  Max.      Mean  50%  75%  90%   95%   99%\n",
       "0         -    15  4.460478  4.0  6.0  8.0  10.0  12.0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP = 3 # decimal points\n",
    "cols = [\"Beam Size\", \"Max.\", \"Mean\",         \n",
    "        #\"25%\",\n",
    "        \"50%\", \"75%\", \n",
    "        \"90%\", \n",
    "        \"95%\", \n",
    "        #\"99%\"\n",
    "       ]\n",
    "\n",
    "def list2stats(l, n):\n",
    "    d = {\n",
    "        \"Beam Size\": n,\n",
    "        \"Mean\": np.mean(l),\n",
    "        \"Max.\": np.max(l),\n",
    "        \"Min.\": np.min(l),\n",
    "        \"25%\": np.percentile(l,25),\n",
    "        \"50%\": np.percentile(l,50),\n",
    "        \"75%\": np.percentile(l,75),\n",
    "        \"90%\": np.percentile(l,90),\n",
    "        \"95%\": np.percentile(l,95),\n",
    "        \"98%\": np.percentile(l,98),\n",
    "        \"99%\": np.percentile(l,99),\n",
    "    }\n",
    "    return d\n",
    "\n",
    "def df2latex_table(df):\n",
    "    # print header\n",
    "    s = \"\"\n",
    "    for col in cols:\n",
    "        s += \"\\\\textbf{{ {col} }} & \".format(col=col.replace(\"%\", \"\\\\%\"))\n",
    "    print(s[:-2] + \" \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    del s\n",
    "    \n",
    "    # print rows\n",
    "    for i,row in df.iterrows():\n",
    "        s = \"\"\n",
    "        for col in cols:\n",
    "            oval = row[col]\n",
    "            if type(oval) == str or oval is None:\n",
    "                val = \"Human Labels\"\n",
    "            else:\n",
    "                v = round(oval,DP)\n",
    "                ival = int(oval)\n",
    "                if v == ival:\n",
    "                    val = str(int(v))\n",
    "                else:\n",
    "                    fmt = \"{val:.\" + str(DP) + \"f}\"\n",
    "                    val = fmt.format(val=oval)\n",
    "            s += val + \" & \"\n",
    "        print(s[:-2].strip() + \" \\\\\\\\\")\n",
    "    print()\n",
    "        \n",
    "l = get_actual_crels_counts()\n",
    "df = pd.DataFrame([list2stats(l, \"-\")])[cols]\n",
    "\n",
    "df2latex_table(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Crels (CB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean } & \\textbf{ 50\\% } & \\textbf{ 75\\% } & \\textbf{ 90\\% } & \\textbf{ 95\\% } & \\textbf{ 99\\% }  \\\\\n",
      "\\midrule\n",
      "Human Labels & 15 & 4.460 & 4 & 6 & 8 & 10 & 12 \\\\\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>15</td>\n",
       "      <td>4.460478</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Beam Size  Max.      Mean  50%  75%  90%   95%   99%\n",
       "0         -    15  4.460478  4.0  6.0  8.0  10.0  12.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = get_actual_crels_counts()\n",
    "df = pd.DataFrame([list2stats(l, \"-\")])[cols]\n",
    "\n",
    "df2latex_table(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Crels (CB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textbf{ Beam Size } & \\textbf{ Max. } & \\textbf{ Mean } & \\textbf{ 50\\% } & \\textbf{ 75\\% } & \\textbf{ 90\\% } & \\textbf{ 95\\% } & \\textbf{ 99\\% }  \\\\\n",
      "\\midrule\n",
      "1 & 13 & 3.974 & 4 & 6 & 8 & 9 & 11 \\\\\n",
      "2 & 13 & 3.975 & 4 & 6 & 8 & 9 & 11 \\\\\n",
      "3 & 15 & 4.811 & 5 & 7 & 9 & 11 & 13 \\\\\n",
      "5 & 25 & 7.747 & 7 & 11 & 15 & 17 & 20 \\\\\n",
      "7 & 25 & 7.747 & 7 & 11 & 15 & 17 & 20 \\\\\n",
      "10 & 25 & 7.747 & 7 & 11 & 15 & 17 & 20 \\\\\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beam Size</th>\n",
       "      <th>Max.</th>\n",
       "      <th>Mean</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>90%</th>\n",
       "      <th>95%</th>\n",
       "      <th>99%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3.974265</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3.975184</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>4.810662</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>7.747243</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>7.747243</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>7.747243</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Beam Size  Max.      Mean  50%   75%   90%   95%   99%\n",
       "0          1    13  3.974265  4.0   6.0   8.0   9.0  11.0\n",
       "1          2    13  3.975184  4.0   6.0   8.0   9.0  11.0\n",
       "2          3    15  4.810662  5.0   7.0   9.0  11.0  13.0\n",
       "3          5    25  7.747243  7.0  11.0  15.0  17.0  20.0\n",
       "4          7    25  7.747243  7.0  11.0  15.0  17.0  20.0\n",
       "5         10    25  7.747243  7.0  11.0  15.0  17.0  20.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts = []\n",
    "for n, l in topn2lens.items():    \n",
    "    dicts.append(list2stats(l, n))\n",
    "df = pd.DataFrame(dicts)[cols]\n",
    "\n",
    "df2latex_table(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2latex_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences by Top N?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n"
     ]
    }
   ],
   "source": [
    "# Are the counts different?\n",
    "# CB: 5 and 10, yes, marginally. 7 and 10 - Nope\n",
    "# SC: 1-3 and 5 diff, 5, 7 and 10 are the same\n",
    "\n",
    "# for i, (a,b) in enumerate(zip(topn2lens[3],topn2lens[5])):\n",
    "#     if a != b:\n",
    "#         print(i,a,b)\n",
    "\n",
    "for i, (a,b) in enumerate(zip(topn2lens[5],topn2lens[7])):\n",
    "    if a != b:\n",
    "        print(i,a,b)\n",
    "print(\".\" * 10)\n",
    "for i, (a,b) in enumerate(zip(topn2lens[7],topn2lens[10])):\n",
    "    if a != b:\n",
    "        print(i,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36] *",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
