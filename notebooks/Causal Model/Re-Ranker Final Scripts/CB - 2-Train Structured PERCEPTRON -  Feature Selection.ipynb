{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "cm_folder = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/\"\n",
    "src_path = os.path.join(cm_folder, \"src\")\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from structured_perceptron import StructuredPerceptron\n",
    "from Settings import Settings\n",
    "\n",
    "from window_based_tagger_config import get_config\n",
    "from crel_helper import get_cr_tags\n",
    "from crel_processing import essay_to_crels_cv\n",
    "from evaluation import evaluate_model_essay_level, get_micro_metrics, metrics_to_df\n",
    "from feature_normalization import min_max_normalize_feats\n",
    "from function_helpers import get_function_names\n",
    "from results_procesor import ResultsProcessor\n",
    "from train_parser import essay_to_crels, create_extractor_functions\n",
    "from cost_functions import micro_f1_cost_plusepsilon\n",
    "from train_reranker import train_model, train_instance, get_essays_for_data, evaluate_ranker\n",
    "from searn_parser_breadth_first import SearnModelBreadthFirst\n",
    "from causal_model_features import CausalModelType\n",
    "from feature_extraction import get_features_from_probabilities\n",
    "from results_procesor import ResultsProcessor\n",
    "from filter_features import filter_feats\n",
    "\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from evaluation import add_cr_labels\n",
    "\n",
    "from random import shuffle\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_fold(xs_train, xs_test, name2essay, learning_rate, max_update_items, set_cr_tags,\\\n",
    "                     initial_weight, max_epochs, early_stop_iters):\n",
    "\n",
    "    mdl = StructuredPerceptron(\n",
    "        learning_rate=learning_rate, max_update_items=max_update_items, initial_weight=initial_weight)\n",
    "\n",
    "    return train_model(mdl, xs_train=xs_train, xs_test=xs_test, name2essay=name2essay,\n",
    "            max_epochs=max_epochs, early_stop_iters=early_stop_iters, set_cr_tags=set_cr_tags,\n",
    "            train_instance_fn=train_instance,\n",
    "            verbose=False, return_metrics=True, early_stopping=False)\n",
    "\n",
    "def train_model_parallel(cv_folds, name2essay, learning_rate, max_update_items, set_cr_tags, \\\n",
    "                         initial_weight, max_epochs=5, early_stop_iters=5, n_jobs=None):\n",
    "\n",
    "    if n_jobs == None:\n",
    "        n_jobs = len(cv_folds)\n",
    "    try:\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(train_model_fold)(train, test, name2essay, learning_rate, max_update_items, set_cr_tags, \\\n",
    "                                      initial_weight, max_epochs, early_stop_iters)\n",
    "            for (train, test) in cv_folds)\n",
    "\n",
    "        f1s = []\n",
    "        for tpl in results:\n",
    "            best_test_f1, best_iterations, train_ys_bytag, train_pred_ys_bytag, test_ys_bytag, test_pred_ys_bytag, num_feats = tpl\n",
    "            f1s.append(best_test_f1)\n",
    "        return np.mean(f1s)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process stopped by user\")\n",
    "\n",
    "def train_model_parallel_logged(training_collection_name: str, results_processor: ResultsProcessor,\n",
    "                                feat_extractors, params,\n",
    "                                cv_folds, name2essay,\n",
    "                                learning_rate: float, max_update_items:int, set_cr_tags, \\\n",
    "                                initial_weight: float,  max_epochs=5, early_stop_iters=5, n_jobs=None):\n",
    "    if not n_jobs or n_jobs == None:\n",
    "        n_jobs = len(cv_folds)\n",
    "\n",
    "    try:\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(train_model_fold)(train, test, name2essay, learning_rate, max_update_items, set_cr_tags, \\\n",
    "                                      initial_weight, max_epochs, early_stop_iters)\n",
    "            for (train, test) in cv_folds)\n",
    "\n",
    "        cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "        cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "        f1s = []\n",
    "        feats = []\n",
    "        for tpl in results:\n",
    "            best_test_f1, best_iterations, train_ys_bytag, train_pred_ys_bytag, test_ys_bytag, test_pred_ys_bytag, num_feats = tpl\n",
    "            feats.append(num_feats)\n",
    "            f1s.append(best_test_f1)\n",
    "\n",
    "            merge_dictionaries(train_ys_bytag, cv_sent_td_ys_by_tag)\n",
    "            merge_dictionaries(test_ys_bytag, cv_sent_vd_ys_by_tag)\n",
    "\n",
    "            merge_dictionaries(train_pred_ys_bytag, cv_sent_td_predictions_by_tag)\n",
    "            merge_dictionaries(test_pred_ys_bytag, cv_sent_vd_predictions_by_tag)\n",
    "\n",
    "\n",
    "        ALGO = \"Structured Perceptron\"\n",
    "        validation_collection = training_collection_name.replace(\"_TD\", \"_VD\")\n",
    "\n",
    "        # extractors = list(map(lambda fn: fn.func_name, feat_extractors))\n",
    "        extractors = list(feat_extractors)\n",
    "\n",
    "        parameters = {\n",
    "            \"learning_rate\":        learning_rate,\n",
    "            \"loss_type\":            \"None\",\n",
    "            \"max_update_items\":     max_update_items,\n",
    "            \"initial_weight\":       initial_weight,\n",
    "\n",
    "            \"max_epochs\":           max_epochs,\n",
    "            \"early_stopping_iters\": early_stop_iters,\n",
    "\n",
    "            \"extractors\":           extractors,\n",
    "\n",
    "            # Add in number of features\n",
    "            \"num_feats_per_fold\":   feats,\n",
    "            \"num_feats_MEAN\":       np.mean(feats)\n",
    "        }\n",
    "        # add in additional parameters not passed in\n",
    "        parameters.update(params)\n",
    "\n",
    "        wd_td_objectid = results_processor.persist_results(training_collection_name,\n",
    "                                                           cv_sent_td_ys_by_tag,\n",
    "                                                           cv_sent_td_predictions_by_tag,\n",
    "                                                           parameters, ALGO)\n",
    "\n",
    "        wd_vd_objectid = results_processor.persist_results(validation_collection,\n",
    "                                                           cv_sent_vd_ys_by_tag,\n",
    "                                                           cv_sent_vd_predictions_by_tag,\n",
    "                                                           parameters, ALGO)\n",
    "\n",
    "        avg_f1 = float(results_processor.get_metric(validation_collection, wd_vd_objectid, __MICRO_F1__)[\"f1_score\"])\n",
    "        return avg_f1\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process stopped by user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "902 226\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "CAUSAL_MODEL_TYPE = CausalModelType.CORAL_BLEACHING\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "crels_folder = \"./crels/CB\"\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "MONGO_COLLECTION = \"CB_STR_PCPTRN_RE-RANKER_FEATURE_SEL_TD\"\n",
    "# first and second were with initial_weight set to 1.0\n",
    "# thrid is with set to 0.001\n",
    "\n",
    "config = get_config(training_folder)\n",
    "results_processor = ResultsProcessor(dbname=\"metrics_causal_model_reranker\")\n",
    "\n",
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "print(len(pred_tagged_essays_train), len(pred_tagged_essays_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:2->Result:4',\n",
       " 'Causer:13->Result:4',\n",
       " 'Causer:4->Result:11',\n",
       " 'Causer:1->Result:11',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:12->Result:14',\n",
       " 'Causer:1->Result:6',\n",
       " 'Causer:1->Result:5',\n",
       " 'Causer:3->Result:13',\n",
       " 'Causer:2->Result:50']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "\n",
    "set_cr_tags = set(cr_tags)\n",
    "list(set_cr_tags)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = pred_tagged_essays_train + pred_tagged_essays_test\n",
    "name2essay = {}\n",
    "for essay in all_essays:\n",
    "    name2essay[essay.name] = essay\n",
    "\n",
    "name2crels = essay_to_crels(all_essays, set_cr_tags)\n",
    "assert len(name2crels) == len(all_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Re-Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Parses from Sentence Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_top_n = 2\n",
    "min_feat_freq = 1\n",
    "best_max_upd = 2 \n",
    "best_max_parses = 300\n",
    "best_min_prob = 0.0  # min prob of 0 seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_fname = \"xs_rerank_\" + str(best_top_n) + \".dill\"\n",
    "with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "    xs_rerank = dill.load(f)\n",
    "\n",
    "rr_fname = \"xs_rerank_test\" + str(best_top_n) + \".dill\"\n",
    "with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "    xs_test_rerank = dill.load(f)\n",
    "    \n",
    "assert len(xs_rerank) == len(pred_tagged_essays_train),     \"Wrong number of train crels\"\n",
    "assert len(xs_test_rerank) == len(pred_tagged_essays_test), \"Wrong number of test crels\"\n",
    "len(xs_rerank), len(xs_test_rerank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 37s, sys: 1.66 s, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xs = get_features_from_probabilities(xs_rerank, name2crels, best_max_parses, \n",
    "                                     causal_model_type=CAUSAL_MODEL_TYPE,\n",
    "                                     min_feat_freq=min_feat_freq, min_prob=best_min_prob)\n",
    "\n",
    "cv_folds_rerank = cross_validation(xs, 5)\n",
    "cv_folds_mm = [min_max_normalize_feats(train, test) for (train, test) in cv_folds_rerank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.12 s, sys: 65.9 ms, total: 5.18 s\n",
      "Wall time: 5.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xs_test = get_features_from_probabilities(xs_test_rerank, name2crels, best_max_parses, \n",
    "                                          causal_model_type=CAUSAL_MODEL_TYPE,\n",
    "                                          min_feat_freq=min_feat_freq, min_prob=best_min_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset \n",
    "  # training data comes from the test fold predictions from CV on the full training dataset\n",
    "xs_train = []\n",
    "for train, test in cv_folds_rerank:\n",
    "    xs_train.extend(test)\n",
    "\n",
    "# Normalize both using training data\n",
    "xs_train_mm, xs_test_mm = min_max_normalize_feats(xs_train,xs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "max_update_items = 2  # best_max_upd - 2\n",
    "initial_weight = 0.01  # was 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# f1 = train_model_parallel(\n",
    "#     cv_folds=cv_folds_mm, name2essay=name2essay, C=best_C, pa_type=1, loss_type=\"ml\", max_update_items=best_max_upd, \n",
    "#     set_cr_tags=set_cr_tags, initial_weight=initial_weight)\n",
    "# print(f1)  # 0.7421167703055035"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(0.8 * len(xs_train_mm))\n",
    "tmp_train_copy = list(xs_train_mm)\n",
    "np.random.shuffle(tmp_train_copy)\n",
    "tmp_train, tmp_test = tmp_train_copy[:num_train], tmp_train_copy[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Number of Training Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7469 Test Accuracy: 0.7377\n",
      "Epoch: 1 Train Accuracy: 0.7520 Test Accuracy: 0.7436\n",
      "Epoch: 2 Train Accuracy: 0.7578 Test Accuracy: 0.7365\n",
      "Epoch: 3 Train Accuracy: 0.7587 Test Accuracy: 0.7348\n",
      "Epoch: 4 Train Accuracy: 0.7601 Test Accuracy: 0.7348\n",
      "Epoch: 5 Train Accuracy: 0.7635 Test Accuracy: 0.7336\n",
      "Epoch: 6 Train Accuracy: 0.7643 Test Accuracy: 0.7367\n",
      "Best Test Acc: 0.7436\n",
      "Best iterations: 2\n",
      "CPU times: user 2min 19s, sys: 454 ms, total: 2min 19s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use training data to determine number of iterations\n",
    "mdl = StructuredPerceptron(learning_rate=learning_rate,\n",
    "                        max_update_items=max_update_items, initial_weight=initial_weight)\n",
    "# Determine number of training iterations\n",
    "best_mdl, test_acc_df_ml, best_iterations = train_model(mdl, xs_train=tmp_train, xs_test=tmp_test, name2essay=name2essay, set_cr_tags=set_cr_tags,\n",
    "     max_epochs=20, early_stop_iters=5, \n",
    "     train_instance_fn = train_instance, verbose=True, early_stopping=True)\n",
    "print(\"Best iterations:\", best_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_features import filter_feats\n",
    "\n",
    "prefixes = [\n",
    "    \"Prob-\",\n",
    "#     \"CREL_Pair-\",\n",
    "    \"Inv-\",\n",
    "    \"num_crels\",\n",
    "    \"Tally-\",\n",
    "    \"CChain-\",\n",
    "    \"CChainStats-\",\n",
    "    \"Above-\",\n",
    "    \"CREL_\",\n",
    "    \"Propn_\",\n",
    "    \"Diff_\"\n",
    "]\n",
    "# xs_fltr_train, xs_fltr_test = filter_feats(xs_train_mm, xs_test_mm, prefixes)\n",
    "assert len(prefixes) == len(set(prefixes)), \"Duplicate prefixes found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = -1\n",
    "current_best = []\n",
    "remaining = list(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CB_STR_PCPTRN_RE-RANKER_FEATURE_SEL_TD', 2, 0.01)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONGO_COLLECTION, best_iterations, initial_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "\t1 feats F1: 0.737605 Prefixes: ['Prob-']\n",
      "\t1 feats F1: 0.000000 Prefixes: ['Inv-']\n",
      "\t1 feats F1: 0.731349 Prefixes: ['num_crels']\n",
      "\t1 feats F1: 0.731736 Prefixes: ['Tally-']\n",
      "\t1 feats F1: 0.339151 Prefixes: ['CChain-']\n",
      "\t1 feats F1: 0.624415 Prefixes: ['CChainStats-']\n",
      "\t1 feats F1: 0.733841 Prefixes: ['Above-']\n",
      "\t1 feats F1: 0.729187 Prefixes: ['CREL_']\n",
      "\t1 feats F1: 0.633727 Prefixes: ['Propn_']\n",
      "\t1 feats F1: 0.635851 Prefixes: ['Diff_']\n",
      "1 feats, new Best F1: 0.737605 Prefixes: ['Prob-']\n",
      "\t2 feats F1: 0.739394 Prefixes: ['Prob-', 'Inv-']\n",
      "\t2 feats F1: 0.735788 Prefixes: ['Prob-', 'num_crels']\n",
      "\t2 feats F1: 0.733785 Prefixes: ['Prob-', 'Tally-']\n",
      "\t2 feats F1: 0.718462 Prefixes: ['Prob-', 'CChain-']\n",
      "\t2 feats F1: 0.708425 Prefixes: ['Prob-', 'CChainStats-']\n",
      "\t2 feats F1: 0.740483 Prefixes: ['Prob-', 'Above-']\n",
      "\t2 feats F1: 0.727019 Prefixes: ['Prob-', 'CREL_']\n",
      "\t2 feats F1: 0.736034 Prefixes: ['Prob-', 'Propn_']\n",
      "\t2 feats F1: 0.727026 Prefixes: ['Prob-', 'Diff_']\n",
      "2 feats, new Best F1: 0.740483 Prefixes: ['Prob-', 'Above-']\n",
      "\t3 feats F1: 0.737657 Prefixes: ['Prob-', 'Above-', 'Inv-']\n",
      "\t3 feats F1: 0.738123 Prefixes: ['Prob-', 'Above-', 'num_crels']\n",
      "\t3 feats F1: 0.738053 Prefixes: ['Prob-', 'Above-', 'Tally-']\n",
      "\t3 feats F1: 0.725949 Prefixes: ['Prob-', 'Above-', 'CChain-']\n",
      "\t3 feats F1: 0.734918 Prefixes: ['Prob-', 'Above-', 'CChainStats-']\n",
      "\t3 feats F1: 0.734596 Prefixes: ['Prob-', 'Above-', 'CREL_']\n",
      "\t3 feats F1: 0.738169 Prefixes: ['Prob-', 'Above-', 'Propn_']\n",
      "\t3 feats F1: 0.736985 Prefixes: ['Prob-', 'Above-', 'Diff_']\n",
      "No further improvement, stopping\n",
      "CPU times: user 20min 36s, sys: 1min 43s, total: 22min 20s\n",
      "Wall time: 31min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    \"learning_rate\":   learning_rate,\n",
    "    \"best_max_upd\":    best_max_upd,\n",
    "    \"best_max_parses\": best_max_parses,\n",
    "    \"best_min_prob\":   best_min_prob,\n",
    "    \"min_feat_freq\":   min_feat_freq,\n",
    "    \"best_iterations\": best_iterations\n",
    "}\n",
    "\n",
    "print(\"Starting...\")\n",
    "while True:\n",
    "    if len(remaining) == 0:\n",
    "        break\n",
    "    \n",
    "    f1_by_prefix = dict()\n",
    "    for prefix in remaining:\n",
    "        new_prefixes = current_best + [prefix]\n",
    "        \n",
    "        cv_filtered = []\n",
    "        for tr, test in cv_folds_mm:\n",
    "            x_tr,x_test = filter_feats(tr, test, new_prefixes)\n",
    "            cv_filtered.append((x_tr,x_test))\n",
    "        \n",
    "        f1_by_prefix[prefix] = train_model_parallel_logged(\n",
    "            training_collection_name=MONGO_COLLECTION, results_processor=results_processor,\n",
    "            feat_extractors=new_prefixes, params=params,\n",
    "            \n",
    "            cv_folds=cv_filtered, name2essay=name2essay,\n",
    "            learning_rate=learning_rate, max_update_items=best_max_upd, \n",
    "            set_cr_tags=set_cr_tags, initial_weight=initial_weight,\n",
    "            # use best iterations from above\n",
    "            max_epochs=best_iterations, early_stop_iters=best_iterations\n",
    "        )\n",
    "        print(\"\\t{length} feats F1: {f1:.6f} Prefixes: {prefixes}\".format(\n",
    "            length=len(new_prefixes), f1=f1_by_prefix[prefix], prefixes=str(new_prefixes)))\n",
    "    \n",
    "    best_prefix, new_best_f1 = sorted(f1_by_prefix.items(), key = lambda tpl: -tpl[1])[0]\n",
    "    if new_best_f1 > best_f1:\n",
    "        best_f1 = new_best_f1\n",
    "        current_best.append(best_prefix)\n",
    "        remaining.remove(best_prefix)\n",
    "        print(\"{length} feats, new Best F1: {f1:.6f} Prefixes: {prefixes}\".format(\n",
    "            length=len(current_best), f1=best_f1, prefixes=str(current_best)))\n",
    "    else:\n",
    "        print(\"No further improvement, stopping\")\n",
    "        break\n",
    "\n",
    "# 1 feats, new Best F1: 0.7430 Prefixes: ['CREL_']\n",
    "# 2 feats, new Best F1: 0.7438 Prefixes: ['CREL_', 'Prob-']\n",
    "# 3 feats, new Best F1: 0.7461 Prefixes: ['CREL_', 'Prob-', 'CChainStats-']\n",
    "# No further improvement, stopping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Prob-', 'Above-'], 0.7404830917874395)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_best, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train_mm_fltr, xs_test_mm_fltr = filter_feats(xs_train_mm, xs_test_mm, current_best) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(0.8 * len(xs_train_mm_fltr))\n",
    "tmp_train_copy = list(xs_train_mm_fltr)\n",
    "np.random.shuffle(tmp_train_copy)\n",
    "tmp_train, tmp_test = tmp_train_copy[:num_train], tmp_train_copy[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Optimal Number of Training Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_weight = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7360 Test Accuracy: 0.7625\n",
      "Epoch: 1 Train Accuracy: 0.7344 Test Accuracy: 0.7684\n",
      "Epoch: 2 Train Accuracy: 0.7344 Test Accuracy: 0.7684\n",
      "Epoch: 3 Train Accuracy: 0.7348 Test Accuracy: 0.7684\n",
      "Epoch: 4 Train Accuracy: 0.7351 Test Accuracy: 0.7691\n",
      "Epoch: 5 Train Accuracy: 0.7353 Test Accuracy: 0.7639\n",
      "Epoch: 6 Train Accuracy: 0.7348 Test Accuracy: 0.7677\n",
      "Epoch: 7 Train Accuracy: 0.7351 Test Accuracy: 0.7677\n",
      "Best Test Acc: 0.7691\n",
      "CPU times: user 43.8 s, sys: 90.1 ms, total: 43.9 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mdl = StructuredPerceptron(learning_rate=learning_rate,\n",
    "                        max_update_items=max_update_items, initial_weight=initial_weight)\n",
    "# Determine number of training iterations\n",
    "best_mdl, test_acc_df_ml, best_iterations = train_model(mdl, xs_train=tmp_train, xs_test=tmp_test, name2essay=name2essay, set_cr_tags=set_cr_tags,\n",
    "     max_epochs=20, early_stop_iters=3, train_instance_fn = train_instance,\n",
    "                                                        verbose=True,  early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for X Iterations on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7402 Test Accuracy: 0.7475\n",
      "Epoch: 1 Train Accuracy: 0.7396 Test Accuracy: 0.7495\n",
      "Epoch: 2 Train Accuracy: 0.7398 Test Accuracy: 0.7500\n",
      "Epoch: 3 Train Accuracy: 0.7417 Test Accuracy: 0.7514\n",
      "Epoch: 4 Train Accuracy: 0.7411 Test Accuracy: 0.7507\n",
      "Best Test Acc: 0.7514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = StructuredPerceptron(learning_rate=learning_rate,\n",
    "                        max_update_items=max_update_items, initial_weight=initial_weight)\n",
    "\n",
    "best_mdl, test_acc_df_ml,_ = train_model(mdl,  \n",
    "    xs_train=xs_train_mm_fltr, xs_test=xs_test_mm_fltr,\n",
    "    name2essay=name2essay, set_cr_tags=set_cr_tags,\n",
    "    max_epochs=best_iterations, early_stop_iters=best_iterations,\n",
    "    train_instance_fn = train_instance, verbose=True, early_stopping=False)\n",
    "\n",
    "# Best Test Acc: 0.7514\n",
    "best_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7506899724011039"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Above-%-0.2', -1.02),\n",
       " ('Prob-max-prob', 0.9429589648398735),\n",
       " ('Prob-95%-prob', 0.8213451209989859),\n",
       " ('Prob-geo-mean', 0.8051268115439676),\n",
       " ('Above-%-0.9', 0.7180519480519487),\n",
       " ('Prob-90%-prob', 0.6997312771580984),\n",
       " ('Prob-prod-prob', -0.6939080802595455),\n",
       " ('Prob-med-prob', 0.6203400901729199),\n",
       " ('Above-All-Above-0.7', 0.6100000000000001),\n",
       " ('Above-0.95', 0.5725000000000002),\n",
       " ('Above-%-0.8', 0.5281818181818139),\n",
       " ('Prob-avg-prob', 0.4793252430954091),\n",
       " ('Prob-10%-prob', -0.4771778016015764),\n",
       " ('Prob-min-prob', 0.44454078233479744),\n",
       " ('Above-%-0.95', 0.42477272727272786),\n",
       " ('Prob-25%-prob', 0.3600578207062597),\n",
       " ('Above-All-Above-0.3', 0.3100000000000001),\n",
       " ('Above-All-Above-0.2', 0.3099999999999999),\n",
       " ('Above-All-Above-0.95', -0.28999999999999987),\n",
       " ('Above-0.5', 0.2799999999999955),\n",
       " ('Above-%-0.7', 0.2780735930735939),\n",
       " ('Above-0.7', 0.27666666666666684),\n",
       " ('Above-%-0.3', 0.14321428571428568),\n",
       " ('Prob-75%-prob', 0.12791653171007278),\n",
       " ('Above-0.8', 0.07666666666666655),\n",
       " ('Above-0.3', -0.04454545454546056),\n",
       " ('Above-0.2', -0.044545454545460335),\n",
       " ('Above-%-0.5', -0.03411255411255454),\n",
       " ('Prob-5%-prob', -0.016318509633390488),\n",
       " ('Above-0.9', 0.010000000000001813),\n",
       " ('Above-All-Above-0.5', 0.01000000000000012),\n",
       " ('Above-All-Above-0.8', 0.01000000000000012),\n",
       " ('Above-All-Above-0.9', 0.01000000000000012)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(best_mdl.weights.items(), key = lambda tpl: -abs(tpl[1]))[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CB_STR_PCPTRN_RE-RANKER_FEATURE_SEL_TD'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONGO_COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-413abae2e552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfeat_extractors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_prefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mcv_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2essay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname2essay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_C\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mpa_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_update_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_max_upd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mset_cr_tags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_cr_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_C' is not defined"
     ]
    }
   ],
   "source": [
    "new_prefixes = list(prefixes) # all features\n",
    "\n",
    "cv_filtered = []\n",
    "for tr, test in cv_folds_mm:\n",
    "    x_tr,x_test = filter_feats(tr, test, new_prefixes)\n",
    "    cv_filtered.append((x_tr,x_test))\n",
    "        \n",
    "train_model_parallel_logged(\n",
    "            training_collection_name=MONGO_COLLECTION + \"_MAX_FT_CNTS\", results_processor=results_processor,\n",
    "            feat_extractors=new_prefixes, params=params,\n",
    "            \n",
    "            cv_folds=cv_filtered, name2essay=name2essay, C=best_C, \n",
    "            pa_type=1, loss_type=\"ml\", max_update_items=best_max_upd, \n",
    "            set_cr_tags=set_cr_tags, initial_weight=initial_weight,\n",
    "            # use best iterations from above\n",
    "            max_epochs=1, early_stop_iters=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
