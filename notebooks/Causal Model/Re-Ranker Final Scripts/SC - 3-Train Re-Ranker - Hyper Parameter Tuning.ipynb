{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "cm_folder = \"/Users/simon.hughes/GitHub/NlpResearch/PythonNlpResearch/notebooks/Causal Model/\"\n",
    "src_path = os.path.join(cm_folder, \"src\")\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from MIRA import CostSensitiveMIRA\n",
    "from Settings import Settings\n",
    "\n",
    "from window_based_tagger_config import get_config\n",
    "from crel_helper import get_cr_tags\n",
    "from crel_processing import essay_to_crels_cv\n",
    "from evaluation import evaluate_model_essay_level, get_micro_metrics, metrics_to_df\n",
    "from feature_normalization import min_max_normalize_feats\n",
    "from function_helpers import get_function_names\n",
    "from results_procesor import ResultsProcessor\n",
    "from train_parser import essay_to_crels, create_extractor_functions\n",
    "from cost_functions import micro_f1_cost_plusepsilon\n",
    "from train_reranker import train_model_parallel_logged, train_model_parallel, train_model, train_cost_sensitive_instance\n",
    "from searn_parser_breadth_first import SearnModelBreadthFirst\n",
    "from causal_model_features import CausalModelType\n",
    "from feature_extraction import get_features_from_probabilities\n",
    "from results_procesor import ResultsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n",
      "870 218\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "CAUSAL_MODEL_TYPE = CausalModelType.SKIN_CANCER\n",
    "root_folder = settings.data_directory + \"SkinCancer/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "crels_folder = \"./crels/SC\"\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "MONGO_COLLECTION = \"SC_RE-RANKER_HYPER_PARAM_TD\"\n",
    "\n",
    "config = get_config(training_folder)\n",
    "results_processor = ResultsProcessor(dbname=\"metrics_causal_model_reranker\")\n",
    "\n",
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "print(len(pred_tagged_essays_train), len(pred_tagged_essays_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:50->Result:3',\n",
       " 'Causer:12->Result:2',\n",
       " 'Causer:12->Result:4',\n",
       " 'Causer:2->Result:1',\n",
       " 'Causer:12->Result:50',\n",
       " 'Causer:4->Result:50',\n",
       " 'Causer:50->Result:1',\n",
       " 'Causer:11->Result:4',\n",
       " 'Causer:1->Result:2',\n",
       " 'Causer:12->Result:12']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "\n",
    "set_cr_tags = set(cr_tags)\n",
    "list(set_cr_tags)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = pred_tagged_essays_train + pred_tagged_essays_test\n",
    "name2essay = {}\n",
    "for essay in all_essays:\n",
    "    name2essay[essay.name] = essay\n",
    "\n",
    "name2crels = essay_to_crels(all_essays, set_cr_tags)\n",
    "assert len(name2crels) == len(all_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Re-Ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Parses from Sentence Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_top_n = 2\n",
    "min_feat_freq = 1\n",
    "best_max_upd = 2 \n",
    "best_max_parses = 300\n",
    "best_min_prob = 0.0  # min prob of 0 seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rerank(top_n):\n",
    "    rr_fname = \"xs_rerank_\" + str(top_n) + \".dill\"\n",
    "    with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "        xs_rerank = dill.load(f)\n",
    "\n",
    "    rr_fname = \"xs_rerank_test\" + str(top_n) + \".dill\"\n",
    "    with open(os.path.join(crels_folder, rr_fname), \"rb\") as f:\n",
    "        xs_test_rerank = dill.load(f)\n",
    "    return xs_rerank, xs_test_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filter_features import filter_feats\n",
    "\n",
    "prefixes = [\n",
    "    \"Prob-\",\n",
    "#     \"CREL_Pair-\",\n",
    "    \"Inv-\",\n",
    "    \"num_crels\",\n",
    "    \"Tally-\",\n",
    "    \"CChain-\",\n",
    "    \"CChainStats-\",\n",
    "    \"Above-\",\n",
    "    \"CREL_\",\n",
    "    \"Propn_\",\n",
    "    \"Diff_\"\n",
    "]\n",
    "\n",
    "# Results for SC\n",
    "current_best = ['CREL_', 'CChain-', 'Prob-']\n",
    "best_iterations = 1\n",
    "\n",
    "# default params (not all of these are optimized)\n",
    "params = {\n",
    "    \"best_top_n\": best_top_n,\n",
    "    \"best_max_upd\": best_max_upd,\n",
    "    \"best_max_parses\": best_max_parses,\n",
    "    \"best_min_prob\": best_min_prob,\n",
    "    \"min_feat_freq\": min_feat_freq\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SC_RE-RANKER_HYPER_PARAM_TD', 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONGO_COLLECTION, best_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Param Hash (avoid re-running same experiment multiple times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = (\"C\", \"best_max_parses\", \n",
    "        #\"best_max_upd\", \n",
    "        \"max_update_items\", \n",
    "        \"best_min_prob\", \"best_top_n\", \"extractors\", \"initial_weight\", \"loss_type\",\\\n",
    "        \"min_feat_freq\", \"pa_type\")\n",
    "\n",
    "def hash_params(params):\n",
    "    p = dict()\n",
    "    # only copy over white list vals\n",
    "    for v in vals:\n",
    "        p[v] = params[v]\n",
    "    return str(sorted(p.items())).replace(\" \",\"\")\n",
    "\n",
    "def load_param_hash(db, collection):\n",
    "    project = {\n",
    "        \"params\": \"$parameters\",\n",
    "#         \"asof\":   \"$asof\",\n",
    "        \"_id\": 1\n",
    "    }\n",
    "    feats_pipeline = [{ \"$project\": project }]\n",
    "    rows = [row for row in db[collection].aggregate(feats_pipeline)]\n",
    "    print(\"len(rows)\", len(rows))\n",
    "    param_hash = set()\n",
    "    for r in rows:\n",
    "        param_hash.add(hash_params(r[\"params\"]))\n",
    "    return param_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rows) 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 'SC_RE-RANKER_HYPER_PARAM_TD')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(serverSelectionTimeoutMS=100, host=\"127.0.0.1\")\n",
    "param_hash = load_param_hash(client.metrics_causal_model_reranker, MONGO_COLLECTION)\n",
    "len(param_hash), MONGO_COLLECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 6, 17, 21, 25, 27, 463774)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = -1\n",
    "# best_f1 = 0.7457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a closure to simplify loop\n",
    "def trn_mdl(top_n, prms, cv_filtrd):\n",
    "    \n",
    "    global best_f1, best_C, pa_type, best_max_upd, initial_weight, loss_type\n",
    "    \n",
    "    n_jobs = None\n",
    "    # Uses too much RAM, drop to single threaded\n",
    "    if top_n > 3:        \n",
    "        n_jobs = 1\n",
    "    \n",
    "    f1 = train_model_parallel_logged(\n",
    "        training_collection_name=MONGO_COLLECTION, results_processor=results_processor,\n",
    "        feat_extractors=current_best, params=prms,\n",
    "\n",
    "        cv_folds=cv_filtrd, name2essay=name2essay, \n",
    "        C=best_C, pa_type=pa_type, loss_type=loss_type, max_update_items=best_max_upd, initial_weight=initial_weight,\n",
    "        set_cr_tags=set_cr_tags,\n",
    "        # use best iterations from above\n",
    "        max_epochs=best_iterations, early_stop_iters=best_iterations,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        print(\"New Best F1: {f1:.6f}:\\t{params}\".format(f1=best_f1, params=\\\n",
    "                                                       str((top_n, best_C, pa_type, best_max_upd, initial_weight, loss_type))))\n",
    "    else:\n",
    "        print(\"         F1: {f1:.6f}:\\t{params}\".format(f1=best_f1, params=\\\n",
    "                                                       str((top_n, best_C, pa_type, best_max_upd, initial_weight, loss_type))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(rows) 0\n",
      "top_n 2\n",
      "New Best F1: 0.801698:\t(2, 0.0005, 1, 1, 0.01, 'pb')\n",
      "New Best F1: 0.806351:\t(2, 0.0005, 1, 1, 0.01, 'ml')\n",
      "         F1: 0.806351:\t(2, 0.0005, 2, 1, 0.01, 'pb')\n",
      "New Best F1: 0.806403:\t(2, 0.0005, 2, 1, 0.01, 'ml')\n"
     ]
    }
   ],
   "source": [
    "param_hash = load_param_hash(client.metrics_causal_model_reranker, MONGO_COLLECTION)\n",
    "\n",
    "for top_n in [2,3,5,1]:\n",
    "    \n",
    "    print(\"top_n\", top_n)\n",
    "    xs_rr, _ = load_rerank(top_n)\n",
    "    xs_temp = get_features_from_probabilities(xs_rr, name2crels, best_max_parses, \n",
    "                                     causal_model_type=CAUSAL_MODEL_TYPE,\n",
    "                                     min_feat_freq=min_feat_freq, min_prob=best_min_prob)\n",
    "    \n",
    "    cv_flds_rr = cross_validation(xs_temp, 5)\n",
    "    cv_flds_mm = [min_max_normalize_feats(train, test) for (train, test) in cv_flds_rr]\n",
    "    \n",
    "    skipped = False\n",
    "    for best_C in [0.0005, 0.0025, 0.0100, 0.1]:\n",
    "        for pa_type in [1,2]: # [0]\n",
    "            for best_max_upd in [1]:\n",
    "                for initial_weight in [0.01]:\n",
    "                    for loss_type in [\"pb\", \"ml\"]:\n",
    "                        \n",
    "                        p = {'C': best_C,\n",
    "                             'best_max_parses': best_max_parses,\n",
    "                             'best_min_prob': best_min_prob,\n",
    "                             'best_top_n': top_n,\n",
    "                             'extractors': list(current_best),\n",
    "                             'initial_weight': initial_weight,\n",
    "                             'loss_type': loss_type,\n",
    "                             'max_update_items': best_max_upd,\n",
    "                             'min_feat_freq': min_feat_freq,\n",
    "                             'pa_type': pa_type\n",
    "                        }\n",
    "                        hash_p = hash_params(p)\n",
    "                        if hash_p in param_hash:\n",
    "                            print(\".\", end = '')\n",
    "                            skipped = True\n",
    "                            continue\n",
    "                        if skipped:\n",
    "                            print()\n",
    "                        trn_mdl(top_n, p, cv_flds_mm)\n",
    "                        skipped = False\n",
    "\n",
    "# top_n, best_C, pa_type, best_max_upd, initial_weight, loss_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "- re-run test on loss_type\n",
    "- Fix CREL_Pair filtering\n",
    "- Try different top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_type pb\n",
    "for best_C in [0.0005, 0.0025, 0.0100, 0.1]:\n",
    "    for pa_type in [1,2]: # [0]\n",
    "        for best_max_upd in [1,2,5,10]:\n",
    "            for initial_weight in [0, 0.01, 0.1, 1.0]:\n",
    "                for loss_type in [\"pb\"]:\n",
    "                    trn_mdl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pa_type 0 doesn't work \n",
    "\n",
    "# for best_C in [0.0005, 0.0025, 0.0100, 0.1]:\n",
    "#     for pa_type in [0]: # [0]\n",
    "#         for best_max_upd in [1,2,5,10]:\n",
    "#             for initial_weight in [0, 0.01, 0.1, 1.0]:\n",
    "#                 for loss_type in [\"pb\",\"ml\"]:\n",
    "#                     trn_mdl()\n",
    "\n",
    "# AssertionError: decision function value is nan for feat: CREL_Causer:6->Result:50-MAX(prob) and val: 0.9499406368952471 and weight: nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current best settings\n",
    "best_C, pa_type, best_max_upd, initial_weight = (0.01, 1, 1, 0.01)\n",
    "loss_type = \"ml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train_mm_fltr, xs_test_mm_fltr = filter_feats(xs_train_mm, xs_test_mm, current_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning dataset from training data\n",
    "num_train = int(0.8 * len(xs_train_mm_fltr))\n",
    "tmp_train_copy = list(xs_train_mm_fltr)\n",
    "np.random.shuffle(tmp_train_copy)\n",
    "tmp_train, tmp_test = tmp_train_copy[:num_train], tmp_train_copy[num_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the Optimal Number of Training Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mdl = CostSensitiveMIRA(C=best_C, pa_type=pa_type, loss_type=loss_type, \n",
    "                        max_update_items=best_max_upd, initial_weight=initial_weight)\n",
    "# Determine number of training iterations\n",
    "best_mdl, test_acc_df_ml, best_iterations = train_model(mdl, xs_train=tmp_train, xs_test=tmp_test, name2essay=name2essay, set_cr_tags=set_cr_tags,\n",
    "     max_epochs=20, early_stop_iters=3, train_instance_fn = train_cost_sensitive_instance,\n",
    "                                                        verbose=True,  early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run for X Iterations on the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = CostSensitiveMIRA(C=best_C, pa_type=pa_type, loss_type=loss_type, \n",
    "                        max_update_items=best_max_upd, initial_weight=initial_weight)\n",
    "\n",
    "best_mdl, test_acc_df_ml,_ = train_model(mdl,  \n",
    "    xs_train=xs_train_mm_fltr, xs_test=xs_test_mm_fltr,\n",
    "    name2essay=name2essay, set_cr_tags=set_cr_tags,\n",
    "    max_epochs=best_iterations, early_stop_iters=best_iterations,\n",
    "    train_instance_fn = train_cost_sensitive_instance, verbose=True, early_stopping=False)\n",
    "\n",
    "# Best Test Acc: 0.7516\n",
    "best_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_TEST_COLLECTION = \"TEST_CB_RE-RANKER_TD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds = [(xs_train_mm_fltr, xs_test_mm_fltr)]\n",
    "\n",
    "test_f1 = train_model_parallel_logged(\n",
    "        training_collection_name=MONGO_TEST_COLLECTION, results_processor=results_processor,\n",
    "        feat_extractors=current_best, params=params,\n",
    "\n",
    "        cv_folds=test_folds, \n",
    "        \n",
    "        name2essay=name2essay, \n",
    "        C=best_C, pa_type=pa_type, loss_type=loss_type, max_update_items=best_max_upd, initial_weight=initial_weight,\n",
    "        set_cr_tags=set_cr_tags,\n",
    "        # use best iterations from above\n",
    "        max_epochs=best_iterations, early_stop_iters=best_iterations\n",
    "    )\n",
    "test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(best_mdl.weights.items(), key = lambda tpl: -abs(tpl[1]))[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "### Ideas\n",
    "- ~~num-crels - add back in the logic to threshold these? But only if needed to improve results here (seemed to help essay parser)~~\n",
    "- ~~Re-run with more realistic initial hyper params~~\n",
    "- ~~Add in logic to store results to mongo~~\n",
    "- ~~Switch back to using an initial_weight of 1~~\n",
    "- Approach seems very sensitive to the initial configuration of the algorithm. However, it also seems correlated to the training data performance on the first epoch. Run the algorithm multiple times, take the model with the best training performance and use that as the final selected model to train futher.\n",
    "- We need to add hyper parameter tuning\n",
    "- Do we want to just remove the BEAM search from this? It makes the explanation a lot more complex. But then again, it's the only way we can really add more crels that the model wouldn't otherwise parse\n",
    "- Do we use the BEAM search with some de-duping? Although we already de-dupe to some extent anyways\n",
    "\n",
    "### Needed to Finish\n",
    "- Record run on test data - needs optimal hyper parameters first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
