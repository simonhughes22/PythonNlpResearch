{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries\n",
    "from searn_parser_breadth_first import ParseActionResult, SearnModelBreadthFirst\n",
    "from MIRA import MIRA, CostSensitiveMIRA\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "cr_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cr_tags = set(cr_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_essay_level(\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        extractor_fn_names_lst: List[str],\n",
    "        cost_function_name: str,\n",
    "        beta: float,\n",
    "        ngrams: int,\n",
    "        stemmed: bool,\n",
    "        max_epochs: int,\n",
    "        down_sample_rate=1.0) -> float:\n",
    "\n",
    "    if down_sample_rate < 1.0:\n",
    "        new_folds = []  # type: List[Tuple[Any, Any]]\n",
    "        for i, (essays_TD, essays_VD) in enumerate(folds):\n",
    "            essays_TD = essays_TD[:int(down_sample_rate * len(essays_TD))]\n",
    "            essays_VD = essays_VD[:int(down_sample_rate * len(essays_VD))]\n",
    "            new_folds.append((essays_TD, essays_VD))\n",
    "        folds = new_folds  # type: List[Tuple[Any, Any]]\n",
    "\n",
    "    serial_results = [\n",
    "        train_sr_parser(essays_TD, essays_VD, extractor_fn_names_lst, cost_function_name, ngrams, stemmed, beta, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    parser_models = []\n",
    "    for (model, num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        parser_models.append(model)\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    return parser_models, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "            \n",
    "def get_label_data_essay_level(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_essay = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        add_labels(unique_cr_tags, ys_bytag_essay)\n",
    "    return dict(ys_bytag_essay) # convert to dict so no issue when iterating over if additional keys are present\n",
    "\n",
    "def essay_to_crels(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    name2crels = defaultdict(set)\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        name2crels[essay.name] = unique_cr_tags\n",
    "    return dict(name2crels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_micro_metrics(df):\n",
    "    return df[df.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"recall\", \"precision\"]]\n",
    "\n",
    "def predict_essay_level(parser, essays):\n",
    "    pred_ys_by_sent = defaultdict(list)\n",
    "    for essay_ix, essay in enumerate(essays):\n",
    "        unq_pre_relations = set()\n",
    "        for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "            predicted_tags = essay.pred_tagged_sentences[sent_ix]\n",
    "            pred_relations = parser.predict_sentence(taggged_sentence, predicted_tags)\n",
    "            unq_pre_relations.update(pred_relations)\n",
    "        # Store predictions for evaluation\n",
    "        add_labels(unq_pre_relations, pred_ys_by_sent)\n",
    "    return pred_ys_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_WIDTH = 80\n",
    "\n",
    "# other settings\n",
    "DOWN_SAMPLE_RATE = 1.0  # For faster smoke testing the algorithm\n",
    "BASE_LEARNER_FACT = None\n",
    "COLLECTION_PREFIX = \"CR_CB_SHIFT_REDUCE_PARSER_TEMPLATED_MOST_RECENT_CODE\"\n",
    "\n",
    "# some of the other extractors aren't functional if the system isn't able to do a basic parse\n",
    "# so the base extractors are the MVP for getting to a basic parser, then additional 'meta' parse\n",
    "# features from all_extractors can be included\n",
    "base_extractors = [\n",
    "    single_words,\n",
    "    word_pairs,\n",
    "    three_words,\n",
    "    between_word_features\n",
    "]\n",
    "\n",
    "all_extractor_fns = base_extractors + [\n",
    "    word_distance,\n",
    "    valency,\n",
    "    unigrams,\n",
    "    third_order,\n",
    "    label_set,\n",
    "    size_features\n",
    "]\n",
    "\n",
    "all_cost_functions = [\n",
    "    micro_f1_cost,\n",
    "    micro_f1_cost_squared,\n",
    "    micro_f1_cost_plusone,\n",
    "    micro_f1_cost_plusepsilon,\n",
    "    binary_cost,\n",
    "    inverse_micro_f1_cost,\n",
    "    uniform_cost\n",
    "]\n",
    "\n",
    "all_extractor_fn_names = get_function_names(all_extractor_fns)\n",
    "base_extractor_fn_names = get_function_names(base_extractors)\n",
    "all_cost_fn_names = get_function_names(all_cost_functions)\n",
    "\n",
    "ngrams = 1\n",
    "stemmed = True\n",
    "cost_function_name = micro_f1_cost_plusepsilon.__name__\n",
    "dual = True\n",
    "fit_intercept = True\n",
    "beta = 0.5\n",
    "max_epochs = 2\n",
    "C = 0.5\n",
    "penalty = \"l2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note these also differ for SC dataset\n",
    "BASE_LEARNER_FACT = lambda : LogisticRegression(dual=dual, C=C, penalty=penalty, fit_intercept=fit_intercept)\n",
    "best_extractor_names = ['single_words', 'between_word_features', 'label_set',\n",
    "                                    'three_words', 'third_order', 'unigrams'] # type: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sr_parser(essays_TD, essays_VD, extractor_names, cost_function_name, ngrams, stemmed, beta, max_epochs):\n",
    "    extractors = get_functions_by_name(extractor_names, all_extractor_fns)\n",
    "    # get single cost function\n",
    "    cost_fn = get_functions_by_name([cost_function_name], all_cost_functions)[0]\n",
    "    assert cost_fn is not None, \"Cost function look up failed\"\n",
    "    # Ensure all extractors located\n",
    "    assert len(extractors) == len(extractor_names), \"number of extractor functions does not match the number of names\"\n",
    "\n",
    "    template_feature_extractor = NonLocalTemplateFeatureExtractor(extractors=extractors)\n",
    "    if stemmed:\n",
    "        ngram_extractor = NgramExtractorStemmed(max_ngram_len=ngrams)\n",
    "    else:\n",
    "        ngram_extractor = NgramExtractor(max_ngram_len=ngrams)\n",
    "    parse_model = SearnModelBreadthFirst(feature_extractor=template_feature_extractor,\n",
    "                                             cost_function=cost_fn,\n",
    "                                             min_feature_freq=MIN_FEAT_FREQ,\n",
    "                                             ngram_extractor=ngram_extractor, cr_tags=cr_tags,\n",
    "                                             base_learner_fact=BASE_LEARNER_FACT,\n",
    "                                             beta=beta,\n",
    "                                             # log_fn=lambda s: print(s))\n",
    "                                             log_fn=lambda s: None)\n",
    "\n",
    "    parse_model.train(essays_TD, max_epochs=max_epochs)\n",
    "\n",
    "    num_feats = template_feature_extractor.num_features()\n",
    "\n",
    "    sent_td_ys_bycode = get_label_data_essay_level(essays_TD)\n",
    "    sent_vd_ys_bycode = get_label_data_essay_level(essays_VD)\n",
    "\n",
    "    sent_td_pred_ys_bycode = predict_essay_level(parse_model, essays_TD)\n",
    "    sent_vd_pred_ys_bycode = predict_essay_level(parse_model, essays_VD)\n",
    "\n",
    "    return parse_model, num_feats, sent_td_ys_bycode, sent_vd_ys_bycode, sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds     = [(pred_tagged_essays_train, pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = cross_validation(pred_tagged_essays_train, CV_FOLDS)  # type: List[Tuple[Any,Any]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essay Level Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_essay_level = evaluate_model_essay_level(\n",
    "    folds=cv_folds,\n",
    "    extractor_fn_names_lst=best_extractor_names,\n",
    "    cost_function_name=cost_function_name,\n",
    "    ngrams=ngrams,\n",
    "    beta=beta,\n",
    "    stemmed=stemmed,\n",
    "    down_sample_rate=DOWN_SAMPLE_RATE,\n",
    "    max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.985703</td>\n",
       "      <td>0.780099</td>\n",
       "      <td>0.759672</td>\n",
       "      <td>0.801656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.985703  0.780099  0.759672   0.801656"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, \\\n",
    "    cv_sent_vd_ys_by_tag = result_test_essay_level\n",
    "    \n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.982846</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.730657</td>\n",
       "      <td>0.749251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.982846  0.739837  0.730657   0.749251"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models, cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, \\\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = result_test_essay_level\n",
    "    \n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Re-Ranker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_possible_crels(predicted_tags):\n",
    "    if len(predicted_tags) < 2:\n",
    "        return set()\n",
    "    predicted_tags = sorted(predicted_tags)\n",
    "    pred_crels = set()\n",
    "    for a,b in combinations(predicted_tags, 2):\n",
    "        pred_crels.add(\"Causer:{a}->Result:{b}\".format(a=a, b=b))\n",
    "        pred_crels.add(\"Causer:{b}->Result:{a}\".format(a=a, b=b))\n",
    "    return pred_crels\n",
    "\n",
    "def to_canonical_parse(crels):\n",
    "    return tuple(sorted(crels))\n",
    "\n",
    "def get_crels(parse):\n",
    "    crels = set()\n",
    "    p = parse\n",
    "    while p:\n",
    "        if p.relations:\n",
    "            crels.update(p.relations)\n",
    "        p = p.parent_action\n",
    "    return crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searn_parser_breadth_first import geo_mean\n",
    "\n",
    "def collapse_sent_parse(pred_parses):\n",
    "    crel2prob = defaultdict(list)\n",
    "    for pact in pred_parses:\n",
    "        act_seq = pact.get_action_sequence()\n",
    "        for act in act_seq:\n",
    "            if not act.relations:\n",
    "                continue\n",
    "\n",
    "            assert act.lr_action_prob >= 0\n",
    "            prob = geo_mean([act.action_prob * act.lr_action_prob])\n",
    "            for r in act.relations:\n",
    "                crel2prob[r].append(prob)\n",
    "    return crel2prob\n",
    "\n",
    "def merge_crel_probs(a, b):    \n",
    "    for k,v in b.items():\n",
    "        a[k].extend(v)\n",
    "    return a\n",
    "\n",
    "def get_max_probs(crel2probs):\n",
    "    crel2max_prob = dict()\n",
    "    for crel, probs in crel2probs.items():\n",
    "        crel2max_prob[crel] = max(probs)\n",
    "    return crel2max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "()\n",
      "(1,)\n",
      "(2,)\n",
      "(3,)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(2, 3)\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_all_combos(items):\n",
    "    # enforces a consistent ordering for the resulting tuples\n",
    "    items = sorted(items) \n",
    "    cbos = [()] # seed with the empty combo\n",
    "    for i in range(1, len(items)+1):\n",
    "        cbos.extend(combinations(items,i))\n",
    "    return cbos\n",
    "\n",
    "cbos = get_all_combos([3,2,1])\n",
    "print(len(cbos)) # 2**len(items)-1\n",
    "if len(cbos) < 1000:\n",
    "    for cbo in sorted(cbos, key = lambda l: (len(l), l)):\n",
    "        print(cbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Parses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(), ('1->2', '10->12', '12->50')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_parse(lst):\n",
    "    return tuple(sorted(lst))\n",
    "\n",
    "def sample_top_parses(crel2maxprobs, top_n):\n",
    "\n",
    "    max_parses = 2**len(crel2maxprobs) # maximum parse combinations\n",
    "    assert max_parses > top_n, (max_parses, top_n) # otherwise brute force it\n",
    "\n",
    "    top_parses = set([()]) # always seed with the empty parse\n",
    "    probs = []\n",
    "    while len(top_parses) < top_n:\n",
    "        new_parse = []\n",
    "        for crel, prob in crel2maxprobs.items():\n",
    "            rand_val = np.random.random() # random number >= 0 and < 1\n",
    "            if rand_val < prob:\n",
    "                new_parse.append(crel)\n",
    "        # make hashable and enforce consistent order\n",
    "        top_parses.add(to_parse(new_parse))\n",
    "    \n",
    "    return list(top_parses)\n",
    "\n",
    "def get_top_parses(crel2maxprobs, threshold=0.5):\n",
    "    top_parse = [crel for crel, prob in crel2maxprobs.items() if prob >= threshold]\n",
    "    if top_parse:\n",
    "        return [()] + [to_parse(top_parse)]\n",
    "    else:\n",
    "        return [()]\n",
    "    \n",
    "def get_top_n_parses(crel2maxprobs, top_n):\n",
    "    top_parses = [()]\n",
    "    by_prob = sorted(crel2maxprobs.keys(), key = lambda k: -crel2maxprobs[k])\n",
    "    for i in range(1, min(top_n, len(crel2maxprobs))+1):\n",
    "        parse = by_prob[:i]\n",
    "        top_parses.append(to_parse(parse))\n",
    "    return top_parses\n",
    "\n",
    "def get_top_n_parses2(crel2maxprobs, top_n):\n",
    "    top_parses = [()]\n",
    "    by_prob = sorted(crel2maxprobs.keys(), key = lambda k: -crel2maxprobs[k])\n",
    "    num_predicted = len([crel for crel in by_prob if crel2maxprobs[crel] >= 0.5])\n",
    "    for i in range(num_predicted-1, len(by_prob)+1):\n",
    "        parse = by_prob[:i]\n",
    "        top_parses.append(to_parse(parse))\n",
    "        if len(top_parses) > top_n:\n",
    "            break\n",
    "    return top_parses\n",
    "\n",
    "crel_probs = {\n",
    "    \"1->2\":   0.8,\n",
    "    \"2->3\":   0.01,\n",
    "    \"5->8\":   0.25,\n",
    "    \"10->12\": 0.75,\n",
    "    \"12->50\": 0.99,\n",
    "    \"3->4\":   0.50,\n",
    "}\n",
    "\n",
    "# important - should see a lot more of the more probable codes\n",
    "# sample_top_parses(crel_probs, 8)\n",
    "get_top_n_parses2(crel_probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NgramGenerator import compute_ngrams\n",
    "\n",
    "def to_short_tag(tag):\n",
    "    return tag.replace(\"Causer:\",\"\").replace(\"Result:\", \"\")\n",
    "\n",
    "def build_chains_inner(tree, l, visited, depth=0):\n",
    "    chains = []\n",
    "    if l not in tree:\n",
    "        return chains\n",
    "    for r in tree[l]:\n",
    "        if r in visited:\n",
    "            continue\n",
    "        visited.add(r) # needed to prevent cycles, which cause infinite recursion\n",
    "        extensions = build_chains_inner(tree, r, visited, depth+1)\n",
    "        visited.remove(r)\n",
    "        for ch in extensions:\n",
    "            chains.append([r] + ch)\n",
    "        if not extensions:\n",
    "            chains.append([r])\n",
    "    return chains\n",
    "\n",
    "def build_chains(tree):    \n",
    "    lhs_items = set(tree.keys())\n",
    "    rhs_items = set()\n",
    "    for l,rhs in tree.items():        \n",
    "        rhs_items.update(rhs)\n",
    "    \n",
    "    chains = []\n",
    "    # starting positions of each chain are those appearing on the lhs but not the rhs\n",
    "    start_codes = lhs_items - rhs_items    \n",
    "    for l in start_codes:\n",
    "        rhs = tree[l]\n",
    "        for r in rhs:\n",
    "            for ch in build_chains_inner(tree, r, {l,r}, 0):\n",
    "                chains.append([l,r] + ch)\n",
    "    return chains\n",
    "\n",
    "def extend_chains(chains):\n",
    "    ext_chains = set()\n",
    "    for tokens in chains:\n",
    "        ext_chains.add(\",\".join(tokens))\n",
    "        ngrams = compute_ngrams(tokens,max_len=None, min_len=3)\n",
    "        for t in ngrams:\n",
    "            ext_chains.add(\",\".join(t))\n",
    "    return ext_chains\n",
    "\n",
    "def extract_features_from_parse(parse, crel2probs):\n",
    "    \n",
    "    feats = defaultdict(float)\n",
    "    tree = defaultdict(set) # maps causers to effects for building chains\n",
    "    max_probs = []    \n",
    "    code_tally = defaultdict(float)\n",
    "    ce_tally = defaultdict(float)\n",
    "    \n",
    "    pairs = set()\n",
    "    inverted_count = 0\n",
    "    for crel in parse:\n",
    "        probs = crel2probs[crel]\n",
    "        max_p = max(probs)\n",
    "        max_probs.append(max_p)\n",
    "        feats[\"{crel}-MAX(prob)\".format(crel=crel)] = max_p\n",
    "        feats[\"{crel}-MIN(prob)\".format(crel=crel)] = min(probs)\n",
    "        feats[\"{crel}-pred-count\".format(crel=crel)] = len(probs)\n",
    "        feats[\"{crel}-pred-count={count}\".format(crel=crel, count=len(probs))] = 1\n",
    "        \n",
    "        # with type\n",
    "        l,r = crel.split(\"->\")\n",
    "        ce_tally[l] +=1\n",
    "        ce_tally[r] +=1\n",
    "        \n",
    "        # without type\n",
    "        l_short, r_short = to_short_tag(l), to_short_tag(r)\n",
    "        code_tally[l_short] +=1\n",
    "        code_tally[r_short] +=1\n",
    "        # ordering of the codes, ignoring the causal direction\n",
    "        feats[l_short + \":\" + r_short] = 1\n",
    "        \n",
    "        # build tree structure so we can retrieve the chains\n",
    "        tree[l_short].add(r_short)\n",
    "        \n",
    "        # track whether the rule exists in the opposite direction\n",
    "        pairs.add((l_short,r_short))\n",
    "        if (r_short,l_short) in pairs:\n",
    "            inverted_count += 1\n",
    "            \n",
    "    if inverted_count:\n",
    "        feats[\"inverted\"] = 1\n",
    "        feats[\"num_inverted\"] = inverted_count\n",
    "    else:\n",
    "        feats[\"not_inverted\"] = 1\n",
    "    \n",
    "    # counts\n",
    "    feats.update(ce_tally)\n",
    "    feats.update(code_tally)\n",
    "    \n",
    "    if len(code_tally) > 0:\n",
    "        max_valency = max(code_tally.values())\n",
    "        feats[\"Max_Valency\"] = max_valency\n",
    "        for i in range(1,4):\n",
    "            feats[\"Max_Valency<={i} = {truth_val}\".format(i=i, truth_val = max_valency <= i)]\n",
    "\n",
    "    if len(ce_tally) > 0:\n",
    "        max_ce_valency = max(ce_tally.values())\n",
    "        feats[\"Max_CE_Valency\"] = max_ce_valency\n",
    "        for i in range(1,4):\n",
    "            feats[\"Max_CE_Valency<={i} = {truth_val}\".format(i=i, truth_val = max_ce_valency <= i)]\n",
    "\n",
    "    diffs = []\n",
    "    num_b4 = 0\n",
    "    num_after = 0\n",
    "    num_same = 0\n",
    "    for l,r in pairs:\n",
    "        lnum = float(l.replace(\"b\",\"\"))\n",
    "        rnum = float(r.replace(\"b\",\"\"))\n",
    "        diffs.append(abs(lnum - rnum))\n",
    "        if rnum > lnum:\n",
    "            num_after +=1\n",
    "        elif lnum > rnum:\n",
    "            num_b4 += 1\n",
    "        else:\n",
    "            num_same += 1\n",
    "        feats[\"Pair:\" + \",\".join((l,r))] = 1\n",
    "        feats[\"Unique_Pair:\" + \",\".join(sorted((l,r)))] = 1\n",
    "\n",
    "    if num_b4 > num_after:\n",
    "        feats[\"More_B4\"] = 1\n",
    "    elif num_after > num_b4:\n",
    "        feats[\"More_After\"] = 1\n",
    "\n",
    "    if num_same > 0:\n",
    "        feats[\"SameToSame\"] = 1\n",
    "        feats[\"NumSame=\" + str(num_same)] = 1\n",
    "\n",
    "    if len(diffs) > 0:\n",
    "        feats[\"avg-diff\"] = np.mean(diffs)\n",
    "        feats[\"med-diff\"] = np.median(diffs)\n",
    "        feats[\"min-diff\"] = np.min(diffs)\n",
    "        feats[\"max-diff\"] = np.max(diffs)\n",
    "    \n",
    "    num_crels = len(parse)\n",
    "    feats[\"num_crels\"] = num_crels\n",
    "    feats[\"num_crels=\"+str(len(parse))] = 1 # includes a tag for the empty parse\n",
    "    for i in range(1,11):\n",
    "        if num_crels <= i:\n",
    "            feats[\"num_crels<={i}\".format(i=i)] = 1\n",
    "        else:\n",
    "            feats[\"num_crels>{i}\".format(i=i)] = 1\n",
    "        \n",
    "    # combination of crels\n",
    "    # need to sort so that order of a and b is consistent across parses\n",
    "    cbo_pairs = combinations(sorted(parse), r=2)\n",
    "    for a, b in cbo_pairs:\n",
    "        feats[\"{a}|{b}\".format(a=a, b=b)] = 1\n",
    "        \n",
    "    #chains\n",
    "    causer_chains = extend_chains(build_chains(tree))\n",
    "    max_ch_len = 0\n",
    "    for ch in causer_chains:\n",
    "        feats[\"CChain:\" + ch] = 1\n",
    "        max_ch_len = max(max_ch_len, len(ch.split(\",\")))\n",
    "    \n",
    "    if max_ch_len > 0:\n",
    "        feats[\"Max_Chain_Len\"] = max_ch_len\n",
    "    feats[\"Max_Chain_Len=\" + str(max_ch_len)] = 1\n",
    "    \n",
    "    if max_probs: # might be an empty parse\n",
    "        for cutoff in [0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95]:\n",
    "            above =  len([p for p in max_probs if p >=cutoff])\n",
    "            feats[\"Above-{cutoff}\".format(cutoff=cutoff)] = above\n",
    "            feats[\"%-Above-{cutoff}\".format(cutoff=cutoff)] = above/len(max_probs)\n",
    "            if above == len(max_probs):\n",
    "                feats[\"All-Above-{cutoff}\".format(cutoff=cutoff)] = 1\n",
    "        \n",
    "        feats[\"avg-prob\"] = np.mean(max_probs)\n",
    "        feats[\"med-prob\"] = np.median(max_probs)\n",
    "        feats[\"prod-prob\"]= np.product(max_probs)\n",
    "        feats[\"min-prob\"] = np.min(max_probs)\n",
    "        feats[\"max-prob\"] = np.max(max_probs)\n",
    "        for p in [5, 10, 25, 75, 90, 95]:\n",
    "            feats[\"{p}%-prob\".format(p=p)] = np.percentile(max_probs, p)\n",
    "        # geometric mean\n",
    "        feats[\"geo-mean\"] = np.prod(max_probs)**(1/len(max_probs))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = pred_tagged_essays_train + pred_tagged_essays_test\n",
    "name2essay = {}\n",
    "for essay in all_essays:\n",
    "    name2essay[essay.name] = essay\n",
    "    \n",
    "name2crels = essay_to_crels(all_essays)\n",
    "assert len(name2crels) == len(all_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_costs(parser_input):\n",
    "    opt_parse = parser_input.opt_parse\n",
    "    other_parses = parser_input.other_parses\n",
    "\n",
    "    other_costs = []\n",
    "    op = set(opt_parse)\n",
    "    for p in other_parses:\n",
    "        p = set(p)\n",
    "        fp = p - op\n",
    "        fn = op - p\n",
    "        cost = len(fp) + len(fn)\n",
    "        other_costs.append(cost)\n",
    "    return other_costs\n",
    "\n",
    "def copy_dflt_dict(d):\n",
    "    copy = defaultdict(d.default_factory)\n",
    "    copy.update(d)\n",
    "    return copy\n",
    "\n",
    "class ParserInputs(object):\n",
    "    def __init__(self, essay_name, opt_parse, all_parses, crel2probs, compute_feats=True):\n",
    "        self.essay_name = essay_name\n",
    "        self.opt_parse = opt_parse\n",
    "        self.crel2probs = crel2probs\n",
    "        \n",
    "        if compute_feats:\n",
    "            self.opt_features = extract_features_from_parse(opt_parse, crel2probs)\n",
    "            \n",
    "            other_parses = []\n",
    "            other_feats_array = []\n",
    "            all_feats_array = []\n",
    "            for p in all_parses:\n",
    "                feats = extract_features_from_parse(p, crel2probs)\n",
    "                all_feats_array.append(feats)\n",
    "                if p != opt_parse:\n",
    "                    other_parses.append(p)\n",
    "                    other_feats_array.append(feats)\n",
    "\n",
    "            self.all_feats_array = all_feats_array\n",
    "            self.other_parses = other_parses\n",
    "            self.other_features_array = other_feats_array\n",
    "            self.other_costs_array = compute_costs(self)\n",
    "                    \n",
    "        self.all_parses = all_parses\n",
    "        \n",
    "    def clone_without_feats(self):\n",
    "        c = ParserInputs(essay_name=self.essay_name, opt_parse=self.opt_parse, \n",
    "                         all_parses=self.all_parses, crel2probs=self.crel2probs, compute_feats=False)\n",
    "        \n",
    "        c.other_parses = self.other_parses\n",
    "        c.other_costs_array = self.other_costs_array\n",
    "        return c\n",
    "\n",
    "    def clone(self):\n",
    "        c = ParserInputs(essay_name=self.essay_name, opt_parse=self.opt_parse, \n",
    "                         all_parses=self.all_parses, crel2probs=self.crel2probs, compute_feats=False)\n",
    "        \n",
    "        c.all_feats_array = [copy_dflt_dict(f) for f in self.all_feats_array]\n",
    "        c.opt_features = copy_dflt_dict(self.opt_features)\n",
    "        c.other_parses = self.other_parses\n",
    "        c.other_features_array = [copy_dflt_dict(f) for f in self.other_features_array]\n",
    "        c.other_costs_array = self.other_costs_array\n",
    "        return c\n",
    "\n",
    "def to_freq_feats(feats, freq_feats):\n",
    "    new_feats = defaultdict(float)\n",
    "    for f, v in feats.items():\n",
    "        if f in freq_feats:\n",
    "            new_feats[f] = v\n",
    "    return new_feats\n",
    "\n",
    "def filter_by_min_freq(xs, feat_freq, min_freq):\n",
    "    if min_freq <= 1:\n",
    "        return xs\n",
    "    freq_feats = set((f for f, cnt in feat_freq.items() if cnt >= min_freq))\n",
    "    for parser_input in xs:\n",
    "        parser_input.opt_features = to_freq_feats(parser_input.opt_features, freq_feats)\n",
    "        parser_input.other_features_array = [to_freq_feats(x, freq_feats)\n",
    "                                             for x in parser_input.other_features_array]\n",
    "    return xs\n",
    "\n",
    "def accumulate_feat_vals(xs_train):\n",
    "    def merge_feats(feats):\n",
    "        for ft,val in feats.items():\n",
    "            fts_vals[ft].append(val)\n",
    "    \n",
    "    fts_vals = defaultdict(list)\n",
    "    cnt = 0\n",
    "    for parser_input in xs_train:\n",
    "        cnt+=1\n",
    "        merge_feats(parser_input.opt_features)\n",
    "        for x in parser_input.other_features_array:\n",
    "            cnt+=1\n",
    "            merge_feats(x)\n",
    "    return fts_vals, cnt\n",
    "\n",
    "def z_score_normalize_feats(xs_train, xs_test):\n",
    "    fts_vals, cnt = accumulate_feat_vals(xs_train)\n",
    "    \n",
    "    fts_mean, fts_std = dict(), dict()\n",
    "    for ft, vals in fts_vals.items():\n",
    "        v_with_zeros = vals + ([0] * (cnt-len(vals)))\n",
    "        std = np.std(v_with_zeros)\n",
    "        if std == 0.0:\n",
    "            fts_mean[ft] = 0\n",
    "            fts_std[ft] = vals[0]\n",
    "        else:\n",
    "            fts_mean[ft] = np.mean(v_with_zeros)\n",
    "            fts_std[ft] =  np.std(v_with_zeros)\n",
    "    \n",
    "    def to_z_score(fts):\n",
    "        new_fts = defaultdict(fts.default_factory)\n",
    "        for ft, val in fts.items():\n",
    "            if ft in fts_mean:\n",
    "                new_val = (val - fts_mean[ft])/fts_std[ft]\n",
    "                if new_val:\n",
    "                    new_fts[ft] = new_val\n",
    "        return new_fts\n",
    "    \n",
    "    def z_score_normalize(parser_input):\n",
    "        clone = parser_input.clone_without_feats()\n",
    "        clone.opt_features = to_z_score(parser_input.opt_features)\n",
    "        clone.all_feats_array = [to_z_score(x) for x in parser_input.all_feats_array]\n",
    "        clone.other_features_array = [to_z_score(x) for x in parser_input.other_features_array]\n",
    "        return clone\n",
    "    \n",
    "    new_xs_train = [z_score_normalize(x) for x in xs_train]\n",
    "    new_xs_test  = [z_score_normalize(x) for x in xs_test]\n",
    "    return new_xs_train, new_xs_test\n",
    "\n",
    "def min_max_normalize_feats(xs_train, xs_test):\n",
    "    fts_vals, cnt = accumulate_feat_vals(xs_train)\n",
    "    \n",
    "    fts_min, fts_range = dict(), dict()\n",
    "    for ft, vals in fts_vals.items():\n",
    "        v_with_zeros = vals + ([0] * (cnt-len(vals)))   \n",
    "        min_val = np.min(v_with_zeros)\n",
    "        range_val = np.max(v_with_zeros) - min_val\n",
    "        fts_min[ft] = min_val\n",
    "        fts_range[ft] = range_val\n",
    "    \n",
    "    def to_min_max_score(fts):\n",
    "        new_fts = defaultdict(fts.default_factory)\n",
    "        for ft, val in fts.items():\n",
    "            if ft in fts_min and fts_range[ft] != 0:\n",
    "                new_val = (val - fts_min[ft])/fts_range[ft]\n",
    "                if new_val:\n",
    "                    new_fts[ft] = new_val\n",
    "        return new_fts\n",
    "    \n",
    "    def min_max_normalize(parser_input):\n",
    "        clone = parser_input.clone_without_feats()\n",
    "        clone.opt_features = to_min_max_score(parser_input.opt_features)\n",
    "        clone.all_feats_array = [to_min_max_score(x) for x in parser_input.all_feats_array]\n",
    "        clone.other_features_array = [to_min_max_score(x) for x in parser_input.other_features_array]\n",
    "        return clone\n",
    "    \n",
    "    new_xs_train = [min_max_normalize(x) for x in xs_train]\n",
    "    new_xs_test  = [min_max_normalize(x) for x in xs_test]\n",
    "    return new_xs_train, new_xs_test\n",
    "\n",
    "def get_crels_above(crel2maxprob, threshold):\n",
    "    return [k for k, p in crel2maxprob.items() if p >= threshold]\n",
    "\n",
    "def get_features_from_probabilities(essay2probs, top_n, min_feat_freq=1, min_prob=0.0):\n",
    "    xs = []\n",
    "    feat_freq = defaultdict(int)\n",
    "    \n",
    "    for ename, crel2probs in essay2probs.items():\n",
    "\n",
    "        act_crels = name2crels[ename]\n",
    "        crel2maxprob = get_max_probs(crel2probs)        \n",
    "        crel2probs = dict(crel2probs)\n",
    "        \n",
    "        keys = list(crel2probs.keys())\n",
    "        n_parses = 2 ** len(keys)\n",
    "        \n",
    "        increment = 0.05\n",
    "        threshold = min_prob - increment\n",
    "        while n_parses > 2 * top_n and threshold < 1.0:\n",
    "            threshold += increment\n",
    "            keys = get_crels_above(crel2maxprob, threshold)\n",
    "            n_parses = 2 ** len(keys)\n",
    "\n",
    "        if n_parses >  2 * top_n:\n",
    "            print(\"n_parses={n_parses} still exceeded max={max_p} at p={p:.4f}\".format(\n",
    "                p=threshold, n_parses=n_parses, max_p=top_n))\n",
    "            parses = get_top_parses(crel2maxprob)\n",
    "        else:\n",
    "            parses = get_all_combos(keys)\n",
    "\n",
    "        # constrain optimal parse to only those crels that are predicted\n",
    "        opt_parse = tuple(sorted(act_crels.intersection(crel2probs.keys())))\n",
    "        x = ParserInputs(essay_name=ename, opt_parse=opt_parse, all_parses=parses, crel2probs=crel2probs)\n",
    "        xs.append(x)\n",
    "\n",
    "        # Get unique features for essay\n",
    "        all_feats = set()\n",
    "        for fts in x.all_feats_array:\n",
    "            all_feats.update(fts.keys())\n",
    "\n",
    "        for ft in all_feats:\n",
    "            feat_freq[ft] += 1\n",
    "\n",
    "    assert len(xs) == len(essay2probs), \"Parses for all essays should be generated\"\n",
    "    return filter_by_min_freq(xs, feat_freq, min_feat_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cr_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "            \n",
    "def evaluate_ranker(model, xs, essay2crels, ys_bytag):\n",
    "    clone = model.clone()\n",
    "    if hasattr(model, \"average_weights\"):\n",
    "        clone.average_weights()\n",
    "\n",
    "    pred_ys_bytag = defaultdict(list)\n",
    "    ename2inps = dict()\n",
    "    for parser_input in xs:\n",
    "        ename2inps[parser_input.essay_name] = parser_input\n",
    "    \n",
    "    for ename, act_crels in essay2crels.items():        \n",
    "        if ename not in ename2inps:\n",
    "            # no predicted crels for this essay\n",
    "            highest_ranked = set()\n",
    "        else:\n",
    "            parser_input = ename2inps[ename]\n",
    "            ixs = clone.rank(parser_input.all_feats_array)\n",
    "            highest_ranked = parser_input.all_parses[ixs[0]] # type: Tuple[str]        \n",
    "            \n",
    "        add_cr_labels(set(highest_ranked), pred_ys_bytag)\n",
    "\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag, pred_ys_bytag)\n",
    "    df = get_micro_metrics(metrics_to_df(mean_metrics))\n",
    "    return df\n",
    "\n",
    "def get_ranked_predictions(model, xs):\n",
    "    clone = model.clone()\n",
    "    if hasattr(model, \"average_weights\"):\n",
    "        clone.average_weights()\n",
    "        \n",
    "    preds_by_essay = dict()\n",
    "    for parser_input in xs:\n",
    "        ixs = clone.rank(parser_input.all_feats_array)\n",
    "        preds_by_essay[parser_input.essay_name] = (parser_input, ixs)        \n",
    "    return preds_by_essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "def train_instance(parser_input, model):\n",
    "    model.train(best_feats=parser_input.opt_features, other_feats_array=parser_input.other_features_array)\n",
    "\n",
    "def train_cost_sensitive_instance(parser_input, model):\n",
    "    model.train(best_feats=parser_input.opt_features, \n",
    "                other_feats_array=parser_input.other_features_array, other_costs_array=parser_input.other_costs_array)\n",
    "    \n",
    "def get_essays_for_data(xs):\n",
    "    return [name2essay[x.essay_name] for x in xs]\n",
    "    \n",
    "def train_model(model, xs_train, xs_test, max_epochs=30, early_stop_iters=8, train_instance_fn=train_instance, verbose=True):\n",
    "    test_accs = [-1]\n",
    "    best_model = None\n",
    "    best_test_accuracy = None\n",
    "    num_declining_acc = 0\n",
    "\n",
    "    train_essays = get_essays_for_data(xs_train)\n",
    "    test_essays  = get_essays_for_data(xs_test)\n",
    "\n",
    "    ys_by_tag_train = get_label_data_essay_level(train_essays)\n",
    "    ys_by_tag_test  = get_label_data_essay_level(test_essays)\n",
    "\n",
    "    essay2crels_train = essay_to_crels(train_essays)\n",
    "    essay2crels_test  = essay_to_crels(test_essays)\n",
    "    \n",
    "    xs_train_copy = list(xs_train)    \n",
    "    for i in range(max_epochs):\n",
    "        shuffle(xs_train_copy)\n",
    "        for parser_input in xs_train_copy:\n",
    "            if len(parser_input.other_parses) > 0:\n",
    "                train_instance_fn(parser_input, model)\n",
    "\n",
    "        train_accuracy_df = evaluate_ranker(model, xs_train, essay2crels_train, ys_by_tag_train)\n",
    "        test_accuracy_df  = evaluate_ranker(model, xs_test,  essay2crels_test,  ys_by_tag_test)\n",
    "        train_accuracy = train_accuracy_df.iloc[0].to_dict()[\"f1_score\"]\n",
    "        test_accuracy  = test_accuracy_df.iloc[0].to_dict()[\"f1_score\"]\n",
    "        if verbose:\n",
    "            print(\"Epoch: {epoch} Train Accuracy: {train_acc:.4f} Test Accuracy: {test_acc:.4f}\".format(\n",
    "            epoch=i,  train_acc=train_accuracy, test_acc=test_accuracy))\n",
    "        if test_accuracy > max(test_accs):\n",
    "            best_model = model.clone()\n",
    "            best_test_accuracy = test_accuracy_df\n",
    "            num_declining_acc = 0\n",
    "        else:\n",
    "            num_declining_acc += 1\n",
    "            if num_declining_acc >= early_stop_iters:\n",
    "                break\n",
    "        test_accs.append(test_accuracy)\n",
    "    if verbose:\n",
    "        print(\"Best Test Acc: {acc:.4f}\".format(acc=max(test_accs)))\n",
    "    return best_model, best_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_essays2crels(essays, sr_model, top_n, search_mode_max_prob=False):\n",
    "    trainessay2probs = defaultdict(list)\n",
    "    for eix, essay in enumerate(essays):\n",
    "        crel2probs = defaultdict(list)        \n",
    "        for sent_ix, taggged_sentence in enumerate(essay.sentences):\n",
    "            predicted_tags = essay.pred_tagged_sentences[sent_ix]\n",
    "            unq_ptags = set([t for t in predicted_tags if t != EMPTY])            \n",
    "            if len(unq_ptags) >= 2:\n",
    "                pred_parses = sr_model.generate_all_potential_parses_for_sentence(\n",
    "                    tagged_sentence=taggged_sentence, predicted_tags=predicted_tags, top_n=top_n, search_mode_max_prob=search_mode_max_prob)\n",
    "                cr2p = collapse_sent_parse(pred_parses)\n",
    "                merge_crel_probs(crel2probs, cr2p)\n",
    "    \n",
    "        if len(crel2probs) > 0:\n",
    "            trainessay2probs[essay.name] = dict(crel2probs)\n",
    "        else:\n",
    "            trainessay2probs[essay.name] = dict()\n",
    "    return trainessay2probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_to_crels_cv(cv_folds, models, top_n, search_mode_max_prob=False):\n",
    "    essay2crelprobs = defaultdict(list)\n",
    "    assert len(cv_folds) == len(models)\n",
    "    for (train, test), mdl in zip(cv_folds, models):\n",
    "        test2probs = get_essays2crels(test, mdl, top_n, search_mode_max_prob)\n",
    "        for k,v in test2probs.items():\n",
    "            assert k not in essay2crelprobs\n",
    "            essay2crelprobs[k] = v\n",
    "    return essay2crelprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split_dict(dct, train_pct):\n",
    "    items = list(dct.items())\n",
    "    np.random.shuffle(items)\n",
    "    num_train = int(len(items) * train_pct)\n",
    "    train_items, test_items = items[:num_train], items[num_train:]\n",
    "    return dict(train_items), dict(test_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_fold(xs_train, xs_test, C, pa_type, loss_type, max_update_items, return_model=False):\n",
    "    \n",
    "    mdl = CostSensitiveMIRA(C=C, pa_type=pa_type, loss_type=loss_type, max_update_items=max_update_items, initial_weight=1)\n",
    "    best_mdl, test_acc_df_ml = train_model(mdl, xs_train=xs_train, xs_test=xs_test, \n",
    "        max_epochs=20, early_stop_iters=5, train_instance_fn = train_cost_sensitive_instance, verbose=False)\n",
    "    f1 = test_acc_df_ml[\"f1_score\"].values[0]\n",
    "    if return_model:\n",
    "        return f1, best_mdl\n",
    "    return f1\n",
    "\n",
    "def train_model_parallel(cv_folds, C, pa_type, loss_type, max_update_items):\n",
    "    try:\n",
    "        f1s = Parallel(n_jobs=len(cv_folds))(delayed(train_model_fold)(train,test, C, pa_type, loss_type, max_update_items) \n",
    "                                         for (train,test) in cv_folds)\n",
    "        return np.mean(f1s)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process stopped by user\")\n",
    "        \n",
    "def train_model_serial(cv_folds, C, pa_type, loss_type, max_update_items):\n",
    "    try:\n",
    "        f1s = [train_model_fold(train,test, C, pa_type, loss_type, max_update_items) \n",
    "                for (train,test) in cv_folds]\n",
    "        return np.mean(f1s)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process stopped by user\")\n",
    "        \n",
    "def train_model_return_model_parallel(cv_folds, C, pa_type, loss_type, max_update_items):\n",
    "    try:\n",
    "        res = Parallel(n_jobs=len(cv_folds))(delayed(train_model_fold)(\n",
    "            train,test, C, pa_type, loss_type, max_update_items, True) \n",
    "                                         for (train,test) in cv_folds)\n",
    "        return res\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Process stopped by user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to re-implement as assumes a parser input obj with other parses and we need for all parses\n",
    "def get_costs(opt_parse, other_parses):\n",
    "    other_costs = []\n",
    "    op = set(opt_parse)\n",
    "    for p in other_parses:\n",
    "        p = set(p)\n",
    "        fp = p - op\n",
    "        fn = op - p\n",
    "        cost = len(fp) + len(fn)\n",
    "        other_costs.append(cost)\n",
    "    return other_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Optimal Parameters and MM Nornalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial settings for other params\n",
    "best_top_n, best_C, best_max_upd, best_max_parses, best_min_prob = (2, 0.0025, 2, 300, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 11.7 s, total: 1min 59s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xs_rerank = essay_to_crels_cv(cv_folds, models, top_n=best_top_n, search_mode_max_prob=False)\n",
    "xs = get_features_from_probabilities(xs_rerank, best_max_parses, min_feat_freq=1, min_prob=best_min_prob)\n",
    "\n",
    "cv_folds_rerank = cross_validation(xs, 5)\n",
    "cv_folds_mm = [min_max_normalize_feats(train,test) for (train,test) in cv_folds_rerank]\n",
    "# Parallelizing this takes longer as it uses a lot of RAM\n",
    "#     cf_folds_mm = Parallel(n_jobs=len(cv_folds_rerank))(delayed(min_max_normalize_feats)(train,test) for (train,test) in cv_folds_rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 902, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs_rerank), len(xs), len(cv_folds_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7414964790306262\n",
      "CPU times: user 1min 54s, sys: 23.6 s, total: 2min 18s\n",
      "Wall time: 30min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1 = train_model_parallel(cv_folds=cv_folds_mm, C=best_C, pa_type=1, loss_type=\"ml\", max_update_items=best_max_upd)\n",
    "print(f1) # 0.74159216523265383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7414964790306262\n",
      "CPU times: user 10min 4s, sys: 3.72 s, total: 10min 8s\n",
      "Wall time: 10min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Parallel is faster but not 5X\n",
    "f1 = train_model_serial(cv_folds=cv_folds_mm, C=best_C, pa_type=1, loss_type=\"ml\", max_update_items=best_max_upd)\n",
    "print(f1) # 0.74159216523265383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = train_model_return_model_parallel(cv_folds=cv_folds_mm, C=best_C, pa_type=1, loss_type=\"ml\", max_update_items=best_max_upd)\n",
    "\n",
    "train, test = cv_folds_mm[0]\n",
    "f1, mdl = train_model_fold(train, test, C=best_C, pa_type=1, loss_type=\"ml\", max_update_items=best_max_upd, return_model=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = get_ranked_predictions(mdl, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ename2costs = dict()\n",
    "for ename, (pi, ixs) in preds.items():\n",
    "    all_costs = get_costs(pi.opt_parse, pi.all_parses)\n",
    "    assert len(all_costs) == len(ixs)\n",
    "    assert ename not in ename2costs, \"Essay already processed: \" + ename\n",
    "    ename2costs[ename] = all_costs[ixs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_essays = get_essays_for_data(train)\n",
    "essay2crels_train = essay_to_crels(train_essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute essay prior probability of crel from training data\n",
    "tally_train_crel = defaultdict(int)\n",
    "for ename, crels in essay2crels_train.items():\n",
    "    for crel in crels:\n",
    "        tally_train_crel[crel] += 1\n",
    "\n",
    "ptrain_crels = defaultdict(float)\n",
    "for crel, cnt in tally_train_crel.items():\n",
    "    ptrain_crels[crel] = cnt / len(train)\n",
    "        \n",
    "# ptrain_crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 EBA1415_KNKC_3_CB_ES-05599.ann\n",
      "('Causer:3->Result:4', 'Causer:6->Result:7')\n",
      "7 FP\n",
      "\t Causer:1->Result:2   0.1068 \t [0.751152411174496, 0.751152411174496]\n",
      "\t Causer:11->Result:12 0.09154 \t [0.29981777519812597, 0.29981777519812597]\n",
      "\t Causer:11->Result:13 0.09015 \t [0.637589255042635, 0.637589255042635, 0.7598836528356077, 0.7598836528356077]\n",
      "\t Causer:11->Result:3  0.00555 \t [0.49973330151496564, 0.49973330151496564]\n",
      "\t Causer:5->Result:5b  0.03606 \t [0.6512346037728104, 0.6512346037728104]\n",
      "\t Causer:1->Result:7   0.00416 \t [0.6623580380462014, 0.6623580380462014]\n",
      "\t Causer:1->Result:3   0.19556 \t [0.6700779538942299, 0.6700779538942299]\n",
      "0 FN:\n",
      "2 TP:\n",
      "\t Causer:3->Result:4   0.14147 \t [0.7774053713797013, 0.7774053713797013]\n",
      "\t Causer:6->Result:7   0.14286 \t [0.9848136734459447, 0.9848136734459447]\n",
      "************************************************************************************************************************\n",
      "6 EBA1415_BGJD_2_CB_ES-05751.ann\n",
      "('Causer:3->Result:4', 'Causer:4->Result:14', 'Causer:4->Result:5')\n",
      "6 FP\n",
      "\t Causer:7->Result:50  0.37864 \t [0.8167303614146069, 0.8167303614146069]\n",
      "\t Causer:1->Result:50  0.28017 \t [0.7447325495372833, 0.7447325495372833, 0.9063801098821945, 0.9063801098821945]\n",
      "\t Causer:1->Result:3   0.19556 \t [0.8366692837144308, 0.8366692837144308]\n",
      "\t Causer:6->Result:14  0.05409 \t [0.9411877665307637, 0.9411877665307637]\n",
      "\t Causer:3->Result:50  0.2552 \t [0.941756719318, 0.941756719318]\n",
      "\t Causer:14->Result:50 0.07074 \t [0.6162072779758583, 0.6162072779758583]\n",
      "0 FN:\n",
      "3 TP:\n",
      "\t Causer:4->Result:5   0.0749 \t [0.9489879703458531, 0.9489879703458531]\n",
      "\t Causer:3->Result:4   0.14147 \t [0.9710905280147191, 0.9710905280147191]\n",
      "\t Causer:4->Result:14  0.0846 \t [0.9666737215846163, 0.9666737215846163]\n",
      "************************************************************************************************************************\n",
      "5 EBA1415_BLHT_5_CB_ES-05202.ann\n",
      "('Causer:1->Result:50', 'Causer:11->Result:12', 'Causer:12->Result:13', 'Causer:13->Result:50', 'Causer:3->Result:50', 'Causer:7->Result:50')\n",
      "4 FP\n",
      "\t Causer:11->Result:13 0.09015 \t [0.740518062524048, 0.740518062524048]\n",
      "\t Causer:6->Result:14  0.05409 \t [0.9411877665307637, 0.9411877665307637]\n",
      "\t Causer:14->Result:50 0.07074 \t [0.9735146340994058, 0.9735146340994058]\n",
      "\t Causer:13->Result:14 0.10541 \t [0.7811595235770563, 0.7811595235770563]\n",
      "1 FN:\n",
      "\t Causer:7->Result:50  0.37864 \t [0.6361864597911737, 0.6361864597911737]\n",
      "5 TP:\n",
      "\t Causer:11->Result:12 0.09154 \t [0.9575430943810317, 0.9575430943810317]\n",
      "\t Causer:1->Result:50  0.28017 \t [0.9459682921240022, 0.9459682921240022, 0.9795005838764519, 0.9795005838764519]\n",
      "\t Causer:13->Result:50 0.09015 \t [0.9712209491353917, 0.9712209491353917]\n",
      "\t Causer:12->Result:13 0.0846 \t [0.9628086134248481, 0.9628086134248481]\n",
      "\t Causer:3->Result:50  0.2552 \t [0.3499207665082301, 0.3499207665082301, 0.6019142067304356, 0.6019142067304356, 0.8500736899428886, 0.8500736899428886, 0.9244729937547619, 0.9244729937547619, 0.9725074856729085, 0.9725074856729085]\n",
      "************************************************************************************************************************\n",
      "4 EBA1415_BLHT_5_CB_ES-05204.ann\n",
      "('Causer:1->Result:50', 'Causer:3->Result:7', 'Causer:7->Result:50')\n",
      "4 FP\n",
      "\t Causer:4->Result:5   0.0749 \t [0.7433539546059617, 0.7433539546059617]\n",
      "\t Causer:3->Result:4   0.14147 \t [0.8738491511000412, 0.8738491511000412]\n",
      "\t Causer:11->Result:7  0.0 \t [0.9169974748520004, 0.9169974748520004]\n",
      "\t Causer:11->Result:3  0.00555 \t [0.7574728856701544, 0.7574728856701544]\n",
      "0 FN:\n",
      "3 TP:\n",
      "\t Causer:7->Result:50  0.37864 \t [0.7598180406687529, 0.7598180406687529]\n",
      "\t Causer:1->Result:50  0.28017 \t [0.7760232518258964, 0.7760232518258964, 0.9564377849087493, 0.9564377849087493]\n",
      "\t Causer:3->Result:7   0.02635 \t [0.9253755830446466, 0.9253755830446466]\n",
      "************************************************************************************************************************\n",
      "4 EBA1415_BLHT_5_CB_ES-05207.ann\n",
      "('Causer:11->Result:13', 'Causer:11->Result:3', 'Causer:3->Result:4')\n",
      "4 FP\n",
      "\t Causer:6->Result:50  0.06103 \t [0.2853822478806195, 0.2853822478806195]\n",
      "\t Causer:3->Result:13  0.00277 \t [0.7218802046969639, 0.7218802046969639]\n",
      "\t Causer:14->Result:50 0.07074 \t [0.5704537484319803, 0.5704537484319803]\n",
      "\t Causer:4->Result:50  0.08738 \t [0.8899511317246507, 0.8899511317246507]\n",
      "0 FN:\n",
      "3 TP:\n",
      "\t Causer:11->Result:13 0.09015 \t [0.7234717456982394, 0.7234717456982394]\n",
      "\t Causer:3->Result:4   0.14147 \t [0.888437389742601, 0.888437389742601]\n",
      "\t Causer:11->Result:3  0.00555 \t [0.4034452734975119, 0.4034452734975119]\n",
      "************************************************************************************************************************\n",
      "4 EBA1415_ERSK_1_CB_ES-05780.ann\n",
      "('Causer:7->Result:50',)\n",
      "4 FP\n",
      "\t Causer:1->Result:2   0.1068 \t [0.7709487700844392, 0.7709487700844392]\n",
      "\t Causer:3->Result:5   0.1165 \t [0.8065382649900148, 0.8065382649900148]\n",
      "\t Causer:5->Result:5b  0.03606 \t [0.42185205461537306, 0.42185205461537306]\n",
      "\t Causer:1->Result:3   0.19556 \t [0.7055526269326206, 0.7055526269326206, 0.7731926557760005, 0.7731926557760005]\n",
      "0 FN:\n",
      "1 TP:\n",
      "\t Causer:7->Result:50  0.37864 \t [0.903760061543109, 0.903760061543109, 0.9359272429980867, 0.9359272429980867]\n",
      "************************************************************************************************************************\n",
      "4 EBA1415_KYLS_5_CB_ES-05647.ann\n",
      "('Causer:1->Result:3', 'Causer:1->Result:50', 'Causer:13->Result:50', 'Causer:3->Result:50', 'Causer:3->Result:7')\n",
      "4 FP\n",
      "\t Causer:7->Result:50  0.37864 \t [0.6340297853179117, 0.6340297853179117, 0.8472165246485813, 0.8472165246485813]\n",
      "\t Causer:13->Result:7  0.00416 \t [0.6824493543879148, 0.6824493543879148]\n",
      "\t Causer:5->Result:50  0.05409 \t [0.6167842732208405, 0.6167842732208405]\n",
      "\t Causer:5b->Result:7  0.00971 \t [0.3650567721099105, 0.3650567721099105]\n",
      "0 FN:\n",
      "5 TP:\n",
      "\t Causer:1->Result:50  0.28017 \t [0.9437832572097646, 0.9437832572097646]\n",
      "\t Causer:13->Result:50 0.09015 \t [0.41887741191038336, 0.41887741191038336, 0.7657975412298877, 0.7657975412298877]\n",
      "\t Causer:1->Result:3   0.19556 \t [0.9726667669939936, 0.9726667669939936]\n",
      "\t Causer:3->Result:7   0.02635 \t [0.7440521675917692, 0.7440521675917692]\n",
      "\t Causer:3->Result:50  0.2552 \t [0.541161672804716, 0.541161672804716, 0.6188666809466963, 0.6188666809466963, 0.7888520047174297, 0.7888520047174297, 0.9148070595463492, 0.9148070595463492, 0.9544543491180374, 0.9544543491180374]\n",
      "************************************************************************************************************************\n",
      "4 EBA1415_KYLS_5_CB_ES-05652.ann\n",
      "('Causer:12->Result:13',)\n",
      "4 FP\n",
      "\t Causer:3->Result:4   0.14147 \t [0.51290897359675, 0.51290897359675]\n",
      "\t Causer:13->Result:14 0.10541 \t [0.7096528552855345, 0.7096528552855345]\n",
      "\t Causer:14->Result:50 0.07074 \t [0.6697090435563785, 0.6697090435563785]\n",
      "\t Causer:4->Result:4   0.0 \t [0.839084583756312, 0.839084583756312]\n",
      "0 FN:\n",
      "1 TP:\n",
      "\t Causer:12->Result:13 0.0846 \t [0.7041687315878103, 0.7041687315878103, 0.8774725902150875, 0.8774725902150875]\n",
      "************************************************************************************************************************\n",
      "4 EBA1415_KYLS_5_CB_ES-05662.ann\n",
      "('Causer:1->Result:2', 'Causer:1->Result:50', 'Causer:3->Result:50', 'Causer:6->Result:7', 'Causer:7->Result:50')\n",
      "4 FP\n",
      "\t Causer:4->Result:50  0.08738 \t [0.8173748331449064, 0.8173748331449064]\n",
      "\t Causer:1->Result:3   0.19556 \t [0.8923015080551514, 0.8923015080551514]\n",
      "\t Causer:13->Result:12 0.00277 \t [0.653418893531621, 0.653418893531621, 0.664412933692179, 0.664412933692179]\n",
      "\t Causer:12->Result:14 0.00416 \t [0.8161647764205396, 0.8161647764205396]\n",
      "0 FN:\n",
      "5 TP:\n",
      "\t Causer:6->Result:7   0.14286 \t [0.9851392035073091, 0.9851392035073091]\n",
      "\t Causer:1->Result:2   0.1068 \t [0.9684523594723297, 0.9684523594723297]\n",
      "\t Causer:7->Result:50  0.37864 \t [0.9575403529177624, 0.9575403529177624]\n",
      "\t Causer:1->Result:50  0.28017 \t [0.8995474767659808, 0.8995474767659808, 0.9363606063067914, 0.9363606063067914, 0.9402141509073207, 0.9402141509073207, 0.9798168010516276, 0.9798168010516276]\n",
      "\t Causer:3->Result:50  0.2552 \t [0.6349111045484779, 0.6349111045484779, 0.8372942715269185, 0.8372942715269185, 0.8779768762421152, 0.8779768762421152, 0.9778279033233437, 0.9778279033233437]\n",
      "************************************************************************************************************************\n",
      "3 EBA1415_BGJD_1_CB_ES-05975.ann\n",
      "('Causer:7->Result:50',)\n",
      "3 FP\n",
      "\t Causer:6->Result:50  0.06103 \t [0.5438950102554848, 0.5438950102554848]\n",
      "\t Causer:6->Result:14  0.05409 \t [0.9093927183233211, 0.9093927183233211]\n",
      "\t Causer:14->Result:50 0.07074 \t [0.989617388772886, 0.989617388772886]\n",
      "0 FN:\n",
      "1 TP:\n",
      "\t Causer:7->Result:50  0.37864 \t [0.9355568420746402, 0.9355568420746402, 0.9888885768831172, 0.9888885768831172]\n",
      "************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for ename, cost in sorted(ename2costs.items(), key = lambda tpl: -tpl[1])[0:10]:\n",
    "    if cost == 0:\n",
    "        continue\n",
    "    (pi, ixs) = preds[ename]\n",
    "    op = set(pi.opt_parse)\n",
    "    highest_ranked = pi.all_parses[ixs[0]]\n",
    "    p = set(highest_ranked)\n",
    "    fp = p - op\n",
    "    fn = op - p\n",
    "    tp = op.intersection(p)\n",
    "    print(cost, ename)\n",
    "    print(pi.opt_parse)\n",
    "    crel2probs = pi.crel2probs\n",
    "    \n",
    "    print(len(fp), \"FP\")\n",
    "    for crel in fp:\n",
    "        print(\"\\t\", crel.ljust(20), round(ptrain_crels[crel],5), \"\\t\", sorted(crel2probs[crel]))\n",
    "    \n",
    "    print(len(fn), \"FN:\")\n",
    "    for crel in fn:\n",
    "        print(\"\\t\", crel.ljust(20), round(ptrain_crels[crel],5), \"\\t\", sorted(crel2probs[crel]))\n",
    "        \n",
    "    print(len(tp), \"TP:\")\n",
    "    for crel in tp:\n",
    "        print(\"\\t\", crel.ljust(20), round(ptrain_crels[crel],5), \"\\t\", sorted(crel2probs[crel]))\n",
    "        \n",
    "    print(\"*\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('num_crels<=4', 1.9249999999999803),\n",
       " ('num_crels<=3', 1.792499999999983),\n",
       " ('num_crels<=2', 1.7899999999999832),\n",
       " ('num_crels<=5', 1.7574999999999839),\n",
       " ('Max_Chain_Len=0', 1.7199999999999847),\n",
       " ('num_crels<=6', 1.5324999999999886),\n",
       " ('not_inverted', 1.517499999999989),\n",
       " ('num_crels<=1', 1.3999999999999915),\n",
       " ('num_crels=2', 1.3899999999999917),\n",
       " ('num_crels<=7', 1.374999999999992),\n",
       " ('All-Above-0.7', 1.3149999999999933),\n",
       " ('prod-prob', 1.2942094262027763),\n",
       " ('num_crels=0', 1.284999999999994),\n",
       " ('All-Above-0.8', 1.279999999999994),\n",
       " ('All-Above-0.9', 1.2449999999999948),\n",
       " ('num_crels=4', 1.1324999999999972),\n",
       " ('All-Above-0.95', 1.1274999999999973),\n",
       " ('num_crels=1', 1.1149999999999975),\n",
       " ('num_crels<=8', 1.1124999999999976),\n",
       " ('All-Above-0.5', 1.1124999999999976),\n",
       " ('%-Above-0.95', 1.0973005952381105),\n",
       " ('%-Above-0.9', 1.0612619047619152),\n",
       " ('min-prob', 1.0287657386023636),\n",
       " ('Causer:12->Result:13|Causer:3->Result:4', 1.0174999999999996),\n",
       " ('CChain:1,3,4,5,7', 1.0149999999999997),\n",
       " ('CChain:3,4,5,7', 1.0149999999999997),\n",
       " ('Causer:11->Result:50|Causer:6->Result:7', 1.0149999999999997),\n",
       " ('Causer:3->Result:5|Causer:3->Result:7', 1.0149999999999997),\n",
       " ('Causer:11->Result:13|Causer:3->Result:4', 1.0149999999999997),\n",
       " ('Causer:11->Result:12|Causer:11->Result:13', 1.0124999999999997),\n",
       " ('Causer:11->Result:12|Causer:6->Result:7', 1.0099999999999998),\n",
       " ('Causer:3->Result:4|Causer:7->Result:50', 1.0099999999999998),\n",
       " ('Causer:3->Result:4|Causer:3->Result:50', 1.0074999999999998),\n",
       " ('Causer:13->Result:50|Causer:6->Result:7', 1.0074999999999998),\n",
       " ('Causer:4->Result:50-pred-count=4', 1.0074999999999998),\n",
       " ('Causer:11->Result:13|Causer:12->Result:13', 1.0074999999999998),\n",
       " ('Causer:1->Result:3|Causer:5b->Result:50', 1.0074999999999998),\n",
       " ('Causer:1->Result:50|Causer:5->Result:5b', 1.0074999999999998),\n",
       " ('Causer:1->Result:2|Causer:1->Result:7', 1.0074999999999998),\n",
       " ('Causer:4->Result:50-pred-count=6', 1.0074999999999998),\n",
       " ('Causer:1->Result:4|Causer:5b->Result:50', 1.0074999999999998),\n",
       " ('Causer:5b->Result:50|Causer:7->Result:50', 1.0074999999999998),\n",
       " ('Causer:11->Result:12|Causer:2->Result:3', 1.0074999999999998),\n",
       " ('Causer:3->Result:5|Causer:4->Result:5b', 1.0074999999999998),\n",
       " ('Causer:4->Result:50|Causer:5->Result:5b', 1.0074999999999998),\n",
       " ('Causer:13->Result:12|Causer:3->Result:1', 1.0074999999999998),\n",
       " ('Causer:11->Result:13|Causer:11->Result:14', 1.0074999999999998),\n",
       " ('Causer:4->Result:50|Causer:4->Result:5b', 1.0074999999999998),\n",
       " ('Causer:1->Result:5|Causer:13->Result:14', 1.0074999999999998),\n",
       " ('Causer:13->Result:14|Causer:5->Result:5b', 1.0074999999999998),\n",
       " ('Causer:3->Result:1|Causer:6->Result:7', 1.0074999999999998),\n",
       " ('Causer:1->Result:4|Causer:11->Result:13', 1.0074999999999998),\n",
       " ('Causer:11->Result:13|Causer:11->Result:3', 1.0074999999999998),\n",
       " ('Causer:3->Result:5|Causer:4->Result:7', 1.0074999999999998),\n",
       " ('Causer:4->Result:3|Causer:6->Result:7', 1.0074999999999998),\n",
       " ('Causer:11->Result:50|Causer:4->Result:7', 1.0074999999999998),\n",
       " ('Causer:13->Result:50|Causer:4->Result:7', 1.0074999999999998),\n",
       " ('Causer:1->Result:14|Causer:6->Result:14', 1.005),\n",
       " ('num_crels=3', 1.0025),\n",
       " ('Causer:1->Result:50|Causer:13->Result:50', 1.0025),\n",
       " ('Causer:11->Result:12|Causer:3->Result:4', 1.0025),\n",
       " ('num_crels<=9', 1.0),\n",
       " ('Causer:3->Result:50|Causer:4->Result:14', 1.0),\n",
       " ('Causer:11->Result:13|Causer:4->Result:14', 1.0),\n",
       " ('Causer:1->Result:2|Causer:11->Result:50', 1.0),\n",
       " ('Causer:1->Result:50|Causer:11->Result:50', 1.0),\n",
       " ('Causer:2->Result:3|Causer:3->Result:7', 1.0),\n",
       " ('CChain:2,3,7', 1.0),\n",
       " ('CChain:2,3,7,50', 1.0),\n",
       " ('Causer:6->Result:7-pred-count=4', 1.0),\n",
       " ('Causer:4->Result:5-pred-count=4', 1.0),\n",
       " ('Causer:3->Result:4|Causer:5->Result:7', 1.0),\n",
       " ('Causer:3->Result:5|Causer:5->Result:7', 1.0),\n",
       " ('CChain:3,5,7', 1.0),\n",
       " ('CChain:1,3,5,7', 1.0),\n",
       " ('Causer:11->Result:50-pred-count=6', 1.0),\n",
       " ('Causer:6->Result:50-pred-count=4', 1.0),\n",
       " ('Causer:1->Result:50|Causer:3->Result:6', 1.0),\n",
       " ('Causer:3->Result:50-pred-count=8', 1.0),\n",
       " ('Causer:11->Result:12|Causer:3->Result:5', 1.0),\n",
       " ('Causer:1->Result:50|Causer:4->Result:7', 1.0),\n",
       " ('Causer:3->Result:5|Causer:5->Result:5b', 1.0),\n",
       " ('Causer:5->Result:5b|Causer:5b->Result:50', 1.0),\n",
       " ('CChain:5,5b,50', 1.0),\n",
       " ('CChain:3,5,5b,50', 1.0),\n",
       " ('CChain:1,3,5,5b', 1.0),\n",
       " ('CChain:1,3,5,5b,50', 1.0),\n",
       " ('CChain:3,5,5b', 1.0),\n",
       " ('Causer:1->Result:3|Causer:3->Result:6', 1.0),\n",
       " ('Causer:13->Result:7|Causer:3->Result:6', 1.0),\n",
       " ('Causer:13->Result:7|Causer:3->Result:7', 1.0),\n",
       " ('Causer:13->Result:7|Causer:6->Result:50', 1.0),\n",
       " ('Causer:3->Result:4|Causer:3->Result:6', 1.0),\n",
       " ('Causer:3->Result:6|Causer:3->Result:7', 1.0),\n",
       " ('Causer:3->Result:7|Causer:6->Result:50', 1.0),\n",
       " ('CChain:1,3,6,50', 1.0),\n",
       " ('CChain:1,3,6', 1.0),\n",
       " ('Causer:1->Result:3-pred-count=8', 1.0),\n",
       " ('Causer:11->Result:13-pred-count=4', 1.0),\n",
       " ('Causer:2->Result:3-pred-count=4', 1.0)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(mdl.weights.items(), key = lambda tpl: -tpl[1])[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Feats\n",
    " - Valency - Num Causers and Effects for each code, and then highest count for each\n",
    " - Prior Prob per crel from training data - then usual prob feats from that array\n",
    " - Longest causal chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
