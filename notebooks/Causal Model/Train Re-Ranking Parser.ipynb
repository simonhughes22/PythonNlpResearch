{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags, get_tag_freq\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freq = get_tag_freq(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "set_cr_tags = set(cr_tags)\n",
    "cr_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causer:50->Result:50 19\n",
      "Causer:11->Result:11 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43227"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for cr in cr_tags:\n",
    "    l,r = cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "    total += tag_freq[cr]\n",
    "    if l == r:\n",
    "        print(cr, tag_freq[cr])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        max_epochs: int) -> float:\n",
    "\n",
    "    serial_results = [\n",
    "        model_train_predict(essays_TD, essays_VD, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    cv_td_preds_by_sent = []\n",
    "    cv_vd_preds_by_sent = []\n",
    "    for (num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "        \n",
    "        cv_td_preds_by_sent.append(td_preds_by_sent)\n",
    "        cv_vd_preds_by_sent.append(vd_preds_by_sent)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    return cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent\n",
    "\n",
    "def add_cr_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "\n",
    "def get_label_data(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        for sentence in essay.sentences:\n",
    "            unique_cr_tags = set()\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "            add_cr_labels(unique_cr_tags, ys_bytag_sent)\n",
    "    return ys_bytag_sent\n",
    "\n",
    "def get_label_data_essay_level(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_essay = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        add_cr_labels(unique_cr_tags, ys_bytag_essay)\n",
    "    return ys_bytag_essay\n",
    "\n",
    "def essay_to_crels(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    name2crels = defaultdict(set)\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        name2crels[essay.name] = unique_cr_tags\n",
    "    return name2crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurevectorizer import FeatureVectorizer\n",
    "\n",
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_micro_metrics(df):\n",
    "    return df[df.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"recall\", \"precision\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from Data.featurevectorizer import FeatureVectorizer\n",
    "\n",
    "class DependencyFeatureInputs(object):\n",
    "    def __init__(self, essay_name, lsent_ix, rsent_ix, causer_tag, result_tag, causer_words, between_words,\n",
    "                 result_words,\n",
    "                 causer_first, between_codes, num_sentences_between):\n",
    "        self.essay_name = essay_name\n",
    "        self.lsent_ix = lsent_ix\n",
    "        self.rsent_ix = rsent_ix\n",
    "        self.num_sentences_between = num_sentences_between\n",
    "        self.between_codes = between_codes\n",
    "        self.causer_first = causer_first\n",
    "        self.result_words = result_words\n",
    "        self.between_words = between_words\n",
    "        self.causer_words = causer_words\n",
    "        self.result_tag = result_tag\n",
    "        self.causer_tag = causer_tag\n",
    "        self.crel = \"Causer:{a}->Result:{b}\".format(a=causer_tag, b=result_tag)\n",
    "\n",
    "\n",
    "class DependencyClassifier(object):\n",
    "    def __init__(self, classifier_fn=LogisticRegression, negative_label=0, sentence_span=2,\n",
    "                 min_feat_freq=10, log_fn=lambda s: print(s), ):\n",
    "        self.log = log_fn\n",
    "        self.epoch = 0\n",
    "        self.negative_label = negative_label\n",
    "        self.sentence_span = sentence_span\n",
    "        self.min_feat_freq = min_feat_freq\n",
    "        self.vectorizer = FeatureVectorizer(min_feature_freq=min_feat_freq)\n",
    "        self.clf = classifier_fn()\n",
    "        self.fit_vectorizer = False\n",
    "            \n",
    "    def __fill_in_gaps__(self, tag_seq):\n",
    "        new_tag_seq = []\n",
    "        for i, tag in enumerate(tag_seq):\n",
    "            if tag == EMPTY \\\n",
    "                    and i > 0 \\\n",
    "                    and tag_seq[i - 1] != EMPTY \\\n",
    "                    and i < len(tag_seq) - 1 \\\n",
    "                    and tag_seq[i - 1] == tag_seq[i + 1]:\n",
    "                tag = tag_seq[i - 1]\n",
    "\n",
    "            new_tag_seq.append(tag)\n",
    "        return new_tag_seq\n",
    "\n",
    "    def __compute_tag_2_spans__(self, essay):\n",
    "        sent_tag2spans = []\n",
    "        wd_ix = -1\n",
    "        essay_words = []\n",
    "        essay_ptags = []\n",
    "        for sent_ix in range(len(essay.sentences)):\n",
    "            words, tag_seq = zip(*essay.sentences[sent_ix])\n",
    "\n",
    "            tag2spans = []  # maps to list of start and end spans for each tag\n",
    "            sent_tag2spans.append(tag2spans)\n",
    "\n",
    "            last_tag = EMPTY\n",
    "            tag_start = None\n",
    "            ptags_sent = self.__fill_in_gaps__(essay.pred_tagged_sentences[sent_ix])\n",
    "            current_crel_tags = set()\n",
    "            for i, ptag in enumerate(ptags_sent):\n",
    "                wd_ix += 1\n",
    "                essay_words.append(words[i])\n",
    "                essay_ptags.append(ptag)\n",
    "                # Tag changed\n",
    "                if ptag != last_tag:\n",
    "                    if last_tag != EMPTY:\n",
    "                        tag2spans.append((last_tag, tag_start, wd_ix - 1, sent_ix, current_crel_tags))\n",
    "                    tag_start = wd_ix\n",
    "                    current_crel_tags = set()\n",
    "                current_crel_tags.update(to_is_valid_crel(tag_seq[i]))\n",
    "                last_tag = ptag\n",
    "            if last_tag != EMPTY:\n",
    "                tag2spans.append((last_tag, tag_start, wd_ix, len(essay.sentences) - 1, current_crel_tags))\n",
    "        assert len(essay_words) == len(essay_ptags)\n",
    "        return sent_tag2spans, essay_words, essay_ptags\n",
    "\n",
    "    def __combine_feats__(self, ftsa, ftsb):\n",
    "        fts = {}\n",
    "        for a, aval in ftsa.items():\n",
    "            for b, bval in ftsb.items():\n",
    "                fts[a + \"|\" + b] = aval * bval\n",
    "        return fts\n",
    "\n",
    "    def create_features(self, feat_inp):\n",
    "        feats = {}\n",
    "        feats[feat_inp.crel] = 1\n",
    "        feats[\"Causer:{tag}\".format(tag=feat_inp.causer_tag)] = 1\n",
    "        feats[\"Result:{tag}\".format(tag=feat_inp.result_tag)] = 1\n",
    "        cs_fts, res_fts = {}, {}\n",
    "        for wd in feat_inp.causer_words:\n",
    "            cs_fts[\"Causer:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(cs_fts)\n",
    "        for wd in feat_inp.result_words:\n",
    "            res_fts[\"Result:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(res_fts)\n",
    "        feats.update(self.__combine_feats__(cs_fts, res_fts))\n",
    "        btwn_fts = {}\n",
    "        for wd in feat_inp.between_words:\n",
    "            btwn_fts[\"Between:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(btwn_fts)\n",
    "        #         feats.update(self.__combine_feats__(cs_fts, btwn_fts))\n",
    "        #         feats.update(self.__combine_feats__(res_fts, btwn_fts))\n",
    "        if feat_inp.causer_first:\n",
    "            feats[\"Left2Right\"] = 1\n",
    "        else:\n",
    "            feats[\"Right2Left\"] = 1\n",
    "\n",
    "        if feat_inp.num_sentences_between == 0:\n",
    "            feats[\"SameSentence\"] = 1\n",
    "        feats[\"SentBetween\"] = feat_inp.num_sentences_between\n",
    "        if feat_inp.num_sentences_between <= 1:\n",
    "            feats[\"SentBetween<=1\"] = 1\n",
    "        if feat_inp.num_sentences_between <= 2:\n",
    "            feats[\"SentBetween<=2\"] = 1\n",
    "        else:\n",
    "            feats[\"SentBetween>2\"] = 1\n",
    "\n",
    "        num_codes_between = len(feat_inp.between_codes)\n",
    "        feats[\"CodesBetween\"] = num_codes_between\n",
    "        if num_codes_between <= 1:\n",
    "            feats[\"CodesBetween<=1\"] = 1\n",
    "        if num_codes_between <= 2:\n",
    "            feats[\"CodesBetween<=2\"] = 1\n",
    "        else:\n",
    "            feats[\"CodesBetween>2\"] = 1\n",
    "        return feats\n",
    "\n",
    "    def __generate_training_data__(self, essays):\n",
    "        xs, ys, essay_sent_feat_inpts = [], [], []\n",
    "        for essay_ix, essay in enumerate(essays):\n",
    "            sent_tag2spans, essay_words, essay_ptags = self.__compute_tag_2_spans__(essay)\n",
    "            for sent_ix in range(len(sent_tag2spans)):\n",
    "                # tag 2 spans for sentence\n",
    "                next_tag2spans = []\n",
    "                # grab next few sentences' predicted tags\n",
    "                for offset in range(0, self.sentence_span + 1):\n",
    "                    if (sent_ix + offset) < len(sent_tag2spans):\n",
    "                        next_tag2spans.extend(sent_tag2spans[sent_ix + offset])\n",
    "\n",
    "                for ltag_ix, (ltag, lstart_ix, lend_ix, lsent_ix, lcrels) in enumerate(sent_tag2spans[sent_ix]):\n",
    "                    for rtag, rstart_ix, rend_ix, rsent_ix, rcrels in next_tag2spans[ltag_ix + 1:]:\n",
    "                        num_sent_between = rsent_ix - lsent_ix\n",
    "\n",
    "                        ltag_words = essay_words[lstart_ix:lend_ix + 1]\n",
    "                        between_words = essay_words[lend_ix + 1:rstart_ix]\n",
    "                        rtag_words = essay_words[rstart_ix:rend_ix + 1]\n",
    "                        between_codes = essay_ptags[lend_ix + 1:rstart_ix]\n",
    "\n",
    "                        lbls = set(lcrels).union(rcrels)\n",
    "\n",
    "                        feat_ext_inp = DependencyFeatureInputs(essay_name=essay.name, lsent_ix=lsent_ix,\n",
    "                                                               rsent_ix=rsent_ix,\n",
    "                                                               causer_tag=ltag, result_tag=rtag,\n",
    "                                                               causer_words=ltag_words, between_words=between_words,\n",
    "                                                               result_words=rtag_words, causer_first=True,\n",
    "                                                               between_codes=between_codes,\n",
    "                                                               num_sentences_between=num_sent_between)\n",
    "                        x = self.create_features(feat_ext_inp)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if feat_ext_inp.crel in lbls else self.negative_label)\n",
    "                        essay_sent_feat_inpts.append(feat_ext_inp)\n",
    "\n",
    "                        feat_ext_inp = DependencyFeatureInputs(essay_name=essay.name, lsent_ix=lsent_ix,\n",
    "                                                               rsent_ix=rsent_ix,\n",
    "                                                               causer_tag=rtag, result_tag=ltag,\n",
    "                                                               causer_words=rtag_words, between_words=between_words,\n",
    "                                                               result_words=ltag_words, causer_first=False,\n",
    "                                                               between_codes=between_codes,\n",
    "                                                               num_sentences_between=num_sent_between)\n",
    "                        x = self.create_features(feat_ext_inp)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if feat_ext_inp.crel in lbls else self.negative_label)\n",
    "                        essay_sent_feat_inpts.append(feat_ext_inp)\n",
    "        \n",
    "        if not self.fit_vectorizer:\n",
    "            xs_array = self.vectorizer.fit_transform(xs)\n",
    "            self.fit_vectorizer = True\n",
    "        else:            \n",
    "            xs_array = self.vectorizer.transform(xs)\n",
    "        return xs_array, ys, essay_sent_feat_inpts\n",
    "\n",
    "    def train(self, train_essays):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=train_essays)\n",
    "        self.clf.fit(X=xs, y=ys)\n",
    "\n",
    "    def __group_predictions_by_essay__(self, essay_sent_feat_inpts, preds, threshold):\n",
    "        name2pred = defaultdict(set)\n",
    "        for feat_inputs, pred in zip(essay_sent_feat_inpts, preds):\n",
    "            if pred >= threshold:\n",
    "                name2pred[feat_inputs.essay_name].add(feat_inputs.crel)\n",
    "        return name2pred\n",
    "\n",
    "    def __group_predictions_by_sentence__(self, essay_sent_feat_inpts, preds, threshold):\n",
    "        namesent2pred = defaultdict(set)\n",
    "        for feat_inputs, pred in zip(essay_sent_feat_inpts, preds):\n",
    "            if pred >= threshold:\n",
    "                namesent2pred[(feat_inputs.essay_name, feat_inputs.lsent_ix)].add(feat_inputs.crel)\n",
    "                namesent2pred[(feat_inputs.essay_name, feat_inputs.rsent_ix)].add(feat_inputs.crel)\n",
    "        return namesent2pred\n",
    "\n",
    "    def predict_probability(self, tagged_essays, min_prob=0.1):\n",
    "        # Get predicted probabilities\n",
    "        xs, _, essay_sent_feat_inpts = self.__generate_training_data__(essays=tagged_essays)\n",
    "        probs = self.clf.predict_proba(xs)[:, 1]\n",
    "        name2pred = defaultdict(list)\n",
    "        for feat_inputs, prob in zip(essay_sent_feat_inpts, probs):\n",
    "            if prob >= min_prob:\n",
    "                name2pred[feat_inputs.essay_name].append((feat_inputs, prob))\n",
    "        return name2pred\n",
    "\n",
    "    def evaluate(self, tagged_essays, print_classification_report=True):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_feat_inpts = self.__generate_training_data__(essays=tagged_essays)\n",
    "        preds = self.clf.predict(xs)\n",
    "        if print_classification_report:\n",
    "            print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        namesent2pred = self.__group_predictions_by_sentence__(\n",
    "            essay_sent_feat_inpts=essay_sent_feat_inpts, preds=preds, threshold=1.0)\n",
    "\n",
    "        pred_ys_bytag_sent = defaultdict(list)\n",
    "        for essay in tagged_essays:\n",
    "            for sent_ix, sentence in enumerate(essay.sentences):\n",
    "                unique_cr_tags = namesent2pred[(essay.name, sent_ix)]\n",
    "                add_cr_labels(unique_cr_tags, pred_ys_bytag_sent)\n",
    "        return pred_ys_bytag_sent\n",
    "\n",
    "    def evaluate_essay_level(self, tagged_essays, print_classification_report=True):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_feat_inpts = self.__generate_training_data__(essays=tagged_essays)\n",
    "        preds = self.clf.predict(xs)\n",
    "        if print_classification_report:\n",
    "            print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        namesent2pred = self.__group_predictions_by_essay__(\n",
    "            essay_sent_feat_inpts=essay_sent_feat_inpts, preds=preds, threshold=1.0)\n",
    "\n",
    "        pred_ys_bytag_essay = defaultdict(list)\n",
    "        for essay in tagged_essays:\n",
    "            unique_cr_tags = namesent2pred[essay.name]\n",
    "            add_cr_labels(unique_cr_tags, pred_ys_bytag_essay)\n",
    "        return pred_ys_bytag_essay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_accuracy(parser, essays):\n",
    "    ys_bytag_sent = get_label_data(tagged_essays=essays)\n",
    "    pred_ys_bytag_sent = parser.evaluate(tagged_essays=essays, print_classification_report=False)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_sent, pred_ys_bytag_sent)\n",
    "    return get_micro_metrics(metrics_to_df(mean_metrics))\n",
    "\n",
    "def compute_essay_accuracy(parser, essays):\n",
    "    ys_bytag_essay = get_label_data_essay_level(tagged_essays=essays)\n",
    "    pred_ys_bytag_essay = parser.evaluate_essay_level(tagged_essays=essays, print_classification_report=False)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_essay, pred_ys_bytag_essay)\n",
    "    return get_micro_metrics(metrics_to_df(mean_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Accuracy is Equivalent to Parser Model (Or Very Close) When We Don't Look Across Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      9036\n",
      "          1       0.89      0.91      0.90      2916\n",
      "\n",
      "avg / total       0.95      0.95      0.95     11952\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.91      0.92      1868\n",
      "          1       0.73      0.78      0.75       586\n",
      "\n",
      "avg / total       0.88      0.88      0.88      2454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.997892</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.735714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.997892  0.691275  0.651899   0.735714"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DependencyClassifier(sentence_span=0)\n",
    "parser.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent = parser.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test = parser.evaluate(pred_tagged_essays_test)\n",
    "\n",
    "compute_sentence_accuracy(parser, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.985948</td>\n",
       "      <td>0.723973</td>\n",
       "      <td>0.69287</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score   recall  precision\n",
       "95  0.985948  0.723973  0.69287      0.758"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_essay_accuracy(parser, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97     44627\n",
      "          1       0.85      0.80      0.82      7487\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52114\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94      9280\n",
      "          1       0.61      0.60      0.60      1484\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.993337</td>\n",
       "      <td>0.435711</td>\n",
       "      <td>0.710443</td>\n",
       "      <td>0.314206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.993337  0.435711  0.710443   0.314206"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser2 = DependencyClassifier(sentence_span=2)\n",
    "parser2.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent2 = parser2.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test2 = parser2.evaluate(pred_tagged_essays_test)\n",
    "\n",
    "compute_sentence_accuracy(parser2, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.980891</td>\n",
       "      <td>0.6661</td>\n",
       "      <td>0.716636</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.980891    0.6661  0.716636   0.622222"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_essay_accuracy(parser2, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Structured Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Structured perceptron classifier. Implementation geared for simplicity rather than\n",
    "efficiency.\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class StructuredPerceptron(object):\n",
    "    '''A structured perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self, learning_rate=0.3, max_update_items=1):\n",
    "        # Each feature gets its own weight\n",
    "        # needs to be non zero otherwise first\n",
    "        self.weights = defaultdict(lambda : 1.0)\n",
    "        self.learning_rate = learning_rate\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "        # how many items do we use to update the weights?\n",
    "        self.max_update_items = max_update_items\n",
    "\n",
    "    def clone(self):\n",
    "        p = StructuredPerceptron(self.learning_rate)\n",
    "        p.weights.update(self.weights)\n",
    "        p._totals.update(self._totals)\n",
    "        p._tstamps.update(self._tstamps)\n",
    "        p.i = self.i\n",
    "        return p\n",
    "\n",
    "    def rank(self, features_array):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores2index = {}\n",
    "        for i, feats in enumerate(features_array):\n",
    "            scores2index[i] = self.decision_function(feats)\n",
    "        # return a ranking of the scores, by best to worse\n",
    "\n",
    "        return [ix for ix, score in sorted(scores2index.items(), key=lambda tpl: -tpl[-1])]\n",
    "\n",
    "    def train(self, best_feats, other_feats_array):\n",
    "        feats_array = [best_feats] + list(other_feats_array)\n",
    "        ixs = self.rank(feats_array)\n",
    "\n",
    "        # go thru up to |max_update_items| items ranked above the best, and update the weights\n",
    "        best_ix = ixs[0]\n",
    "        if best_ix != 0:\n",
    "            for rank, ix in enumerate(ixs):\n",
    "                # don't update items ranked below the best parse\n",
    "                if ix == 0 or rank >= self.max_update_items:\n",
    "                    break\n",
    "\n",
    "                self.update(best_feats=best_feats, highest_ranked_feats=feats_array[ix])\n",
    "\n",
    "    def decision_function(self, features):\n",
    "        '''Dot-product the features and current weights and return the score.'''\n",
    "        score = 0.0\n",
    "        for feat, value in features.items():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            score += self.weights[feat] * value\n",
    "        return score\n",
    "\n",
    "    def update(self, best_feats, highest_ranked_feats):\n",
    "        '''Update the feature weights.'''\n",
    "\n",
    "        # TODO - weight the weight update by the difference in errors\n",
    "        def upd_feat(feat, val):\n",
    "            w = self.weights[feat]\n",
    "            # update the totals by the number of timestamps the current value has survived * val\n",
    "            self._totals[feat] += (self.i - self._tstamps[feat]) * w\n",
    "            # store latest update timestamp\n",
    "            self._tstamps[feat] = self.i\n",
    "            # finally, update the current weight\n",
    "            self.weights[feat] = w + (self.learning_rate * val)\n",
    "\n",
    "        self.i += 1\n",
    "        for feat, weight in self.weights.items():\n",
    "            val = best_feats[feat] - highest_ranked_feats[feat]\n",
    "            upd_feat(feat, val)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        new_feat_weights = defaultdict(float)\n",
    "        for feat, weight in self.weights.items():\n",
    "            total = self._totals[feat]\n",
    "            total += (self.i - self._tstamps[feat]) * weight\n",
    "            averaged = round(total / float(self.i), 5)\n",
    "            if averaged != 0.0:\n",
    "                new_feat_weights[feat] = averaged\n",
    "        self.weights = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = parser.predict_probability(pred_tagged_essays_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.204109589041096, 4.0, 16, 6.0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# look at the number of predicted items\n",
    "lens = []\n",
    "for ename, lst in probs_train.items():\n",
    "    crels = set()\n",
    "    for fts, p in lst:\n",
    "        crels.add(fts.crel)\n",
    "    lens.append(len(crels))\n",
    "\n",
    "np.mean(lens), np.median(lens), np.max(lens), np.percentile(lens, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "()\n",
      "(1,)\n",
      "(2,)\n",
      "(3,)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(2, 3)\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_all_combos(items):\n",
    "    # enforces a consistent ordering for the resulting tuples\n",
    "    items = sorted(items) \n",
    "    cbos = [()] # seed with the empty combo\n",
    "    for i in range(1, len(items)+1):\n",
    "        cbos.extend(combinations(items,i))\n",
    "    return cbos\n",
    "\n",
    "cbos = get_all_combos([3,2,1])\n",
    "print(len(cbos)) # 2**len(items)-1\n",
    "if len(cbos) < 1000:\n",
    "    for cbo in sorted(cbos, key = lambda l: (len(l), l)):\n",
    "        print(cbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(),\n",
       " ('1->2', '10->12', '12->50'),\n",
       " ('1->2', '10->12', '12->50', '5->8'),\n",
       " ('1->2', '12->50'),\n",
       " ('1->2', '12->50', '5->8'),\n",
       " ('10->12', '12->50'),\n",
       " ('10->12', '12->50', '5->8'),\n",
       " ('12->50',)}"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_top_parses(crel2maxprobs, top_n):\n",
    "\n",
    "    max_parses = 2**len(crel2maxprobs) # maximum parse combinations\n",
    "    assert max_parses > top_n, (max_parses, top_n) # otherwise brute force it\n",
    "\n",
    "    top_parses = set([()]) # always seed with the empty parse\n",
    "    probs = []\n",
    "    while len(top_parses) < top_n:\n",
    "        new_parse = []\n",
    "        for crel, prob in crel2maxprobs.items():\n",
    "            rand_val = np.random.random() # random number >= 0 and < 1\n",
    "            if rand_val < prob:\n",
    "                new_parse.append(crel)\n",
    "        # make hashable and enforce consistent order\n",
    "        top_parses.add(tuple(sorted(new_parse)))\n",
    "    \n",
    "    return top_parses\n",
    "\n",
    "def get_top_parses(crel2maxprobs, threshold=0.5):\n",
    "\n",
    "    top_parse = [crel for crel, prob in crel2maxprob.items() if prob >= threshold]\n",
    "    if top_parse:\n",
    "        return [tuple(sorted(top_parse))]\n",
    "    else:\n",
    "        return [()]\n",
    "\n",
    "crel_probs = {\n",
    "    \"1->2\":   0.8,\n",
    "    \"2->3\":   0.01,\n",
    "    \"5->8\":   0.25,\n",
    "    \"10->12\": 0.75,\n",
    "    \"12->50\": 0.99,\n",
    "}\n",
    "\n",
    "# important - should see a lot more of the more probable codes\n",
    "sample_top_parses(crel_probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NgramGenerator import compute_ngrams\n",
    "\n",
    "def to_short_tag(tag):\n",
    "    return tag.replace(\"Causer:\",\"\").replace(\"Result:\", \"\")\n",
    "\n",
    "def build_chains_inner(tree, l, visited, depth=0):\n",
    "    chains = []\n",
    "    if l not in tree:\n",
    "        return chains\n",
    "    for r in tree[l]:\n",
    "        if r in visited:\n",
    "            continue\n",
    "        visited.add(r) # needed to prevent cycles, which cause infinite recursion\n",
    "        extensions = build_chains_inner(tree, r, visited, depth+1)\n",
    "        visited.remove(r)\n",
    "        for ch in extensions:\n",
    "            chains.append([r] + ch)\n",
    "        if not extensions:\n",
    "            chains.append([r])\n",
    "    return chains\n",
    "\n",
    "def build_chains(tree):    \n",
    "    lhs_items = set(tree.keys())\n",
    "    rhs_items = set()\n",
    "    for l,rhs in tree.items():        \n",
    "        rhs_items.update(rhs)\n",
    "    \n",
    "    chains = []\n",
    "    # starting positions of each chain are those appearing on the lhs but not the rhs\n",
    "    start_codes = lhs_items - rhs_items    \n",
    "    for l in start_codes:\n",
    "        rhs = tree[l]\n",
    "        for r in rhs:\n",
    "            for ch in build_chains_inner(tree, r, {l,r}, 0):\n",
    "                chains.append([l,r] + ch)\n",
    "    return chains\n",
    "\n",
    "def extend_chains(chains):\n",
    "    ext_chains = set()\n",
    "    for tokens in chains:\n",
    "        ext_chains.add(\",\".join(tokens))\n",
    "        ngrams = compute_ngrams(tokens,max_len=None, min_len=3)\n",
    "        for t in ngrams:\n",
    "            ext_chains.add(\",\".join(t))\n",
    "    return ext_chains\n",
    "\n",
    "def extract_features_from_parse(parse, crel2probs):\n",
    "    \n",
    "    feats = defaultdict(float)\n",
    "    tree = defaultdict(set) # maps causers to effects for building chains\n",
    "    max_probs = []    \n",
    "    code_tally = defaultdict(float)\n",
    "    \n",
    "    pairs = set()\n",
    "    inverted_count = 0\n",
    "    for crel in parse:\n",
    "        probs = crel2probs[crel]\n",
    "        max_p = max(probs)\n",
    "        max_probs.append(max_p)\n",
    "        feats[\"{crel}-MAX(prob)\".format(crel=crel)] = max_p\n",
    "        feats[\"{crel}-MIN(prob)\".format(crel=crel)] = min(probs)\n",
    "        feats[\"{crel}-pred-count\".format(crel=crel)] = len(probs)\n",
    "        feats[\"{crel}-pred-count={count}\".format(crel=crel, count=len(probs))] = 1\n",
    "        \n",
    "        # with type\n",
    "        l,r = crel.split(\"->\")\n",
    "        code_tally[l] +=1\n",
    "        code_tally[r] +=1\n",
    "        \n",
    "        # without type\n",
    "        l_short, r_short = to_short_tag(l), to_short_tag(r)\n",
    "        code_tally[l_short] +=1\n",
    "        code_tally[r_short] +=1\n",
    "        # ordering of the codes, ignoring the causal direction\n",
    "        feats[l_short + \":\" + r_short] = 1\n",
    "        \n",
    "        # build tree structure so we can retrieve the chains\n",
    "        tree[l_short].add(r_short)\n",
    "        \n",
    "        # track whether the rule exists in the opposite direction\n",
    "        pairs.add((l_short,r_short))\n",
    "        if (r_short,l_short) in pairs:\n",
    "            inverted_count += 1\n",
    "            \n",
    "    if inverted_count:\n",
    "        feats[\"inverted\"] = 1\n",
    "        feats[\"num_inverted\"] = inverted_count\n",
    "    else:\n",
    "        feats[\"not_inverted\"] = 1\n",
    "    \n",
    "    # counts\n",
    "    feats.update(code_tally)\n",
    "    num_crels = len(parse)\n",
    "    feats[\"num_crels\"] = num_crels\n",
    "    feats[\"num_crels=\"+str(len(parse))] = 1 # includes a tag for the empty parse\n",
    "    for i in range(1,11):\n",
    "        if num_crels <= i:\n",
    "            feats[\"num_crels<={i}\".format(i=i)] = 1\n",
    "        else:\n",
    "            feats[\"num_crels>{i}\".format(i=i)] = 1\n",
    "        \n",
    "    # combination of crels\n",
    "    # need to sort so that order of a and b is consistent across parses\n",
    "    pairs = combinations(sorted(parse), r=2)\n",
    "    for a, b in pairs:\n",
    "        feats[\"{a}|{b}\".format(a=a, b=b)] = 1\n",
    "        \n",
    "    #chains\n",
    "    causer_chains = extend_chains(build_chains(tree))\n",
    "    for ch in causer_chains:\n",
    "        feats[\"CChain:\" + ch] = 1\n",
    "    \n",
    "    if max_probs: # might be an empty parse\n",
    "        for cutoff in [0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95]:\n",
    "            above =  len([p for p in max_probs if p >=cutoff])\n",
    "            feats[\"Above-{cutoff}\".format(cutoff=cutoff)] = above\n",
    "            feats[\"%-Above-{cutoff}\".format(cutoff=cutoff)] = above/len(max_probs)\n",
    "            if above == len(max_probs):\n",
    "                feats[\"All-Above-{cutoff}\".format(cutoff=cutoff)] = 1\n",
    "        \n",
    "        feats[\"avg-prob\"] = np.mean(max_probs)\n",
    "        feats[\"med-prob\"] = np.median(max_probs)\n",
    "        feats[\"prod-prob\"]= np.product(max_probs)\n",
    "        feats[\"min-prob\"] = np.min(max_probs)\n",
    "        feats[\"max-prob\"] = np.max(max_probs)\n",
    "        for p in [5, 10, 25, 75, 90, 95]:\n",
    "            feats[\"{p}%-prob\".format(p=p)] = np.percentile(max_probs, p)\n",
    "        # geometric mean\n",
    "        feats[\"geo-mean\"] = np.prod(max_probs)**(1/len(max_probs))\n",
    "    return feats\n",
    "\n",
    "def additional_features(parse, feats_input):\n",
    "    #TODO - ratio of number of concept codes to number of relations\n",
    "    #TODO - average, min and max word distance between codes in a relation\n",
    "    #TODO - average, min and sentence distance between codes in a relation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranker(model, xs):\n",
    "    preds = []\n",
    "    clone = model.clone()\n",
    "    clone.average_weights()\n",
    "    for opt_parse_fts, other_fts in xs:\n",
    "        ixs = clone.rank([opt_parse_fts] + other_fts)\n",
    "        preds.append(1 if ixs[0] == 0 else 0)\n",
    "    return np.mean(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay2crels_train = essay_to_crels(pred_tagged_essays_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 s, sys: 141 ms, total: 37.3 s\n",
      "Wall time: 37.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOP_N = 600\n",
    "\n",
    "tmp_xs = []\n",
    "max_overall_parses = 0\n",
    "feat_freq = defaultdict(int)\n",
    "\n",
    "for ename, lst in probs_train.items():\n",
    "\n",
    "    act_crels = essay2crels_train[ename]\n",
    "    crel2probs = defaultdict(list)\n",
    "    crel2maxprob = defaultdict(float)\n",
    "    for fts, prob in lst:\n",
    "        crel2probs[fts.crel].append(prob)\n",
    "        crel2maxprob[fts.crel] = max(crel2maxprob[fts.crel], prob)\n",
    "        \n",
    "    num_crels = len(crel2probs)\n",
    "    max_parses = 2**num_crels\n",
    "    if max_parses > 2 * TOP_N:\n",
    "#         parses = sample_top_parses(crel2maxprob, TOP_N)\n",
    "        parses = get_top_parses(crel2maxprob) # just get the predicted parses (probability >= 0.5)\n",
    "    else:\n",
    "        # brute force it\n",
    "        parses = get_all_combos(crel2probs.keys())\n",
    "            \n",
    "    opt_parse = tuple(sorted(act_crels.intersection(crel2probs.keys())))\n",
    "    \n",
    "    max_overall_parses = max(max_overall_parses, len(parses))\n",
    "    feats_array = [extract_features_from_parse(p, crel2probs) for p in parses if p != opt_parse]\n",
    "    opt_feats = extract_features_from_parse(opt_parse, crel2probs)\n",
    "    all_feats_arr = [opt_feats] + feats_array\n",
    "    \n",
    "    tmp_xs.append(all_feats_arr)\n",
    "    all_feats = set()\n",
    "    for fts in all_feats_arr:\n",
    "        all_feats.update(fts.keys())\n",
    "    for ft in all_feats:\n",
    "        feat_freq[ft] += 1\n",
    "        \n",
    "assert len(tmp_xs) == len(probs_train), \"Parses for all essays should be generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 730, 730)"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_overall_parses, len(tmp_xs), len(probs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the Feature Freq (Doesn't Help for Now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PARSER_FEAT_FREQ = 1\n",
    "freq_feats = set((f for f, cnt in feat_freq.items() if cnt >= MIN_PARSER_FEAT_FREQ))\n",
    "\n",
    "def to_freq_feats(feats, freq_feats):\n",
    "    new_feats = defaultdict(float)\n",
    "    for f, v in feats.items():\n",
    "        if f in freq_feats:\n",
    "            new_feats[f] = v\n",
    "    return new_feats\n",
    "        \n",
    "xs = []\n",
    "for feats_arr in tmp_xs:\n",
    "    filtered = [to_freq_feats(x, freq_feats) for x in feats_arr]\n",
    "    xs.append((filtered[0], filtered[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Re-Ranker - Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Accuracy: 61.37931034482759%\n",
      "Epoch: 1 Accuracy: 61.37931034482759%\n",
      "Epoch: 2 Accuracy: 60.689655172413794%\n",
      "Epoch: 3 Accuracy: 60.689655172413794%\n",
      "Epoch: 4 Accuracy: 59.310344827586206%\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "MAX_EPOCHS = 20\n",
    "PROPN_TRAIN = 0.8\n",
    "\n",
    "num_test = int(len(xs) * (1-PROPN_TRAIN))\n",
    "test_xs, train_xs = xs[:num_test], xs[num_test:]\n",
    "\n",
    "test_accs = [-1]\n",
    "best_model = None\n",
    "num_declining_acc = 0\n",
    "\n",
    "model = StructuredPerceptron(learning_rate=0.3, max_update_items=2)\n",
    "for i in range(MAX_EPOCHS):\n",
    "    shuffle(train_xs)\n",
    "    for opt_parse_fts, other_fts in train_xs:\n",
    "        model.train(best_feats=opt_parse_fts, other_feats_array=other_fts)\n",
    "    test_accuracy = evaluate_ranker(model, test_xs)\n",
    "    print(\"Epoch: {epoch} Accuracy: {acc}%\".format(epoch=i, acc=test_accuracy*100))\n",
    "    if test_accuracy > max(test_accs):\n",
    "        best_model = model.clone()\n",
    "        num_declining_acc = 0\n",
    "    else:\n",
    "        num_declining_acc += 1\n",
    "        if num_declining_acc >= 5:\n",
    "            break\n",
    "    test_accs.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('num_crels', -6.24398),\n",
       " ('num_crels<=7', 3.07369),\n",
       " ('num_crels<=8', 2.9644),\n",
       " ('num_crels<=6', 2.49909),\n",
       " ('num_crels<=9', 2.22091),\n",
       " ('All-Above-0.2', 2.1117),\n",
       " ('not_inverted', 1.92183),\n",
       " ('num_crels=0', 1.82772),\n",
       " ('num_crels>1', 1.79793),\n",
       " ('Causer:1->Result:50|Causer:5->Result:50', 1.79701),\n",
       " ('num_crels=6', 1.78432),\n",
       " ('Above-0.2', -1.77842),\n",
       " ('Causer:3->Result:50|Causer:4->Result:14', 1.75842),\n",
       " ('num_crels>2', 1.74863),\n",
       " ('Causer:3->Result:4|Causer:7->Result:50', 1.72481),\n",
       " ('num_crels<=5', 1.71477),\n",
       " ('Causer:1->Result:3-MIN(prob)', 1.65358),\n",
       " ('num_crels=4', 1.58631),\n",
       " ('num_crels=7', 1.57461),\n",
       " ('Causer:11->Result:14|Causer:7->Result:50', 1.56415)]"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.average_weights()\n",
    "sorted(best_model.weights.items(), key = lambda tpl: -abs(tpl[1]))[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6137931034482759"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "PROPN_TRAIN = 0.8\n",
    "\n",
    "num_test = int(len(xs) * (1-PROPN_TRAIN))\n",
    "train_items = list(probs_train.items())\n",
    "test_xs, train_xs = train_items[:num_test], train_items[num_test:]\n",
    "    \n",
    "preds = []\n",
    "for ename, lst in test_xs:\n",
    "\n",
    "    act_crels = essay2crels_train[ename]\n",
    "    crel2maxprob = defaultdict(float)\n",
    "    for fts, prob in lst:\n",
    "        crel2maxprob[fts.crel] = max(crel2maxprob[fts.crel], prob)\n",
    "    pred_crels = [crel for crel, p in crel2maxprob.items() if p >= 0.5]\n",
    "    parse = tuple(sorted(pred_crels))\n",
    "    opt_parse = tuple(sorted(act_crels.intersection(crel2maxprob.keys())))\n",
    "    preds.append(1 if opt_parse == parse else 0)\n",
    "    \n",
    "np.mean(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chains Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = (\n",
    "'Causer:1->Result:2',\n",
    "'Causer:2->Result:3',\n",
    "'Causer:3->Result:50',\n",
    "'Causer:2->Result:4',\n",
    "'Causer:3->Result:5',\n",
    "'Causer:5->Result:6',\n",
    "'Causer:6->Result:50',\n",
    "'Causer:7->Result:11',\n",
    "'Causer:11->Result:12',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = defaultdict(set) # maps causers to effects for building chains\n",
    "for crel in parse:\n",
    "    # with type\n",
    "    l,r = crel.split(\"->\")\n",
    "    l_short, r_short = to_short_tag(l), to_short_tag(r)\n",
    "    tree[l_short].add(r_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1,2,3',\n",
       " '1,2,3,5',\n",
       " '1,2,3,5,6',\n",
       " '1,2,3,5,6,50',\n",
       " '1,2,3,50',\n",
       " '1,2,4',\n",
       " '2,3,5',\n",
       " '2,3,5,6',\n",
       " '2,3,5,6,50',\n",
       " '2,3,50',\n",
       " '3,5,6',\n",
       " '3,5,6,50',\n",
       " '5,6,50',\n",
       " '7,11,12'}"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains = build_chains(tree)\n",
    "print(len(extend_chains(chains)))\n",
    "extend_chains(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Compute F1 scores for the test essays (not test xs, but test essays)\n",
    "- Implement early stopping to judge optimal number of epochs\n",
    "- Figure out how to handle the small number of items with too many crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
