{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags, get_tag_freq\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freq = get_tag_freq(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "set_cr_tags = set(cr_tags)\n",
    "cr_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causer:50->Result:50 19\n",
      "Causer:11->Result:11 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43227"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for cr in cr_tags:\n",
    "    l,r = cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "    total += tag_freq[cr]\n",
    "    if l == r:\n",
    "        print(cr, tag_freq[cr])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        max_epochs: int) -> float:\n",
    "\n",
    "    serial_results = [\n",
    "        model_train_predict(essays_TD, essays_VD, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    cv_td_preds_by_sent = []\n",
    "    cv_vd_preds_by_sent = []\n",
    "    for (num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "        \n",
    "        cv_td_preds_by_sent.append(td_preds_by_sent)\n",
    "        cv_vd_preds_by_sent.append(vd_preds_by_sent)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    return cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent\n",
    "\n",
    "def add_cr_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "\n",
    "def get_label_data(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        for sentence in essay.sentences:\n",
    "            unique_cr_tags = set()\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "            add_cr_labels(unique_cr_tags, ys_bytag_sent)\n",
    "    return ys_bytag_sent\n",
    "\n",
    "def get_label_data_essay_level(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_essay = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        add_cr_labels(unique_cr_tags, ys_bytag_essay)\n",
    "    return ys_bytag_essay\n",
    "\n",
    "def essay_to_crels(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    name2crels = defaultdict(set)\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        name2crels[essay.name] = unique_cr_tags\n",
    "    return dict(name2crels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurevectorizer import FeatureVectorizer\n",
    "\n",
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_micro_metrics(df):\n",
    "    return df[df.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"recall\", \"precision\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from Data.featurevectorizer import FeatureVectorizer\n",
    "\n",
    "class DependencyFeatureInputs(object):\n",
    "    def __init__(self, essay_name, lsent_ix, rsent_ix, causer_tag, result_tag, causer_words, between_words,\n",
    "                 result_words,\n",
    "                 causer_first, between_codes, num_sentences_between):\n",
    "        self.essay_name = essay_name\n",
    "        self.lsent_ix = lsent_ix\n",
    "        self.rsent_ix = rsent_ix\n",
    "        self.num_sentences_between = num_sentences_between\n",
    "        self.between_codes = between_codes\n",
    "        self.causer_first = causer_first\n",
    "        self.result_words = result_words\n",
    "        self.between_words = between_words\n",
    "        self.causer_words = causer_words\n",
    "        self.result_tag = result_tag\n",
    "        self.causer_tag = causer_tag\n",
    "        self.crel = \"Causer:{a}->Result:{b}\".format(a=causer_tag, b=result_tag)\n",
    "\n",
    "class DependencyClassifier(object):\n",
    "    def __init__(self, classifier_fn=LogisticRegression, negative_label=0, sentence_span=2,\n",
    "                 min_feat_freq=10, log_fn=lambda s: print(s), ):\n",
    "        self.log = log_fn\n",
    "        self.epoch = 0\n",
    "        self.negative_label = negative_label\n",
    "        self.sentence_span = sentence_span\n",
    "        self.min_feat_freq = min_feat_freq\n",
    "        self.vectorizer = FeatureVectorizer(min_feature_freq=min_feat_freq)\n",
    "        self.clf = classifier_fn()\n",
    "        self.fit_vectorizer = False\n",
    "            \n",
    "    def __fill_in_gaps__(self, tag_seq):\n",
    "        new_tag_seq = []\n",
    "        for i, tag in enumerate(tag_seq):\n",
    "            if tag == EMPTY \\\n",
    "                    and i > 0 \\\n",
    "                    and tag_seq[i - 1] != EMPTY \\\n",
    "                    and i < len(tag_seq) - 1 \\\n",
    "                    and tag_seq[i - 1] == tag_seq[i + 1]:\n",
    "                tag = tag_seq[i - 1]\n",
    "\n",
    "            new_tag_seq.append(tag)\n",
    "        return new_tag_seq\n",
    "\n",
    "    def __compute_tag_2_spans__(self, essay):\n",
    "        sent_tag2spans = []\n",
    "        wd_ix = -1\n",
    "        essay_words = []\n",
    "        essay_ptags = []\n",
    "        for sent_ix in range(len(essay.sentences)):\n",
    "            words, tag_seq = zip(*essay.sentences[sent_ix])\n",
    "\n",
    "            tag2spans = []  # maps to list of start and end spans for each tag\n",
    "            sent_tag2spans.append(tag2spans)\n",
    "\n",
    "            last_tag = EMPTY\n",
    "            tag_start = None\n",
    "            ptags_sent = self.__fill_in_gaps__(essay.pred_tagged_sentences[sent_ix])\n",
    "            current_crel_tags = set()\n",
    "            for i, ptag in enumerate(ptags_sent):\n",
    "                wd_ix += 1\n",
    "                essay_words.append(words[i])\n",
    "                essay_ptags.append(ptag)\n",
    "                # Tag changed\n",
    "                if ptag != last_tag:\n",
    "                    if last_tag != EMPTY:\n",
    "                        tag2spans.append((last_tag, tag_start, wd_ix - 1, sent_ix, current_crel_tags))\n",
    "                    tag_start = wd_ix\n",
    "                    current_crel_tags = set()\n",
    "                current_crel_tags.update(to_is_valid_crel(tag_seq[i]))\n",
    "                last_tag = ptag\n",
    "            if last_tag != EMPTY:\n",
    "                tag2spans.append((last_tag, tag_start, wd_ix, len(essay.sentences) - 1, current_crel_tags))\n",
    "        assert len(essay_words) == len(essay_ptags)\n",
    "        return sent_tag2spans, essay_words, essay_ptags\n",
    "\n",
    "    def __combine_feats__(self, ftsa, ftsb):\n",
    "        fts = {}\n",
    "        for a, aval in ftsa.items():\n",
    "            for b, bval in ftsb.items():\n",
    "                fts[a + \"|\" + b] = aval * bval\n",
    "        return fts\n",
    "\n",
    "    def create_features(self, feat_inp):\n",
    "        feats = {}\n",
    "        feats[feat_inp.crel] = 1\n",
    "        feats[\"Causer:{tag}\".format(tag=feat_inp.causer_tag)] = 1\n",
    "        feats[\"Result:{tag}\".format(tag=feat_inp.result_tag)] = 1\n",
    "        cs_fts, res_fts = {}, {}\n",
    "        for wd in feat_inp.causer_words:\n",
    "            cs_fts[\"Causer:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(cs_fts)\n",
    "        for wd in feat_inp.result_words:\n",
    "            res_fts[\"Result:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(res_fts)\n",
    "        feats.update(self.__combine_feats__(cs_fts, res_fts))\n",
    "        btwn_fts = {}\n",
    "        for wd in feat_inp.between_words:\n",
    "            btwn_fts[\"Between:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(btwn_fts)\n",
    "        #         feats.update(self.__combine_feats__(cs_fts, btwn_fts))\n",
    "        #         feats.update(self.__combine_feats__(res_fts, btwn_fts))\n",
    "        if feat_inp.causer_first:\n",
    "            feats[\"Left2Right\"] = 1\n",
    "        else:\n",
    "            feats[\"Right2Left\"] = 1\n",
    "\n",
    "        if feat_inp.num_sentences_between == 0:\n",
    "            feats[\"SameSentence\"] = 1\n",
    "        feats[\"SentBetween\"] = feat_inp.num_sentences_between\n",
    "        if feat_inp.num_sentences_between <= 1:\n",
    "            feats[\"SentBetween<=1\"] = 1\n",
    "        if feat_inp.num_sentences_between <= 2:\n",
    "            feats[\"SentBetween<=2\"] = 1\n",
    "        else:\n",
    "            feats[\"SentBetween>2\"] = 1\n",
    "\n",
    "        num_codes_between = len(feat_inp.between_codes)\n",
    "        feats[\"CodesBetween\"] = num_codes_between\n",
    "        if num_codes_between <= 1:\n",
    "            feats[\"CodesBetween<=1\"] = 1\n",
    "        if num_codes_between <= 2:\n",
    "            feats[\"CodesBetween<=2\"] = 1\n",
    "        else:\n",
    "            feats[\"CodesBetween>2\"] = 1\n",
    "        return feats\n",
    "\n",
    "    def __generate_training_data__(self, essays):\n",
    "        xs, ys, essay_sent_feat_inpts = [], [], []\n",
    "        for essay_ix, essay in enumerate(essays):\n",
    "            sent_tag2spans, essay_words, essay_ptags = self.__compute_tag_2_spans__(essay)\n",
    "            for sent_ix in range(len(sent_tag2spans)):\n",
    "                # tag 2 spans for sentence\n",
    "                next_tag2spans = []\n",
    "                # grab next few sentences' predicted tags\n",
    "                for offset in range(0, self.sentence_span + 1):\n",
    "                    if (sent_ix + offset) < len(sent_tag2spans):\n",
    "                        next_tag2spans.extend(sent_tag2spans[sent_ix + offset])\n",
    "\n",
    "                for ltag_ix, (ltag, lstart_ix, lend_ix, lsent_ix, lcrels) in enumerate(sent_tag2spans[sent_ix]):\n",
    "                    for rtag, rstart_ix, rend_ix, rsent_ix, rcrels in next_tag2spans[ltag_ix + 1:]:\n",
    "                        num_sent_between = rsent_ix - lsent_ix\n",
    "\n",
    "                        ltag_words = essay_words[lstart_ix:lend_ix + 1]\n",
    "                        between_words = essay_words[lend_ix + 1:rstart_ix]\n",
    "                        rtag_words = essay_words[rstart_ix:rend_ix + 1]\n",
    "                        between_codes = essay_ptags[lend_ix + 1:rstart_ix]\n",
    "\n",
    "                        lbls = set(lcrels).union(rcrels)\n",
    "\n",
    "                        feat_ext_inp = DependencyFeatureInputs(essay_name=essay.name, lsent_ix=lsent_ix,\n",
    "                                                               rsent_ix=rsent_ix,\n",
    "                                                               causer_tag=ltag, result_tag=rtag,\n",
    "                                                               causer_words=ltag_words, between_words=between_words,\n",
    "                                                               result_words=rtag_words, causer_first=True,\n",
    "                                                               between_codes=between_codes,\n",
    "                                                               num_sentences_between=num_sent_between)\n",
    "                        x = self.create_features(feat_ext_inp)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if feat_ext_inp.crel in lbls else self.negative_label)\n",
    "                        essay_sent_feat_inpts.append(feat_ext_inp)\n",
    "\n",
    "                        feat_ext_inp = DependencyFeatureInputs(essay_name=essay.name, lsent_ix=lsent_ix,\n",
    "                                                               rsent_ix=rsent_ix,\n",
    "                                                               causer_tag=rtag, result_tag=ltag,\n",
    "                                                               causer_words=rtag_words, between_words=between_words,\n",
    "                                                               result_words=ltag_words, causer_first=False,\n",
    "                                                               between_codes=between_codes,\n",
    "                                                               num_sentences_between=num_sent_between)\n",
    "                        x = self.create_features(feat_ext_inp)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if feat_ext_inp.crel in lbls else self.negative_label)\n",
    "                        essay_sent_feat_inpts.append(feat_ext_inp)\n",
    "        \n",
    "        if not self.fit_vectorizer:\n",
    "            xs_array = self.vectorizer.fit_transform(xs)\n",
    "            self.fit_vectorizer = True\n",
    "        else:            \n",
    "            xs_array = self.vectorizer.transform(xs)\n",
    "        return xs_array, ys, essay_sent_feat_inpts\n",
    "\n",
    "    def train(self, train_essays):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=train_essays)\n",
    "        self.clf.fit(X=xs, y=ys)\n",
    "\n",
    "    def __group_predictions_by_essay__(self, essay_sent_feat_inpts, preds, threshold):\n",
    "        name2pred = defaultdict(set)\n",
    "        for feat_inputs, pred in zip(essay_sent_feat_inpts, preds):\n",
    "            if pred >= threshold:\n",
    "                name2pred[feat_inputs.essay_name].add(feat_inputs.crel)\n",
    "        return name2pred\n",
    "\n",
    "    def __group_predictions_by_sentence__(self, essay_sent_feat_inpts, preds, threshold):\n",
    "        namesent2pred = defaultdict(set)\n",
    "        for feat_inputs, pred in zip(essay_sent_feat_inpts, preds):\n",
    "            if pred >= threshold:\n",
    "                namesent2pred[(feat_inputs.essay_name, feat_inputs.lsent_ix)].add(feat_inputs.crel)\n",
    "                namesent2pred[(feat_inputs.essay_name, feat_inputs.rsent_ix)].add(feat_inputs.crel)\n",
    "        return namesent2pred\n",
    "\n",
    "    def predict_probability(self, tagged_essays, min_prob=0.1):\n",
    "        # Get predicted probabilities\n",
    "        xs, _, essay_sent_feat_inpts = self.__generate_training_data__(essays=tagged_essays)\n",
    "        probs = self.clf.predict_proba(xs)[:, 1]\n",
    "        name2pred = defaultdict(list)\n",
    "        for feat_inputs, prob in zip(essay_sent_feat_inpts, probs):\n",
    "            if prob >= min_prob:\n",
    "                name2pred[feat_inputs.essay_name].append((feat_inputs, prob))\n",
    "        return name2pred\n",
    "\n",
    "    def evaluate(self, tagged_essays, print_classification_report=True):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_feat_inpts = self.__generate_training_data__(essays=tagged_essays)\n",
    "        preds = self.clf.predict(xs)\n",
    "        if print_classification_report:\n",
    "            print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        namesent2pred = self.__group_predictions_by_sentence__(\n",
    "            essay_sent_feat_inpts=essay_sent_feat_inpts, preds=preds, threshold=1.0)\n",
    "\n",
    "        pred_ys_bytag_sent = defaultdict(list)\n",
    "        for essay in tagged_essays:\n",
    "            for sent_ix, sentence in enumerate(essay.sentences):\n",
    "                unique_cr_tags = namesent2pred[(essay.name, sent_ix)]\n",
    "                add_cr_labels(unique_cr_tags, pred_ys_bytag_sent)\n",
    "        return pred_ys_bytag_sent\n",
    "\n",
    "    def evaluate_essay_level(self, tagged_essays, print_classification_report=True):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_feat_inpts = self.__generate_training_data__(essays=tagged_essays)\n",
    "        preds = self.clf.predict(xs)\n",
    "        if print_classification_report:\n",
    "            print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        namesent2pred = self.__group_predictions_by_essay__(\n",
    "            essay_sent_feat_inpts=essay_sent_feat_inpts, preds=preds, threshold=1.0)\n",
    "\n",
    "        pred_ys_bytag_essay = defaultdict(list)\n",
    "        for essay in tagged_essays:\n",
    "            unique_cr_tags = namesent2pred[essay.name]\n",
    "            add_cr_labels(unique_cr_tags, pred_ys_bytag_essay)\n",
    "        return pred_ys_bytag_essay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_accuracy(parser, essays):\n",
    "    ys_bytag_sent = get_label_data(tagged_essays=essays)\n",
    "    pred_ys_bytag_sent = parser.evaluate(tagged_essays=essays, print_classification_report=False)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_sent, pred_ys_bytag_sent)\n",
    "    return get_micro_metrics(metrics_to_df(mean_metrics))\n",
    "\n",
    "def compute_essay_accuracy(parser, essays):\n",
    "    ys_bytag_essay = get_label_data_essay_level(tagged_essays=essays)\n",
    "    pred_ys_bytag_essay = parser.evaluate_essay_level(tagged_essays=essays, print_classification_report=False)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_essay, pred_ys_bytag_essay)\n",
    "    return get_micro_metrics(metrics_to_df(mean_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Accuracy is Equivalent to Parser Model (Or Very Close) When We Don't Look Across Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      9036\n",
      "          1       0.89      0.91      0.90      2916\n",
      "\n",
      "avg / total       0.95      0.95      0.95     11952\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.91      0.92      1868\n",
      "          1       0.73      0.78      0.75       586\n",
      "\n",
      "avg / total       0.88      0.88      0.88      2454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.997892</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.735714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.997892  0.691275  0.651899   0.735714"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DependencyClassifier(sentence_span=0)\n",
    "parser.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent = parser.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test = parser.evaluate(pred_tagged_essays_test)\n",
    "\n",
    "compute_sentence_accuracy(parser, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.985948</td>\n",
       "      <td>0.723973</td>\n",
       "      <td>0.69287</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score   recall  precision\n",
       "95  0.985948  0.723973  0.69287      0.758"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_essay_accuracy(parser, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97     44627\n",
      "          1       0.85      0.80      0.82      7487\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52114\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94      9280\n",
      "          1       0.61      0.60      0.60      1484\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.993337</td>\n",
       "      <td>0.435711</td>\n",
       "      <td>0.710443</td>\n",
       "      <td>0.314206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.993337  0.435711  0.710443   0.314206"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser2 = DependencyClassifier(sentence_span=2)\n",
    "parser2.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent2 = parser2.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test2 = parser2.evaluate(pred_tagged_essays_test)\n",
    "\n",
    "compute_sentence_accuracy(parser2, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.980891</td>\n",
       "      <td>0.6661</td>\n",
       "      <td>0.716636</td>\n",
       "      <td>0.622222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.980891    0.6661  0.716636   0.622222"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_essay_accuracy(parser2, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data (Predictions from First Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay2crels_train = essay_to_crels(pred_tagged_essays_train)\n",
    "essay2crels_test  = essay_to_crels(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = parser.predict_probability(pred_tagged_essays_train)\n",
    "probs_test  = parser.predict_probability(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train2 = parser2.predict_probability(pred_tagged_essays_train)\n",
    "probs_test2  = parser2.predict_probability(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Structured Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class StructuredPerceptron(object):\n",
    "    '''A structured perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self, learning_rate=0.3, max_update_items=1):\n",
    "        # Each feature gets its own weight\n",
    "        # needs to be non zero otherwise first\n",
    "        self.weights = defaultdict(lambda : 1.0)\n",
    "        self.learning_rate = learning_rate\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "        # how many items do we use to update the weights?\n",
    "        self.max_update_items = max_update_items\n",
    "\n",
    "    def clone(self):\n",
    "        p = StructuredPerceptron(self.learning_rate)\n",
    "        p.weights.update(self.weights)\n",
    "        p._totals.update(self._totals)\n",
    "        p._tstamps.update(self._tstamps)\n",
    "        p.i = self.i\n",
    "        return p\n",
    "\n",
    "    def rank(self, features_array, existence_check=True):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores2index = {}\n",
    "        for i, feats in enumerate(features_array):\n",
    "            scores2index[i] = self.decision_function(feats, existence_check)\n",
    "        # return a ranking of the scores, by best to worse\n",
    "        return [ix for ix, score in sorted(scores2index.items(), key=lambda tpl: -tpl[-1])]\n",
    "\n",
    "    def decision_function(self, features, existence_check=True):\n",
    "        '''Dot-product the features and current weights and return the score.'''\n",
    "        score = 0.0\n",
    "        for feat, value in features.items():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            if existence_check and feat not in self.weights:\n",
    "                continue\n",
    "            score += self.weights[feat] * value\n",
    "        return score\n",
    "\n",
    "    def train(self, best_feats, other_feats_array):\n",
    "        feats_array = [best_feats] + list(other_feats_array)\n",
    "        ixs = self.rank(feats_array, existence_check=False)\n",
    "\n",
    "        # go thru up to |max_update_items| items ranked above the best, and update the weights\n",
    "        best_ix = ixs[0]\n",
    "        if best_ix != 0:\n",
    "            for rank, ix in enumerate(ixs):\n",
    "                # don't update items ranked below the best parse\n",
    "                if ix == 0 or rank >= self.max_update_items:\n",
    "                    break\n",
    "\n",
    "                self.update(best_feats=best_feats, highest_ranked_feats=feats_array[ix])\n",
    "\n",
    "    def __upd_feat__(self, feat, val):\n",
    "        w = self.weights[feat]\n",
    "        # update the totals by the number of timestamps the current value has survived * val\n",
    "        self._totals[feat] += (self.i - self._tstamps[feat]) * w\n",
    "        # store latest update timestamp\n",
    "        self._tstamps[feat] = self.i\n",
    "        # finally, update the current weight\n",
    "        self.weights[feat] = w + (self.learning_rate * val)\n",
    "\n",
    "    def update(self, best_feats, highest_ranked_feats):\n",
    "        '''Update the feature weights.'''\n",
    "\n",
    "        self.i += 1\n",
    "        for feat, weight in self.weights.items():\n",
    "            val = best_feats[feat] - highest_ranked_feats[feat]\n",
    "            self.__upd_feat__(feat, val)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        new_feat_weights = defaultdict(float)\n",
    "        for feat, weight in self.weights.items():\n",
    "            total = self._totals[feat]\n",
    "            total += (self.i - self._tstamps[feat]) * weight\n",
    "            averaged = round(total / float(self.i), 5)\n",
    "            if averaged != 0.0:\n",
    "                new_feat_weights[feat] = averaged\n",
    "        self.weights = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "class CostSensitiveStructuredPerceptron(StructuredPerceptron):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CostSensitiveStructuredPerceptron, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def train(self, best_feats, other_feats_array, other_costs_array):\n",
    "        \n",
    "        feats_array = [best_feats] + list(other_feats_array)\n",
    "        costs_array = [0] + other_costs_array\n",
    "        ixs = self.rank(feats_array, existence_check=False)\n",
    "\n",
    "        # go thru up to |max_update_items| items ranked above the best, and update the weights\n",
    "        best_ix = ixs[0]\n",
    "        if best_ix != 0:\n",
    "            for rank, ix in enumerate(ixs):\n",
    "                # don't update items ranked below the best parse\n",
    "                if ix == 0 or rank >= self.max_update_items:\n",
    "                    break\n",
    "\n",
    "                self.update(best_feats=best_feats, \n",
    "                            highest_ranked_feats=feats_array[ix], highest_ranked_cost=costs_array[ix])\n",
    "\n",
    "    def update(self, best_feats, highest_ranked_feats, highest_ranked_cost):\n",
    "        '''Update the feature weights.'''\n",
    "\n",
    "        self.i += 1\n",
    "        for feat, weight in self.weights.items():\n",
    "            val = (best_feats[feat] - highest_ranked_feats[feat]) * highest_ranked_cost\n",
    "            self.__upd_feat__(feat, val)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.204109589041096, 4.0, 16, 6.0)"
      ]
     },
     "execution_count": 1030,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# look at the number of predicted items\n",
    "lens = []\n",
    "for ename, lst in probs_train.items():\n",
    "    crels = set()\n",
    "    for fts, p in lst:\n",
    "        crels.add(fts.crel)\n",
    "    lens.append(len(crels))\n",
    "\n",
    "np.mean(lens), np.median(lens), np.max(lens), np.percentile(lens, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "()\n",
      "(1,)\n",
      "(2,)\n",
      "(3,)\n",
      "(1, 2)\n",
      "(1, 3)\n",
      "(2, 3)\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def get_all_combos(items):\n",
    "    # enforces a consistent ordering for the resulting tuples\n",
    "    items = sorted(items) \n",
    "    cbos = [()] # seed with the empty combo\n",
    "    for i in range(1, len(items)+1):\n",
    "        cbos.extend(combinations(items,i))\n",
    "    return cbos\n",
    "\n",
    "cbos = get_all_combos([3,2,1])\n",
    "print(len(cbos)) # 2**len(items)-1\n",
    "if len(cbos) < 1000:\n",
    "    for cbo in sorted(cbos, key = lambda l: (len(l), l)):\n",
    "        print(cbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('12->50',),\n",
       " ('1->2', '10->12', '12->50', '5->8'),\n",
       " ('10->12', '12->50'),\n",
       " ('10->12', '12->50', '5->8'),\n",
       " ('1->2', '12->50', '5->8'),\n",
       " (),\n",
       " ('1->2', '12->50'),\n",
       " ('1->2', '10->12', '12->50')]"
      ]
     },
     "execution_count": 1111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample_top_parses(crel2maxprobs, top_n):\n",
    "\n",
    "    max_parses = 2**len(crel2maxprobs) # maximum parse combinations\n",
    "    assert max_parses > top_n, (max_parses, top_n) # otherwise brute force it\n",
    "\n",
    "    top_parses = set([()]) # always seed with the empty parse\n",
    "    probs = []\n",
    "    while len(top_parses) < top_n:\n",
    "        new_parse = []\n",
    "        for crel, prob in crel2maxprobs.items():\n",
    "            rand_val = np.random.random() # random number >= 0 and < 1\n",
    "            if rand_val < prob:\n",
    "                new_parse.append(crel)\n",
    "        # make hashable and enforce consistent order\n",
    "        top_parses.add(tuple(sorted(new_parse)))\n",
    "    \n",
    "    return list(top_parses)\n",
    "\n",
    "def get_top_parses(crel2maxprobs, threshold=0.5):\n",
    "\n",
    "    top_parse = [crel for crel, prob in crel2maxprobs.items() if prob >= threshold]\n",
    "    if top_parse:\n",
    "        return [tuple(sorted(top_parse))]\n",
    "    else:\n",
    "        return [()]\n",
    "\n",
    "crel_probs = {\n",
    "    \"1->2\":   0.8,\n",
    "    \"2->3\":   0.01,\n",
    "    \"5->8\":   0.25,\n",
    "    \"10->12\": 0.75,\n",
    "    \"12->50\": 0.99,\n",
    "}\n",
    "\n",
    "# important - should see a lot more of the more probable codes\n",
    "sample_top_parses(crel_probs, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NgramGenerator import compute_ngrams\n",
    "\n",
    "def to_short_tag(tag):\n",
    "    return tag.replace(\"Causer:\",\"\").replace(\"Result:\", \"\")\n",
    "\n",
    "def build_chains_inner(tree, l, visited, depth=0):\n",
    "    chains = []\n",
    "    if l not in tree:\n",
    "        return chains\n",
    "    for r in tree[l]:\n",
    "        if r in visited:\n",
    "            continue\n",
    "        visited.add(r) # needed to prevent cycles, which cause infinite recursion\n",
    "        extensions = build_chains_inner(tree, r, visited, depth+1)\n",
    "        visited.remove(r)\n",
    "        for ch in extensions:\n",
    "            chains.append([r] + ch)\n",
    "        if not extensions:\n",
    "            chains.append([r])\n",
    "    return chains\n",
    "\n",
    "def build_chains(tree):    \n",
    "    lhs_items = set(tree.keys())\n",
    "    rhs_items = set()\n",
    "    for l,rhs in tree.items():        \n",
    "        rhs_items.update(rhs)\n",
    "    \n",
    "    chains = []\n",
    "    # starting positions of each chain are those appearing on the lhs but not the rhs\n",
    "    start_codes = lhs_items - rhs_items    \n",
    "    for l in start_codes:\n",
    "        rhs = tree[l]\n",
    "        for r in rhs:\n",
    "            for ch in build_chains_inner(tree, r, {l,r}, 0):\n",
    "                chains.append([l,r] + ch)\n",
    "    return chains\n",
    "\n",
    "def extend_chains(chains):\n",
    "    ext_chains = set()\n",
    "    for tokens in chains:\n",
    "        ext_chains.add(\",\".join(tokens))\n",
    "        ngrams = compute_ngrams(tokens,max_len=None, min_len=3)\n",
    "        for t in ngrams:\n",
    "            ext_chains.add(\",\".join(t))\n",
    "    return ext_chains\n",
    "\n",
    "def extract_features_from_parse(parse, crel2probs):\n",
    "    \n",
    "    feats = defaultdict(float)\n",
    "    tree = defaultdict(set) # maps causers to effects for building chains\n",
    "    max_probs = []    \n",
    "    code_tally = defaultdict(float)\n",
    "    \n",
    "    pairs = set()\n",
    "    inverted_count = 0\n",
    "    for crel in parse:\n",
    "        probs = crel2probs[crel]\n",
    "        max_p = max(probs)\n",
    "        max_probs.append(max_p)\n",
    "        feats[\"{crel}-MAX(prob)\".format(crel=crel)] = max_p\n",
    "        feats[\"{crel}-MIN(prob)\".format(crel=crel)] = min(probs)\n",
    "        feats[\"{crel}-pred-count\".format(crel=crel)] = len(probs)\n",
    "        feats[\"{crel}-pred-count={count}\".format(crel=crel, count=len(probs))] = 1\n",
    "        \n",
    "        # with type\n",
    "        l,r = crel.split(\"->\")\n",
    "        code_tally[l] +=1\n",
    "        code_tally[r] +=1\n",
    "        \n",
    "        # without type\n",
    "        l_short, r_short = to_short_tag(l), to_short_tag(r)\n",
    "        code_tally[l_short] +=1\n",
    "        code_tally[r_short] +=1\n",
    "        # ordering of the codes, ignoring the causal direction\n",
    "        feats[l_short + \":\" + r_short] = 1\n",
    "        \n",
    "        # build tree structure so we can retrieve the chains\n",
    "        tree[l_short].add(r_short)\n",
    "        \n",
    "        # track whether the rule exists in the opposite direction\n",
    "        pairs.add((l_short,r_short))\n",
    "        if (r_short,l_short) in pairs:\n",
    "            inverted_count += 1\n",
    "            \n",
    "    if inverted_count:\n",
    "        feats[\"inverted\"] = 1\n",
    "        feats[\"num_inverted\"] = inverted_count\n",
    "    else:\n",
    "        feats[\"not_inverted\"] = 1\n",
    "    \n",
    "    # counts\n",
    "    feats.update(code_tally)\n",
    "    num_crels = len(parse)\n",
    "    feats[\"num_crels\"] = num_crels\n",
    "    feats[\"num_crels=\"+str(len(parse))] = 1 # includes a tag for the empty parse\n",
    "    for i in range(1,11):\n",
    "        if num_crels <= i:\n",
    "            feats[\"num_crels<={i}\".format(i=i)] = 1\n",
    "        else:\n",
    "            feats[\"num_crels>{i}\".format(i=i)] = 1\n",
    "        \n",
    "    # combination of crels\n",
    "    # need to sort so that order of a and b is consistent across parses\n",
    "    pairs = combinations(sorted(parse), r=2)\n",
    "    for a, b in pairs:\n",
    "        feats[\"{a}|{b}\".format(a=a, b=b)] = 1\n",
    "        \n",
    "    #chains\n",
    "    causer_chains = extend_chains(build_chains(tree))\n",
    "    for ch in causer_chains:\n",
    "        feats[\"CChain:\" + ch] = 1\n",
    "    \n",
    "    if max_probs: # might be an empty parse\n",
    "        for cutoff in [0.2, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95]:\n",
    "            above =  len([p for p in max_probs if p >=cutoff])\n",
    "            feats[\"Above-{cutoff}\".format(cutoff=cutoff)] = above\n",
    "            feats[\"%-Above-{cutoff}\".format(cutoff=cutoff)] = above/len(max_probs)\n",
    "            if above == len(max_probs):\n",
    "                feats[\"All-Above-{cutoff}\".format(cutoff=cutoff)] = 1\n",
    "        \n",
    "        feats[\"avg-prob\"] = np.mean(max_probs)\n",
    "        feats[\"med-prob\"] = np.median(max_probs)\n",
    "        feats[\"prod-prob\"]= np.product(max_probs)\n",
    "        feats[\"min-prob\"] = np.min(max_probs)\n",
    "        feats[\"max-prob\"] = np.max(max_probs)\n",
    "        for p in [5, 10, 25, 75, 90, 95]:\n",
    "            feats[\"{p}%-prob\".format(p=p)] = np.percentile(max_probs, p)\n",
    "        # geometric mean\n",
    "        feats[\"geo-mean\"] = np.prod(max_probs)**(1/len(max_probs))\n",
    "    return feats\n",
    "\n",
    "def additional_features(parse, feats_input):\n",
    "    #TODO - ratio of number of concept codes to number of relations\n",
    "    #TODO - average, min and max word distance between codes in a relation\n",
    "    #TODO - average, min and sentence distance between codes in a relation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranker(model, xs, essay2crels, ys_bytag):\n",
    "    clone = model.clone()\n",
    "    if hasattr(model, \"average_weights\"):\n",
    "        clone.average_weights()\n",
    "    rank_acc = []\n",
    "    pred_ys_bytag = defaultdict(list)\n",
    "    ename2inps = dict()\n",
    "    for parser_input in xs:\n",
    "        ename2inps[parser_input.essay_name] = parser_input\n",
    "    \n",
    "    for ename, act_crels in essay2crels.items():        \n",
    "        if ename not in ename2inps:\n",
    "            # no predicted crels for this essay\n",
    "            highest_ranked = set()\n",
    "        else:\n",
    "            parser_input = ename2inps[ename]\n",
    "            ixs = clone.rank(parser_input.all_feats_array)\n",
    "            highest_ranked = parser_input.all_parses[ixs[0]] # type: Tuple[str]        \n",
    "            rank_acc.append(1 if highest_ranked == parser_input.opt_parse else 0)\n",
    "            \n",
    "        add_cr_labels(set(highest_ranked), pred_ys_bytag)\n",
    "\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag, pred_ys_bytag)\n",
    "    df = get_micro_metrics(metrics_to_df(mean_metrics))\n",
    "    df[\"rank_acc\"] = np.mean(rank_acc)\n",
    "    return df\n",
    "\n",
    "# straw man model - use predicted parse instead\n",
    "def eval_use_best_parse(xs, essay2crels, prob_threshold=0.5):\n",
    "    preds = []\n",
    "    for parser_inp in xs:\n",
    "        act_crels = essay2crels[parser_inp.essay_name]\n",
    "        crel2maxprob = defaultdict(float)\n",
    "        for crel, probs in parser_inp.crel2probs.items():\n",
    "            crel2maxprob[crel] = max(probs)\n",
    "\n",
    "        pred_crels = [crel for crel, p in crel2maxprob.items() if p >= prob_threshold]\n",
    "        parse = tuple(sorted(pred_crels))\n",
    "        opt_parse = tuple(sorted(act_crels.intersection(crel2maxprob.keys())))\n",
    "        preds.append(1 if opt_parse == parse else 0)\n",
    "    return np.mean(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_costs(parser_input):\n",
    "    opt_parse = parser_input.opt_parse\n",
    "    other_parses = parser_input.other_parses\n",
    "\n",
    "    other_costs = []\n",
    "    op = set(opt_parse)\n",
    "    for p in other_parses:\n",
    "        p = set(p)\n",
    "        fp = p - op\n",
    "        fn = op - p\n",
    "        cost = len(fp) + len(fn)\n",
    "        other_costs.append(cost)\n",
    "    return other_costs\n",
    "\n",
    "class ParserInputs(object):\n",
    "    def __init__(self, essay_name, opt_parse, all_parses, crel2probs, dep_features):\n",
    "        self.essay_name = essay_name\n",
    "        self.opt_parse = opt_parse\n",
    "        self.opt_features = extract_features_from_parse(opt_parse, crel2probs)\n",
    "        self.crel2probs = crel2probs\n",
    "        \n",
    "        other_parses = []\n",
    "        other_feats_array = []\n",
    "        all_feats_array = []\n",
    "        for p in all_parses:\n",
    "            feats = extract_features_from_parse(p, crel2probs)\n",
    "            all_feats_array.append(feats)\n",
    "            if p != opt_parse:\n",
    "                other_parses.append(p)\n",
    "                other_feats_array.append(feats)\n",
    "        \n",
    "        self.all_parses = all_parses\n",
    "        self.all_feats_array = all_feats_array\n",
    "        self.other_parses = other_parses\n",
    "        self.other_features_array = other_feats_array\n",
    "        self.dep_features = dep_features\n",
    "        self.other_costs_array = compute_costs(self)\n",
    "\n",
    "def to_freq_feats(feats, freq_feats):\n",
    "    new_feats = defaultdict(float)\n",
    "    for f, v in feats.items():\n",
    "        if f in freq_feats:\n",
    "            new_feats[f] = v\n",
    "    return new_feats\n",
    "\n",
    "def filter_by_min_freq(xs, feat_freq, min_freq):\n",
    "    if min_freq <= 1:\n",
    "        return xs\n",
    "    freq_feats = set((f for f, cnt in feat_freq.items() if cnt >= min_freq))\n",
    "    for parser_input in xs:\n",
    "        parser_input.opt_features = to_freq_feats(parser_input.opt_features, freq_feats)\n",
    "        parser_input.other_features_array = [to_freq_feats(x, freq_feats)\n",
    "                                             for x in parser_input.other_features_array]\n",
    "    return xs\n",
    "\n",
    "def get_features_from_probabilities(probs, essay2lbls, top_n, min_feat_freq=1):\n",
    "    xs = []\n",
    "    feat_freq = defaultdict(int)\n",
    "    \n",
    "    for ename, lst in probs.items():\n",
    "        \n",
    "        act_crels = essay2lbls[ename]\n",
    "        crel2probs = defaultdict(list)\n",
    "        crel2maxprob = defaultdict(float)\n",
    "        dep_features = defaultdict(list)\n",
    "        for fts, prob in lst:\n",
    "            crel2probs[fts.crel].append(prob)\n",
    "            crel2maxprob[fts.crel] = max(crel2maxprob[fts.crel], prob)\n",
    "            dep_features[fts.crel].append((prob, fts))\n",
    "        \n",
    "        crel2probs = dict(crel2probs)\n",
    "        crel2maxprob = dict(crel2maxprob)\n",
    "\n",
    "        num_crels = len(crel2probs)\n",
    "        max_parses = 2 ** num_crels\n",
    "        if max_parses > 2 * top_n:\n",
    "            #parses = sample_top_parses(crel2maxprob, top_n)\n",
    "            #parses.extend(get_top_parses(crel2maxprob))  # just get the predicted parses (probability >= 0.5)\n",
    "            parses = get_top_parses(crel2maxprob)  # just get the predicted parses (probability >= 0.5)\n",
    "        else:\n",
    "            # brute force it\n",
    "            parses = get_all_combos(crel2probs.keys())\n",
    "\n",
    "        # constrain optimal parse to only those crels that are predicted\n",
    "        opt_parse = tuple(sorted(act_crels.intersection(crel2probs.keys())))\n",
    "        xs.append(ParserInputs(essay_name=ename, opt_parse=opt_parse, all_parses=parses, crel2probs=crel2probs, dep_features=dep_features))\n",
    "\n",
    "        # Get unique features for essay\n",
    "        all_feats = set(opt_feats.keys())\n",
    "        for fts in feats_array:\n",
    "            all_feats.update(fts.keys())\n",
    "\n",
    "        for ft in all_feats:\n",
    "            feat_freq[ft] += 1\n",
    "\n",
    "    assert len(xs) == len(probs), \"Parses for all essays should be generated\"\n",
    "    return filter_by_min_freq(xs, feat_freq, min_feat_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1367,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train = get_features_from_probabilities(probs_train, essay2crels_train, top_n=500, min_feat_freq=1) # better with min feat freq of 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1368,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test  = get_features_from_probabilities(probs_test,  essay2crels_test,  top_n=500, min_feat_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1371,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pi in xs_train:\n",
    "    assert pi.essay_name in essay2crels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pi in xs_test:\n",
    "    assert pi.essay_name in essay2crels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Re-Ranker - Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "\n",
    "def train_instance(parser_input, model):\n",
    "    model.train(best_feats=parser_input.opt_features, other_feats_array=parser_input.other_features_array)\n",
    "\n",
    "def train_cost_sensitive_instance(parser_input, model):\n",
    "    model.train(best_feats=parser_input.opt_features, \n",
    "                other_feats_array=parser_input.other_features_array, other_costs_array=parser_input.other_costs_array)\n",
    "        \n",
    "def train_model(model, xs_train, xs_test, essay2crels_train, essay2crels_test, max_epochs=30, early_stop_iters=8, train_instance_fn=train_instance):\n",
    "    test_accs = [-1]\n",
    "    best_model = None\n",
    "    best_test_accuracy = None\n",
    "    num_declining_acc = 0\n",
    "\n",
    "    xs_train_copy = list(xs_train)    \n",
    "    for i in range(max_epochs):\n",
    "        shuffle(xs_train_copy)\n",
    "        for parser_input in xs_train_copy:\n",
    "            if len(parser_input.other_parses) > 0:\n",
    "                train_instance_fn(parser_input, model)\n",
    "\n",
    "        train_accuracy_df = evaluate_ranker(model, xs_train, essay2crels_train, ys_by_tag_train)\n",
    "        test_accuracy_df  = evaluate_ranker(model, xs_test,  essay2crels_test,  ys_by_tag_test)\n",
    "        train_accuracy = train_accuracy_df.iloc[0].to_dict()[\"f1_score\"]\n",
    "        test_accuracy  = test_accuracy_df.iloc[0].to_dict()[\"f1_score\"]\n",
    "        print(\"Epoch: {epoch} Train Accuracy: {train_acc:.4f} Test Accuracy: {test_acc:.4f}\".format(\n",
    "            epoch=i,  train_acc=train_accuracy, test_acc=test_accuracy))\n",
    "        if test_accuracy > max(test_accs):\n",
    "            best_model = model.clone()\n",
    "            best_test_accuracy = test_accuracy_df\n",
    "            num_declining_acc = 0\n",
    "        else:\n",
    "            num_declining_acc += 1\n",
    "            if num_declining_acc >= early_stop_iters:\n",
    "                break\n",
    "        test_accs.append(test_accuracy)\n",
    "    print(\"Best Test Acc: {acc:.4f}\".format(acc=max(test_accs)))\n",
    "    return best_model, best_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6287671232876713, 0.45454545454545453)"
      ]
     },
     "execution_count": 1311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute what is achieved if using only most probably crels\n",
    "eval_use_best_parse(xs_train, essay2crels_train), eval_use_best_parse(xs_test, essay2crels_test, prob_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1382,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_by_tag_train = get_label_data_essay_level(pred_tagged_essays_train)\n",
    "ys_by_tag_test  = get_label_data_essay_level(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7700 Test Accuracy: 0.7232\n",
      "Epoch: 1 Train Accuracy: 0.7766 Test Accuracy: 0.7290\n",
      "Epoch: 2 Train Accuracy: 0.7828 Test Accuracy: 0.7266\n",
      "Epoch: 3 Train Accuracy: 0.7871 Test Accuracy: 0.7269\n",
      "Epoch: 4 Train Accuracy: 0.7906 Test Accuracy: 0.7285\n",
      "Epoch: 5 Train Accuracy: 0.7940 Test Accuracy: 0.7292\n",
      "Epoch: 6 Train Accuracy: 0.7965 Test Accuracy: 0.7294\n",
      "Epoch: 7 Train Accuracy: 0.7988 Test Accuracy: 0.7280\n",
      "Epoch: 8 Train Accuracy: 0.7993 Test Accuracy: 0.7311\n",
      "Epoch: 9 Train Accuracy: 0.8002 Test Accuracy: 0.7324\n",
      "Epoch: 10 Train Accuracy: 0.8026 Test Accuracy: 0.7324\n",
      "Epoch: 11 Train Accuracy: 0.8038 Test Accuracy: 0.7318\n",
      "Epoch: 12 Train Accuracy: 0.8050 Test Accuracy: 0.7318\n",
      "Epoch: 13 Train Accuracy: 0.8074 Test Accuracy: 0.7313\n",
      "Epoch: 14 Train Accuracy: 0.8085 Test Accuracy: 0.7306\n",
      "Epoch: 15 Train Accuracy: 0.8086 Test Accuracy: 0.7306\n",
      "Epoch: 16 Train Accuracy: 0.8089 Test Accuracy: 0.7301\n",
      "Epoch: 17 Train Accuracy: 0.8100 Test Accuracy: 0.7308\n",
      "Epoch: 18 Train Accuracy: 0.8115 Test Accuracy: 0.7320\n",
      "Epoch: 19 Train Accuracy: 0.8129 Test Accuracy: 0.7313\n",
      "Best Test Acc: 0.7324\n"
     ]
    }
   ],
   "source": [
    "model = StructuredPerceptron(learning_rate=0.1, max_update_items=1) # best if learning_rate = 0.1 (0.05 also works well)\n",
    "best_model, test_acc_df = train_model(model, xs_train=xs_train, xs_test=xs_test, \n",
    "        essay2crels_train=essay2crels_train, essay2crels_test=essay2crels_test, \n",
    "        max_epochs=30, early_stop_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>rank_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.986531</td>\n",
       "      <td>0.732367</td>\n",
       "      <td>0.69287</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.465241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score   recall  precision  rank_acc\n",
       "95  0.986531  0.732367  0.69287   0.776639  0.465241"
      ]
     },
     "execution_count": 1386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.average_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('num_crels', -5.7953),\n",
       " ('num_crels<=7', 2.83349),\n",
       " ('num_crels<=8', 2.55895),\n",
       " ('num_crels<=6', 2.41824),\n",
       " ('num_crels<=9', 1.95066),\n",
       " ('num_crels<=5', 1.87638),\n",
       " ('num_crels>1', 1.75379),\n",
       " ('Above-0.2', -1.66547),\n",
       " ('Causer:7->Result:50-pred-count=1', 1.64672),\n",
       " ('num_crels>2', 1.62354)]"
      ]
     },
     "execution_count": 1388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(best_model.weights.items(), key = lambda tpl: -abs(tpl[1]))[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Causer:3->Result:5-pred-count', -0.00645),\n",
       " ('4', -0.01951),\n",
       " ('6', 0.02208),\n",
       " ('%-Above-0.7', -0.0223),\n",
       " ('num_crels>9', 0.04934),\n",
       " ('Result:7', -0.06707),\n",
       " ('All-Above-0.9', 0.08252),\n",
       " ('Above-0.9', 0.08303),\n",
       " ('12', 0.09229),\n",
       " ('5b', -0.09333)]"
      ]
     },
     "execution_count": 1389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(best_model.weights.items(), key = lambda tpl: abs(tpl[1]))[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Sensitive Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7915 Test Accuracy: 0.7294\n",
      "Epoch: 1 Train Accuracy: 0.7970 Test Accuracy: 0.7255\n",
      "Epoch: 2 Train Accuracy: 0.7999 Test Accuracy: 0.7336\n",
      "Epoch: 3 Train Accuracy: 0.8051 Test Accuracy: 0.7248\n",
      "Epoch: 4 Train Accuracy: 0.8063 Test Accuracy: 0.7237\n",
      "Epoch: 5 Train Accuracy: 0.8072 Test Accuracy: 0.7207\n",
      "Epoch: 6 Train Accuracy: 0.8083 Test Accuracy: 0.7257\n",
      "Epoch: 7 Train Accuracy: 0.8110 Test Accuracy: 0.7220\n",
      "Epoch: 8 Train Accuracy: 0.8113 Test Accuracy: 0.7227\n",
      "Epoch: 9 Train Accuracy: 0.8130 Test Accuracy: 0.7237\n",
      "Epoch: 10 Train Accuracy: 0.8128 Test Accuracy: 0.7230\n",
      "Epoch: 11 Train Accuracy: 0.8133 Test Accuracy: 0.7230\n",
      "Epoch: 12 Train Accuracy: 0.8148 Test Accuracy: 0.7237\n",
      "Best Test Acc: 0.7336\n"
     ]
    }
   ],
   "source": [
    "model = CostSensitiveStructuredPerceptron(learning_rate=0.1, max_update_items=1) #  update_items = 1 is best\n",
    "best_model, test_acc_df = train_model(model, xs_train=xs_train, xs_test=xs_test, \n",
    "        essay2crels_train=essay2crels_train, essay2crels_test=essay2crels_test, \n",
    "        max_epochs=30, early_stop_iters=10, train_instance_fn = train_cost_sensitive_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALMA\n",
    "- See p 175 - 176 of my structured learning book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class ALMA(object):\n",
    "    ''' ALMA Algorithm - see pages 175-176 in my structured learning book\n",
    "    '''\n",
    "\n",
    "    def __init__(self, features, alpha=1.0, B=None, C=2**0.5):\n",
    "        # Each feature gets its own weight\n",
    "        # needs to be non zero otherwise first\n",
    "        if B is None:\n",
    "            B = 1/alpha\n",
    "        self.C = C\n",
    "        self.B = B\n",
    "        self.alpha = alpha\n",
    "        self.features = features\n",
    "        self.weights = self.proj(dict([(f,1) for f in features]))\n",
    "        self.k = 1\n",
    "\n",
    "    def l2_norm(self, weights):\n",
    "        return sum((v ** 2 for v in weights.values())) ** 0.5\n",
    "\n",
    "    def to_unitl2_norm(self, fts):\n",
    "        if type(fts) == dict or type(fts) == defaultdict:\n",
    "            norm = self.l2_norm(fts)\n",
    "            return self.update_dict(fts, norm)\n",
    "        elif type(fts) == list:\n",
    "            a = []\n",
    "            for item in fts:\n",
    "                a.append(self.to_unitl2_norm(item))\n",
    "            return a\n",
    "        else:\n",
    "            raise Exception(\"Unexpected type: \" + str(type(fts)))\n",
    "\n",
    "    def proj(self, weights):\n",
    "        l2_n = self.l2_norm(weights)\n",
    "        denom = max(1, l2_n)\n",
    "        return self.update_dict(weights, denom)\n",
    "\n",
    "    def update_dict(self, dct, denom):\n",
    "        u = defaultdict(float)\n",
    "        for k, v in dct.items():\n",
    "            u[k] = v / denom\n",
    "        return u\n",
    "\n",
    "    def clone(self):\n",
    "        clone = ALMA(self.features)\n",
    "        clone.weights.update(self.weights)\n",
    "        clone.k = self.k\n",
    "        return clone\n",
    "\n",
    "    def rank(self, features_array):\n",
    "        normed_array = self.to_unitl2_norm(features_array)\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores2index = {}\n",
    "        for i, feats in enumerate(normed_array):\n",
    "            scores2index[i] = self.decision_function(feats)\n",
    "        # return a ranking of the scores, by best to worse\n",
    "        return [ix for ix, score in sorted(scores2index.items(), key=lambda tpl: -tpl[-1])]\n",
    "\n",
    "    def decision_function(self, features):\n",
    "        '''Dot-product the features and current weights and return the score.'''\n",
    "        score = 0.0\n",
    "        for feat, value in features.items():\n",
    "            if value == 0 or feat not in self.features:\n",
    "                continue\n",
    "            score += self.weights[feat] * value\n",
    "        return score\n",
    "\n",
    "    def weight_product(self, features):\n",
    "        '''Dot-product the features and current weights and return the score.'''\n",
    "        prod = defaultdict(float)\n",
    "        for feat, value in features.items():\n",
    "            if value == 0:\n",
    "                continue\n",
    "            prod[feat] = self.weights[feat] * value\n",
    "        return prod\n",
    "\n",
    "    def add_dicts(self, d1, d2):\n",
    "        for k,v in d2.items():\n",
    "            d1[k] += v\n",
    "        return d1\n",
    "\n",
    "    def train(self, best_feats, other_feats_array):\n",
    "\n",
    "        feats_array = [best_feats] + list(other_feats_array)\n",
    "        ixs = self.rank(feats_array)\n",
    "\n",
    "        # go thru up to |max_update_items| items ranked above the best, and update the weights\n",
    "        best_ix = ixs[0]\n",
    "        if best_ix == 0:\n",
    "            return\n",
    "\n",
    "        other_feats_array = [feats_array[best_ix]]\n",
    "        best_fts_prod = self.weight_product(self.to_unitl2_norm(best_feats))\n",
    "        num_other_feats = len(other_feats_array)\n",
    "        assert num_other_feats > 0\n",
    "        # total up other features vectors\n",
    "        other_feats_product = defaultdict(float)\n",
    "        for fts in other_feats_array:\n",
    "            product = self.weight_product(self.to_unitl2_norm(fts))\n",
    "            self.add_dicts(other_feats_product, product)\n",
    "\n",
    "        delta = dict()\n",
    "        # normalize by the number of other features\n",
    "        for feat in self.features:\n",
    "            delta[feat] = best_fts_prod[feat] - (other_feats_product[feat] / num_other_feats) # need to normalize the other feats value\n",
    "\n",
    "        proj_delta = self.proj(delta)\n",
    "        new_weights = defaultdict(float)\n",
    "        for ft in self.features:\n",
    "            new_weights[ft] = self.weights[ft] + (self.C * self.k**-0.5 * proj_delta[ft])\n",
    "        new_weights = self.proj(new_weights)\n",
    "        self.weights = new_weights\n",
    "        self.k += 1\n",
    "        \n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n",
    "\n",
    "\n",
    "p = ALMA(features={\"a\",\"b\",\"c\"})\n",
    "best = defaultdict(float)\n",
    "best.update({ \"a\": 1, \"b\": 2})\n",
    "\n",
    "rest = defaultdict(float)\n",
    "rest.update({\"a\": -1, \"b\": 3, \"c\": 4})\n",
    "\n",
    "p.train(best, [rest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_feats(xs):\n",
    "    fts = set()\n",
    "    for parser_input in xs:\n",
    "        fts.update(parser_input.opt_features.keys())\n",
    "        for other_feats in parser_input.other_features_array:\n",
    "            fts.update(other_feats.keys())\n",
    "    return fts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = get_all_feats(xs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7273 Test Accuracy: 0.7000\n",
      "Epoch: 1 Train Accuracy: 0.7447 Test Accuracy: 0.7044\n",
      "Epoch: 2 Train Accuracy: 0.7453 Test Accuracy: 0.7030\n",
      "Epoch: 3 Train Accuracy: 0.7481 Test Accuracy: 0.7092\n",
      "Epoch: 4 Train Accuracy: 0.7543 Test Accuracy: 0.7109\n",
      "Epoch: 5 Train Accuracy: 0.7520 Test Accuracy: 0.7099\n",
      "Epoch: 6 Train Accuracy: 0.7482 Test Accuracy: 0.7090\n",
      "Epoch: 7 Train Accuracy: 0.7526 Test Accuracy: 0.7106\n",
      "Epoch: 8 Train Accuracy: 0.7559 Test Accuracy: 0.7153\n",
      "Epoch: 9 Train Accuracy: 0.7518 Test Accuracy: 0.7123\n",
      "Epoch: 10 Train Accuracy: 0.7595 Test Accuracy: 0.7156\n",
      "Epoch: 11 Train Accuracy: 0.7614 Test Accuracy: 0.7198\n",
      "Epoch: 12 Train Accuracy: 0.7558 Test Accuracy: 0.7184\n",
      "Epoch: 13 Train Accuracy: 0.7498 Test Accuracy: 0.7091\n",
      "Epoch: 14 Train Accuracy: 0.7589 Test Accuracy: 0.7213\n",
      "Epoch: 15 Train Accuracy: 0.7597 Test Accuracy: 0.7217\n",
      "Epoch: 16 Train Accuracy: 0.7518 Test Accuracy: 0.7126\n",
      "Epoch: 17 Train Accuracy: 0.7571 Test Accuracy: 0.7168\n",
      "Epoch: 18 Train Accuracy: 0.7574 Test Accuracy: 0.7163\n",
      "Epoch: 19 Train Accuracy: 0.7598 Test Accuracy: 0.7219\n",
      "Epoch: 20 Train Accuracy: 0.7557 Test Accuracy: 0.7174\n",
      "Epoch: 21 Train Accuracy: 0.7565 Test Accuracy: 0.7157\n",
      "Epoch: 22 Train Accuracy: 0.7577 Test Accuracy: 0.7194\n",
      "Epoch: 23 Train Accuracy: 0.7622 Test Accuracy: 0.7194\n",
      "Epoch: 24 Train Accuracy: 0.7587 Test Accuracy: 0.7168\n",
      "Epoch: 25 Train Accuracy: 0.7603 Test Accuracy: 0.7161\n",
      "Epoch: 26 Train Accuracy: 0.7594 Test Accuracy: 0.7175\n",
      "Epoch: 27 Train Accuracy: 0.7610 Test Accuracy: 0.7181\n",
      "Epoch: 28 Train Accuracy: 0.7621 Test Accuracy: 0.7181\n",
      "Epoch: 29 Train Accuracy: 0.7585 Test Accuracy: 0.7143\n",
      "Best Test Acc: 0.7219\n"
     ]
    }
   ],
   "source": [
    "model = ALMA(features=all_feats)\n",
    "best_model, test_acc_df = train_model(model, xs_train=xs_train, xs_test=xs_test, \n",
    "        essay2crels_train=essay2crels_train, essay2crels_test=essay2crels_test, \n",
    "        max_epochs=30, early_stop_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>rank_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.986288</td>\n",
       "      <td>0.721893</td>\n",
       "      <td>0.669104</td>\n",
       "      <td>0.783726</td>\n",
       "      <td>0.433155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision  rank_acc\n",
       "95  0.986288  0.721893  0.669104   0.783726  0.433155"
      ]
     },
     "execution_count": 1331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class MIRA(StructuredPerceptron):\n",
    "    ''' MIRA Algorithm for multi-class classification as detailed in p 569 of\n",
    "        http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, C=0.01, max_update_items=1, pa_type=1, initial_weight=1):\n",
    "        self.C = C\n",
    "        # Each feature gets its own weight\n",
    "        # needs to be non zero otherwise first\n",
    "        assert initial_weight >= 0.0\n",
    "        self.weights = defaultdict(lambda: initial_weight)\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "        # how many items do we use to update the weights?\n",
    "        self.max_update_items = max_update_items\n",
    "        self.pa_type = pa_type\n",
    "        assert self.max_update_items >= 1, \"Max update items must be at least 1\"\n",
    "        assert self.pa_type in {0, 1, 2}  # PA I or PA II\n",
    "\n",
    "        # This isn't used, so set to 1\n",
    "        self.learning_rate = 1\n",
    "        self.train_feats = set()\n",
    "\n",
    "    def clone(self):\n",
    "        cloney = MIRA(C=self.C, max_update_items=self.max_update_items, pa_type=self.pa_type)\n",
    "        cloney.weights.update(self.weights)\n",
    "        cloney._totals.update(self._totals)\n",
    "        cloney._tstamps.update(self._tstamps)\n",
    "        cloney.i = self.i\n",
    "        cloney.train_feats.update(self.train_feats)\n",
    "        return cloney\n",
    "\n",
    "    def train(self, best_feats, other_feats_array):\n",
    "        if len(other_feats_array) == 0:\n",
    "            return\n",
    "\n",
    "        best_feats_score = self.decision_function(best_feats, existence_check=False)\n",
    "        scores = [self.decision_function(feats, existence_check=False) for feats in other_feats_array]\n",
    "        ixs = np.argsort(scores)[::-1]\n",
    "\n",
    "        # go thru up to |max_update_items| items ranked above the best, and update the weights\n",
    "        for rank, ix in enumerate(ixs):\n",
    "            if rank >= self.max_update_items:\n",
    "                break\n",
    "            feats_score = scores[ix]\n",
    "            diff = best_feats_score - feats_score\n",
    "            hinge_loss = 0 if diff >= 1 else 1 - diff\n",
    "            if hinge_loss > 0:\n",
    "                self.update(loss=hinge_loss, best_feats=best_feats, highest_ranked_feats=other_feats_array[ix])\n",
    "\n",
    "    def update(self, loss, best_feats, highest_ranked_feats):\n",
    "        self.i += 1\n",
    "        feats_union = set(best_feats.keys()).union(highest_ranked_feats.keys())\n",
    "        sum_sq_diff = 0\n",
    "        for ft in feats_union:\n",
    "            sum_sq_diff += (best_feats[ft] - highest_ranked_feats[ft]) ** 2\n",
    "        l2_norm_of_diffs = (sum_sq_diff ** 0.5)\n",
    "\n",
    "        if sum_sq_diff == 0 and self.pa_type in {0, 1}:\n",
    "            tau = self.C\n",
    "        elif self.pa_type == 0:\n",
    "            tau = loss / l2_norm_of_diffs\n",
    "        elif self.pa_type == 1:\n",
    "            tau = min(self.C, loss / l2_norm_of_diffs)\n",
    "        else:\n",
    "            tau = loss / (l2_norm_of_diffs + 1 / (2 * self.C))\n",
    "\n",
    "        for feat, weight in self.weights.items():\n",
    "            self.train_feats.add(feat)\n",
    "            val = tau * (best_feats[feat] - highest_ranked_feats[feat])\n",
    "            self.__upd_feat__(feat, val)\n",
    "        return None\n",
    "\n",
    "class CostSensitiveMIRA(MIRA):\n",
    "\n",
    "    def __init__(self, C=0.01, max_update_items=1, pa_type=1, loss_type=\"pb\", initial_weight=1):\n",
    "\n",
    "        assert loss_type in {\"pb\",\"ml\"}, \"Unrecognized loss type: {loss_type}\".format(loss_type=loss_type)\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "        super(CostSensitiveMIRA, self).__init__(\n",
    "            C=C, max_update_items=max_update_items, pa_type=pa_type, initial_weight=initial_weight)\n",
    "\n",
    "    def clone(self):\n",
    "        cloney = CostSensitiveMIRA(\n",
    "            C=self.C, max_update_items=self.max_update_items, pa_type=self.pa_type, loss_type=self.loss_type)\n",
    "        cloney.weights.update(self.weights)\n",
    "        cloney._totals.update(self._totals)\n",
    "        cloney._tstamps.update(self._tstamps)\n",
    "        cloney.i = self.i\n",
    "        cloney.train_feats.update(self.train_feats)\n",
    "        return cloney\n",
    "\n",
    "    def train(self, best_feats, other_feats_array, other_costs_array):\n",
    "\n",
    "        if len(other_feats_array) == 0:\n",
    "            return\n",
    "\n",
    "        best_feats_score = self.decision_function(best_feats, existence_check=False)\n",
    "        other_feat_scores = [self.decision_function(feats, existence_check=False) for feats in other_feats_array]\n",
    "        cs_losses = np.asarray(other_feat_scores) - best_feats_score + (np.asarray(other_costs_array) ** 0.5)\n",
    "\n",
    "        if self.loss_type == \"ml\":\n",
    "            ixs = np.argsort(cs_losses)[::-1]\n",
    "        else:\n",
    "            ixs = np.argsort(other_feat_scores)[::-1]\n",
    "\n",
    "        # go thru up to |max_update_items| items ranked above the best, and update the weights\n",
    "        for rank, ix in enumerate(ixs):\n",
    "            if rank >= self.max_update_items:\n",
    "                break\n",
    "\n",
    "            cost_sensitive_loss = cs_losses[ix]\n",
    "            if cost_sensitive_loss > 0:\n",
    "                self.update(loss=cost_sensitive_loss, best_feats=best_feats, highest_ranked_feats=other_feats_array[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7613 Test Accuracy: 0.7189\n",
      "Epoch: 1 Train Accuracy: 0.7676 Test Accuracy: 0.7218\n",
      "Epoch: 2 Train Accuracy: 0.7686 Test Accuracy: 0.7221\n",
      "Epoch: 3 Train Accuracy: 0.7704 Test Accuracy: 0.7222\n",
      "Epoch: 4 Train Accuracy: 0.7742 Test Accuracy: 0.7256\n",
      "Epoch: 5 Train Accuracy: 0.7774 Test Accuracy: 0.7293\n",
      "Epoch: 6 Train Accuracy: 0.7781 Test Accuracy: 0.7293\n",
      "Epoch: 7 Train Accuracy: 0.7778 Test Accuracy: 0.7338\n",
      "Epoch: 8 Train Accuracy: 0.7791 Test Accuracy: 0.7353\n",
      "Epoch: 9 Train Accuracy: 0.7796 Test Accuracy: 0.7342\n",
      "Epoch: 10 Train Accuracy: 0.7821 Test Accuracy: 0.7311\n",
      "Epoch: 11 Train Accuracy: 0.7845 Test Accuracy: 0.7323\n",
      "Epoch: 12 Train Accuracy: 0.7868 Test Accuracy: 0.7312\n",
      "Epoch: 13 Train Accuracy: 0.7871 Test Accuracy: 0.7324\n",
      "Epoch: 14 Train Accuracy: 0.7869 Test Accuracy: 0.7293\n",
      "Epoch: 15 Train Accuracy: 0.7893 Test Accuracy: 0.7290\n",
      "Epoch: 16 Train Accuracy: 0.7904 Test Accuracy: 0.7304\n",
      "Epoch: 17 Train Accuracy: 0.7919 Test Accuracy: 0.7304\n",
      "Epoch: 18 Train Accuracy: 0.7937 Test Accuracy: 0.7313\n",
      "Best Test Acc: 0.7353\n"
     ]
    }
   ],
   "source": [
    "model = MIRA(C=0.01, pa_type=1,  max_update_items=1, initial_weight=1)\n",
    "best_model, test_acc_df = train_model(model, xs_train=xs_train, xs_test=xs_test, \n",
    "        essay2crels_train=essay2crels_train, essay2crels_test=essay2crels_test, \n",
    "        max_epochs=30, early_stop_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1400,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.average_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('num_crels', -4.65007),\n",
       " ('num_crels<=7', 2.46899),\n",
       " ('num_crels<=6', 2.45953),\n",
       " ('num_crels<=5', 2.02019),\n",
       " ('num_crels=0', 1.97637),\n",
       " ('num_crels<=8', 1.81892),\n",
       " ('Above-0.2', -1.79778),\n",
       " ('All-Above-0.2', 1.60926),\n",
       " ('num_crels=4', 1.53875),\n",
       " ('num_crels>1', 1.53757)]"
      ]
     },
     "execution_count": 1404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_wts = sorted([(wt,v) \n",
    "                   for (wt,v) in best_model.weights.items() \n",
    "                   if wt in best_model.train_feats and v != 0.01],\n",
    "                  key = lambda tpl: -abs(tpl[1]))\n",
    "best_wts[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Result:50', -0.0144),\n",
       " ('13', -0.01768),\n",
       " ('num_crels>5', -0.02019),\n",
       " ('14', 0.04293),\n",
       " ('All-Above-0.9', -0.06717),\n",
       " ('All-Above-0.95', 0.0795),\n",
       " ('4', 0.10343),\n",
       " ('Result:7', 0.10739),\n",
       " ('7', -0.11044),\n",
       " ('50', -0.13242)]"
      ]
     },
     "execution_count": 1405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_wts[-10:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Sensitive MIRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Accuracy: 0.7602 Test Accuracy: 0.7195\n"
     ]
    }
   ],
   "source": [
    "model = CostSensitiveMIRA(C=0.01, pa_type=1, loss_type=\"pb\", max_update_items=1, initial_weight=1)\n",
    "best_model, test_acc_df = train_model(model, xs_train=xs_train, xs_test=xs_test, \n",
    "        essay2crels_train=essay2crels_train, essay2crels_test=essay2crels_test, \n",
    "        max_epochs=30, early_stop_iters=10, train_instance_fn = train_cost_sensitive_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CostSensitiveMIRA(C=0.01, pa_type=1, loss_type=\"ml\", max_update_items=1, initial_weight=1)\n",
    "best_model, test_acc_df = train_model(model, xs_train=xs_train, xs_test=xs_test, \n",
    "        essay2crels_train=essay2crels_train, essay2crels_test=essay2crels_test, \n",
    "        max_epochs=30, early_stop_iters=10, train_instance_fn = train_cost_sensitive_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Chains Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = (\n",
    "'Causer:1->Result:2',\n",
    "'Causer:2->Result:3',\n",
    "'Causer:3->Result:50',\n",
    "'Causer:2->Result:4',\n",
    "'Causer:3->Result:5',\n",
    "'Causer:5->Result:6',\n",
    "'Causer:6->Result:50',\n",
    "'Causer:7->Result:11',\n",
    "'Causer:11->Result:12',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = defaultdict(set) # maps causers to effects for building chains\n",
    "for crel in parse:\n",
    "    # with type\n",
    "    l,r = crel.split(\"->\")\n",
    "    l_short, r_short = to_short_tag(l), to_short_tag(r)\n",
    "    tree[l_short].add(r_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1,2,3',\n",
       " '1,2,3,5',\n",
       " '1,2,3,5,6',\n",
       " '1,2,3,5,6,50',\n",
       " '1,2,3,50',\n",
       " '1,2,4',\n",
       " '2,3,5',\n",
       " '2,3,5,6',\n",
       " '2,3,5,6,50',\n",
       " '2,3,50',\n",
       " '3,5,6',\n",
       " '3,5,6,50',\n",
       " '5,6,50',\n",
       " '7,11,12'}"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains = build_chains(tree)\n",
    "print(len(extend_chains(chains)))\n",
    "extend_chains(chains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- If there is a large number of predicted crels, in some cases we only have one optional parse, fix this\n",
    "  - Instead try picking the top parses greedily, by going from most to least probable\n",
    "- Add a feature to both models that determines if the crel(s) are same code to same code.\n",
    "- If both directions of a crel are predicted for the same concept code pair, enforce only the more probable one.\n",
    "- Do feature normalization using zscore\n",
    "- Add in additional features for re-ranker\n",
    "- Do feature selection for both algorithms\n",
    "- Implement early stopping to judge optimal number of epochs\n",
    "    - Change code to use this value to retrain on all the data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16**(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
