{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags, get_tag_freq\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freq = get_tag_freq(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "set_cr_tags = set(cr_tags)\n",
    "cr_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causer:50->Result:50 19\n",
      "Causer:11->Result:11 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43227"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for cr in cr_tags:\n",
    "    l,r = cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "    total += tag_freq[cr]\n",
    "    if l == r:\n",
    "        print(cr, tag_freq[cr])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        max_epochs: int) -> float:\n",
    "\n",
    "    serial_results = [\n",
    "        model_train_predict(essays_TD, essays_VD, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    cv_td_preds_by_sent = []\n",
    "    cv_vd_preds_by_sent = []\n",
    "    for (num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "        \n",
    "        cv_td_preds_by_sent.append(td_preds_by_sent)\n",
    "        cv_vd_preds_by_sent.append(vd_preds_by_sent)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    return cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent\n",
    "\n",
    "def add_cr_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "\n",
    "def get_label_data(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        for sentence in essay.sentences:\n",
    "            unique_cr_tags = set()\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "            add_cr_labels(unique_cr_tags, ys_bytag_sent)\n",
    "    return ys_bytag_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurevectorizer import FeatureVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     44627\n",
      "          1       0.98      0.99      0.98      7487\n",
      "\n",
      "avg / total       1.00      1.00      1.00     52114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class DependencyClassifier(object):\n",
    "    def __init__(self, classifier_fn=LogisticRegression, \n",
    "                 negative_label=0, sentence_span=2, \n",
    "                 min_feat_freq=10,\n",
    "                 log_fn=lambda s: print(s), ):\n",
    "        self.log = log_fn\n",
    "        self.epoch = 0\n",
    "        self.negative_label = negative_label\n",
    "        self.sentence_span = sentence_span\n",
    "        self.min_feat_freq=min_feat_freq\n",
    "        self.vectorizer = FeatureVectorizer(min_feature_freq=min_feat_freq)\n",
    "        self.clf = classifier_fn()\n",
    "    \n",
    "    def __fill_in_gaps__(self, tag_seq):\n",
    "        new_tag_seq = []\n",
    "        for i, tag in enumerate(tag_seq):\n",
    "            if tag == EMPTY \\\n",
    "                and i > 0 \\\n",
    "                and tag_seq[i-1] != EMPTY \\\n",
    "                and i < len(tag_seq)-1 \\\n",
    "                and tag_seq[i-1] == tag_seq[i+1]:\n",
    "                    tag = tag_seq[i-1]\n",
    "\n",
    "            new_tag_seq.append(tag)\n",
    "        return new_tag_seq\n",
    "\n",
    "    def __compute_tag_2_spans__(self, essay):\n",
    "        sent_tag2spans = []\n",
    "        wd_ix = -1\n",
    "        essay_words = []\n",
    "        for sent_ix in range(len(essay.sentences)):\n",
    "            words, tag_seq = zip(*essay.sentences[sent_ix])\n",
    "\n",
    "            tag2spans = [] # maps to list of start and end spans for each tag\n",
    "            sent_tag2spans.append(tag2spans)\n",
    "\n",
    "            last_tag = EMPTY\n",
    "            tag_start = None\n",
    "            ptags_sent = self.__fill_in_gaps__(essay.pred_tagged_sentences[sent_ix])\n",
    "            current_crel_tags = set()\n",
    "            for i, ptag in enumerate(ptags_sent):\n",
    "                wd_ix += 1\n",
    "                essay_words.append(words[i])\n",
    "                # Tag changed\n",
    "                if ptag != last_tag:\n",
    "                    if last_tag != EMPTY:\n",
    "                        tag2spans.append((last_tag, tag_start, wd_ix-1, current_crel_tags))                    \n",
    "                    tag_start = wd_ix\n",
    "                    current_crel_tags = set()\n",
    "                current_crel_tags.update(to_is_valid_crel(tag_seq[i]))\n",
    "                last_tag = ptag\n",
    "            if last_tag != EMPTY:\n",
    "                tag2spans.append((last_tag, tag_start, wd_ix, current_crel_tags))\n",
    "        return sent_tag2spans, essay_words\n",
    "    \n",
    "    def __combine_feats__(self, ftsa, ftsb):\n",
    "        fts = {}\n",
    "        for a, aval in ftsa.items():\n",
    "            for b, bval in ftsb.items():\n",
    "                fts[a + \"|\" + b] = aval * bval\n",
    "        return fts\n",
    "    \n",
    "    def create_features(self, causer_tag, result_tag, causer_words, between_words, result_words, causer_first):\n",
    "        feats = {}\n",
    "        crel = \"Causer:{a}->Result:{b}\".format(a=causer_tag, b=result_tag)\n",
    "        feats[crel] = 1\n",
    "        feats[\"Causer:{tag}\".format(tag=causer_tag)] = 1\n",
    "        feats[\"Result:{tag}\".format(tag=result_tag)] = 1\n",
    "        cs_fts, res_fts = {},{}\n",
    "        for wd in causer_words:\n",
    "            cs_fts[\"Causer:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(cs_fts)\n",
    "        for wd in result_words:\n",
    "            res_fts[\"Result:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(res_fts)\n",
    "        feats.update(self.__combine_feats__(cs_fts, res_fts))\n",
    "        btwn_fts = {}\n",
    "        for wd in between_words:\n",
    "            btwn_fts[\"Between:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(btwn_fts)\n",
    "        feats.update(self.__combine_feats__(cs_fts, btwn_fts))\n",
    "        feats.update(self.__combine_feats__(res_fts, btwn_fts))\n",
    "        if causer_first:\n",
    "            feats[\"Left2Right\"] = 1\n",
    "        else:\n",
    "            feats[\"Right2Left\"] = 1\n",
    "        return feats, crel\n",
    "    \n",
    "    def __generate_training_data__(self, essays):\n",
    "        xs, ys, essay_sent_crel = [],[],[]\n",
    "        for essay_ix, essay in enumerate(essays):\n",
    "            sent_tag2spans, essay_words = self.__compute_tag_2_spans__(essay)\n",
    "            for sent_ix in range(len(sent_tag2spans)):\n",
    "                # tag 2 spans for sentence\n",
    "                next_tag2spans = []\n",
    "                # grab next few sentences' predicted tags\n",
    "                for offset in range(0, self.sentence_span+1):\n",
    "                    if (sent_ix+offset) < len(sent_tag2spans):\n",
    "                        next_tag2spans.extend(sent_tag2spans[sent_ix+offset])\n",
    "                \n",
    "                for ltag_ix, (ltag, lstart_ix, lend_ix, lcrels) in enumerate(sent_tag2spans[sent_ix]):\n",
    "                    for rtag, rstart_ix, rend_ix, rcrels in next_tag2spans[ltag_ix+1:]:\n",
    "                        ltag_words    = essay_words[lstart_ix:lend_ix+1]\n",
    "                        between_words = essay_words[lend_ix+1:rstart_ix]\n",
    "                        rtag_words    = essay_words[rstart_ix:rend_ix+1]\n",
    "                        \n",
    "                        lbls = set(lcrels).union(rcrels)\n",
    "                        x,ft_crel = self.create_features(\n",
    "                                causer_tag=ltag, result_tag=rtag, \n",
    "                                causer_words=ltag_words, between_words=between_words, result_words=rtag_words, \n",
    "                                causer_first=True)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if ft_crel in lbls else self.negative_label)\n",
    "                        essay_sent_crel.append((e.name, sent_ix, ft_crel))\n",
    "                        \n",
    "                        x,ft_crel = self.create_features(\n",
    "                                causer_tag=rtag, result_tag=ltag, \n",
    "                                causer_words=rtag_words, between_words=between_words, result_words=ltag_words, \n",
    "                                causer_first=False)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if ft_crel in lbls else self.negative_label)\n",
    "                        essay_sent_crel.append((e.name, sent_ix, ft_crel))\n",
    "        return xs, ys, essay_sent_crel\n",
    "    \n",
    "    def train(self, train_essays, sent_span=2):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=train_essays)\n",
    "        xs_array = self.vectorizer.fit_transform(xs)\n",
    "        self.clf.fit(X=xs_array, y=ys)\n",
    "        preds = self.clf.predict(xs_array)\n",
    "\n",
    "    def evaluate(self, tagged_essays):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=tagged_essays)\n",
    "        xs_array = self.vectorizer.transform(xs)\n",
    "        preds = self.clf.predict(xs_array)\n",
    "        print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        #TODO - This doesn't work\n",
    "        namesent2pred = defaultdict(set)\n",
    "        for (name, sent_ix, crel), pred in zip(essay_sent_crel, preds):\n",
    "            if pred == 1:\n",
    "                namesent2pred[(name, sent_ix)].add(crel)\n",
    "\n",
    "        pred_ys_bytag_sent = defaultdict(list)\n",
    "        for essay in pred_tagged_essays_train:\n",
    "            for sent_ix, sentence in enumerate(essay.sentences):\n",
    "                unique_cr_tags = namesent2pred[(essay.name, sent_ix)]\n",
    "                add_cr_labels(unique_cr_tags, pred_ys_bytag_sent)\n",
    "        return pred_ys_bytag_sent\n",
    "    \n",
    "parser = DependencyClassifier()\n",
    "parser.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent = parser.evaluate(pred_tagged_essays_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94      9280\n",
      "          1       0.61      0.60      0.61      1484\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_ys_bytag_sent_test = parser.evaluate(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     44627\n",
      "          1       0.99      0.95      0.97      7487\n",
      "\n",
      "avg / total       0.99      0.99      0.99     52114\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.94      9280\n",
      "          1       0.72      0.44      0.54      1484\n",
      "\n",
      "avg / total       0.89      0.90      0.89     10764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "parser = DependencyClassifier(classifier_fn=RandomForestClassifier)\n",
    "parser.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent = parser.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test = parser.evaluate(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_bytag_sent = get_label_data(pred_tagged_essays_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.995669</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.995669  0.000612  0.000308   0.041667"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_sent, pred_ys_bytag_sent)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train_predict(essays_TD, essays_VD, max_epochs):\n",
    "    \n",
    "    parse_model = ReRankingParser()\n",
    "    parse_model.train(essays_TD, max_epochs=max_epochs)\n",
    "\n",
    "    num_feats = template_feature_extractor.num_features()\n",
    "\n",
    "    sent_td_ys_bycode = get_label_data(essays_TD)\n",
    "    sent_vd_ys_bycode = get_label_data(essays_VD)\n",
    "\n",
    "    sent_td_pred_ys_bycode = parse_model.predict(essays_TD)\n",
    "    sent_vd_pred_ys_bycode = parse_model.predict(essays_VD)\n",
    "\n",
    "    td_preds_by_sent = predict_by_sent(essays_TD, parse_model)\n",
    "    vd_preds_by_sent = predict_by_sent(essays_VD, parse_model)\n",
    "    \n",
    "    return num_feats, sent_td_ys_bycode, sent_vd_ys_bycode, sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_micro_metrics(df):\n",
    "    return df[df.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"recall\", \"precision\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that these are different for Skin Cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folds     = [(pred_tagged_essays_train, pred_tagged_essays_test)]  # type: List[Tuple[Any,Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 s, sys: 242 ms, total: 29.5 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_test = evaluate_model(folds=test_folds, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Metrics (All Codes Inc. Ana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.997865</td>\n",
       "      <td>0.741951</td>\n",
       "      <td>0.713494</td>\n",
       "      <td>0.772773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.997865  0.741951  0.713494   0.772773"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, \\\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent = result_test\n",
    "    \n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.99776</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.727848</td>\n",
       "      <td>0.677467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95   0.99776  0.701754  0.727848   0.677467"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, \\\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent = result_test\n",
    "    \n",
    "mean_metrics = ResultsProcessor.compute_mean_metrics(cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag)\n",
    "get_micro_metrics(metrics_to_df(mean_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1918, 1918)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_crels_by_sent = cv_vd_preds_by_sent[0]\n",
    "crels_by_sent = get_crel_tags_by_sent(pred_tagged_essays_test)\n",
    "len(pred_crels_by_sent), len(crels_by_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
