{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Any\n",
    "\n",
    "from CrossValidation import cross_validation\n",
    "from Settings import Settings\n",
    "from cost_functions import *\n",
    "from crel_helper import get_cr_tags, get_tag_freq\n",
    "from function_helpers import get_function_names, get_functions_by_name\n",
    "from results_procesor import ResultsProcessor, __MICRO_F1__\n",
    "from searn_parser import SearnModelTemplateFeatures\n",
    "from template_feature_extractor import *\n",
    "from window_based_tagger_config import get_config\n",
    "from wordtagginghelper import merge_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Dir: /Users/simon.hughes/Google Drive/Phd/Results/\n",
      "Data Dir:    /Users/simon.hughes/Google Drive/Phd/Data/\n",
      "Root Dir:    /Users/simon.hughes/GitHub/NlpResearch/\n",
      "Public Data: /Users/simon.hughes/GitHub/NlpResearch/Data/PublicDatasets/\n"
     ]
    }
   ],
   "source": [
    "# Data Set Partition\n",
    "CV_FOLDS = 5\n",
    "MIN_FEAT_FREQ = 5\n",
    "\n",
    "# Global settings\n",
    "settings = Settings()\n",
    "root_folder = settings.data_directory + \"CoralBleaching/Thesis_Dataset/\"\n",
    "training_folder = root_folder + \"Training\" + \"/\"\n",
    "test_folder = root_folder + \"Test\" + \"/\"\n",
    "\n",
    "coref_root = root_folder + \"CoReference/\"\n",
    "coref_output_folder = coref_root + \"CRel/\"\n",
    "\n",
    "config = get_config(training_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname = coref_output_folder + \"training_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(train_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_train = dill.load(f)\n",
    "\n",
    "test_fname = coref_output_folder + \"test_crel_anatagged_essays_most_recent_code.dill\"\n",
    "with open(test_fname, \"rb\") as f:\n",
    "    pred_tagged_essays_test = dill.load(f)\n",
    "\n",
    "len(pred_tagged_essays_train),len(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY = \"Empty\"\n",
    "from BrattEssay import ANAPHORA\n",
    "\n",
    "def to_is_valid_crel(tags):\n",
    "    filtered = set()\n",
    "    for t in tags:\n",
    "        t_lower = t.lower()\n",
    "        if \"rhetorical\" in t_lower or \"change\" in t_lower or \"other\" in t_lower:\n",
    "            continue\n",
    "        if \"->\" in t and ANAPHORA not in t:\n",
    "            filtered.add(t)\n",
    "    return filtered\n",
    "\n",
    "def get_crel_tags_by_sent(essays_a):\n",
    "    crels_by_sent = []\n",
    "    for ea in essays_a:\n",
    "        for asent in ea.sentences:\n",
    "            all_atags = set()\n",
    "            for awd, atags in asent:\n",
    "                all_atags.update(to_is_valid_crel(atags))\n",
    "            crels_by_sent.append(all_atags)\n",
    "    return crels_by_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freq = get_tag_freq(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Causer:5->Result:50',\n",
       " 'Causer:7->Result:50',\n",
       " 'Causer:3->Result:4',\n",
       " 'Causer:11->Result:50',\n",
       " 'Causer:1->Result:50',\n",
       " 'Causer:13->Result:50',\n",
       " 'Causer:6->Result:50',\n",
       " 'Causer:3->Result:5',\n",
       " 'Causer:4->Result:14',\n",
       " 'Causer:3->Result:1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_tags = get_cr_tags(train_tagged_essays=pred_tagged_essays_train, tag_essays_test=pred_tagged_essays_test)\n",
    "set_cr_tags = set(cr_tags)\n",
    "cr_tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causer:50->Result:50 19\n",
      "Causer:11->Result:11 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43227"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for cr in cr_tags:\n",
    "    l,r = cr.replace(\"Causer:\",\"\").replace(\"Result:\",\"\").split(\"->\")\n",
    "    total += tag_freq[cr]\n",
    "    if l == r:\n",
    "        print(cr, tag_freq[cr])\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "        folds: List[Tuple[Any, Any]],\n",
    "        max_epochs: int) -> float:\n",
    "\n",
    "    serial_results = [\n",
    "        model_train_predict(essays_TD, essays_VD, max_epochs)\n",
    "        for essays_TD, essays_VD in folds\n",
    "    ]\n",
    "\n",
    "    cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "    cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    # record the number of features in each fold\n",
    "    number_of_feats = []\n",
    "\n",
    "    # Parallel is almost 5X faster!!!\n",
    "    cv_td_preds_by_sent = []\n",
    "    cv_vd_preds_by_sent = []\n",
    "    for (num_feats,\n",
    "         sent_td_ys_bycode, sent_vd_ys_bycode,\n",
    "         sent_td_pred_ys_bycode, sent_vd_pred_ys_bycode, td_preds_by_sent, vd_preds_by_sent) in serial_results:\n",
    "        number_of_feats.append(num_feats)\n",
    "\n",
    "        merge_dictionaries(sent_td_ys_bycode, cv_sent_td_ys_by_tag)\n",
    "        merge_dictionaries(sent_vd_ys_bycode, cv_sent_vd_ys_by_tag)\n",
    "        merge_dictionaries(sent_td_pred_ys_bycode, cv_sent_td_predictions_by_tag)\n",
    "        merge_dictionaries(sent_vd_pred_ys_bycode, cv_sent_vd_predictions_by_tag)\n",
    "        \n",
    "        cv_td_preds_by_sent.append(td_preds_by_sent)\n",
    "        cv_vd_preds_by_sent.append(vd_preds_by_sent)\n",
    "\n",
    "    # print(processor.results_to_string(sent_td_objectid, CB_SENT_TD, sent_vd_objectid, CB_SENT_VD, \"SENTENCE\"))\n",
    "    return cv_sent_td_ys_by_tag, cv_sent_td_predictions_by_tag, cv_td_preds_by_sent, cv_sent_vd_ys_by_tag, cv_sent_vd_predictions_by_tag, cv_vd_preds_by_sent\n",
    "\n",
    "def add_cr_labels(observed_tags, ys_bytag_sent):\n",
    "    global set_cr_tags\n",
    "    for tag in set_cr_tags:\n",
    "        if tag in observed_tags:\n",
    "            ys_bytag_sent[tag].append(1)\n",
    "        else:\n",
    "            ys_bytag_sent[tag].append(0)\n",
    "\n",
    "def get_label_data(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_sent = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        for sentence in essay.sentences:\n",
    "            unique_cr_tags = set()\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "            add_cr_labels(unique_cr_tags, ys_bytag_sent)\n",
    "    return ys_bytag_sent\n",
    "\n",
    "def get_label_data_essay_level(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    ys_bytag_essay = defaultdict(list)\n",
    "\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        add_cr_labels(unique_cr_tags, ys_bytag_essay)\n",
    "    return ys_bytag_essay\n",
    "\n",
    "def essay_to_crels(tagged_essays):\n",
    "    global set_cr_tags\n",
    "    # outputs\n",
    "    name2crels = defaultdict(set)\n",
    "    for essay in tagged_essays:\n",
    "        unique_cr_tags = set()\n",
    "        for sentence in essay.sentences:\n",
    "            for word, tags in sentence:\n",
    "                unique_cr_tags.update(set_cr_tags.intersection(tags))\n",
    "        name2crels[essay.name] = unique_cr_tags\n",
    "    return name2crels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from featurevectorizer import FeatureVectorizer\n",
    "\n",
    "def metrics_to_df(metrics):\n",
    "    import Rpfa\n",
    "\n",
    "    rows = []\n",
    "    for k,val in metrics.items():\n",
    "        if type(val) == Rpfa.rpfa:\n",
    "            d = dict(val.__dict__) # convert obj to dict\n",
    "        elif type(val) == dict:\n",
    "            d = dict(val)\n",
    "        else:\n",
    "            d = dict()\n",
    "        d[\"code\"] = k\n",
    "        rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def get_micro_metrics(df):\n",
    "    return df[df.code == \"MICRO_F1\"][[\"accuracy\", \"f1_score\", \"recall\", \"precision\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class DependencyClassifier(object):\n",
    "    def __init__(self, classifier_fn=LogisticRegression, \n",
    "                 negative_label=0, sentence_span=2, \n",
    "                 min_feat_freq=10,\n",
    "                 log_fn=lambda s: print(s), ):\n",
    "        self.log = log_fn\n",
    "        self.epoch = 0\n",
    "        self.negative_label = negative_label\n",
    "        self.sentence_span = sentence_span\n",
    "        self.min_feat_freq=min_feat_freq\n",
    "        self.vectorizer = FeatureVectorizer(min_feature_freq=min_feat_freq)\n",
    "        self.clf = classifier_fn()\n",
    "    \n",
    "    def __fill_in_gaps__(self, tag_seq):\n",
    "        new_tag_seq = []\n",
    "        for i, tag in enumerate(tag_seq):\n",
    "            if tag == EMPTY \\\n",
    "                and i > 0 \\\n",
    "                and tag_seq[i-1] != EMPTY \\\n",
    "                and i < len(tag_seq)-1 \\\n",
    "                and tag_seq[i-1] == tag_seq[i+1]:\n",
    "                    tag = tag_seq[i-1]\n",
    "\n",
    "            new_tag_seq.append(tag)\n",
    "        return new_tag_seq\n",
    "\n",
    "    def __compute_tag_2_spans__(self, essay):\n",
    "        sent_tag2spans = []\n",
    "        wd_ix = -1\n",
    "        essay_words = []\n",
    "        essay_ptags = []\n",
    "        for sent_ix in range(len(essay.sentences)):\n",
    "            words, tag_seq = zip(*essay.sentences[sent_ix])\n",
    "\n",
    "            tag2spans = [] # maps to list of start and end spans for each tag\n",
    "            sent_tag2spans.append(tag2spans)\n",
    "\n",
    "            last_tag = EMPTY\n",
    "            tag_start = None\n",
    "            ptags_sent = self.__fill_in_gaps__(essay.pred_tagged_sentences[sent_ix])\n",
    "            current_crel_tags = set()\n",
    "            for i, ptag in enumerate(ptags_sent):\n",
    "                wd_ix += 1\n",
    "                essay_words.append(words[i])\n",
    "                essay_ptags.append(ptag)\n",
    "                # Tag changed\n",
    "                if ptag != last_tag:\n",
    "                    if last_tag != EMPTY:\n",
    "                        tag2spans.append((last_tag, tag_start, wd_ix-1, sent_ix, current_crel_tags))                    \n",
    "                    tag_start = wd_ix\n",
    "                    current_crel_tags = set()\n",
    "                current_crel_tags.update(to_is_valid_crel(tag_seq[i]))\n",
    "                last_tag = ptag\n",
    "            if last_tag != EMPTY:\n",
    "                tag2spans.append((last_tag, tag_start, wd_ix, len(essay.sentences)-1, current_crel_tags))\n",
    "        assert len(essay_words) == len(essay_ptags)\n",
    "        return sent_tag2spans, essay_words, essay_ptags\n",
    "    \n",
    "    def __combine_feats__(self, ftsa, ftsb):\n",
    "        fts = {}\n",
    "        for a, aval in ftsa.items():\n",
    "            for b, bval in ftsb.items():\n",
    "                fts[a + \"|\" + b] = aval * bval\n",
    "        return fts\n",
    "    \n",
    "    def create_features(self, causer_tag, result_tag, causer_words, between_words, result_words, \n",
    "                        causer_first, sentences_between, codes_between):\n",
    "        feats = {}\n",
    "        crel = \"Causer:{a}->Result:{b}\".format(a=causer_tag, b=result_tag)\n",
    "        feats[crel] = 1\n",
    "        feats[\"Causer:{tag}\".format(tag=causer_tag)] = 1\n",
    "        feats[\"Result:{tag}\".format(tag=result_tag)] = 1\n",
    "        cs_fts, res_fts = {},{}\n",
    "        for wd in causer_words:\n",
    "            cs_fts[\"Causer:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(cs_fts)\n",
    "        for wd in result_words:\n",
    "            res_fts[\"Result:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(res_fts)\n",
    "        feats.update(self.__combine_feats__(cs_fts, res_fts))\n",
    "        btwn_fts = {}\n",
    "        for wd in between_words:\n",
    "            btwn_fts[\"Between:{wd}\".format(wd=wd)] = 1\n",
    "        feats.update(btwn_fts)\n",
    "#         feats.update(self.__combine_feats__(cs_fts, btwn_fts))\n",
    "#         feats.update(self.__combine_feats__(res_fts, btwn_fts))\n",
    "        if causer_first:\n",
    "            feats[\"Left2Right\"] = 1\n",
    "        else:\n",
    "            feats[\"Right2Left\"] = 1\n",
    "\n",
    "        if sentences_between == 0:\n",
    "            feats[\"SameSentence\"] = 1\n",
    "        feats[\"SentBetween\"] = sentences_between\n",
    "        if sentences_between <= 1:\n",
    "            feats[\"SentBetween<=1\"] = 1\n",
    "        if sentences_between <= 2:\n",
    "            feats[\"SentBetween<=2\"] = 1\n",
    "        else:\n",
    "            feats[\"SentBetween>2\"] = 1\n",
    "            \n",
    "        feats[\"CodesBetween=\" + str(codes_between)] = 1\n",
    "        if codes_between <= 1:\n",
    "            feats[\"CodesBetween<=1\"] = 1\n",
    "        if codes_between <= 2:\n",
    "            feats[\"CodesBetween<=2\"] = 1\n",
    "        else:\n",
    "            feats[\"CodesBetween>2\"] = 1\n",
    "        return feats, crel\n",
    "    \n",
    "    def __generate_training_data__(self, essays):\n",
    "        xs, ys, essay_sent_crel = [],[],[]\n",
    "        for essay_ix, essay in enumerate(essays):\n",
    "            sent_tag2spans, essay_words, essay_ptags = self.__compute_tag_2_spans__(essay)\n",
    "            for sent_ix in range(len(sent_tag2spans)):\n",
    "                # tag 2 spans for sentence\n",
    "                next_tag2spans = []\n",
    "                # grab next few sentences' predicted tags\n",
    "                for offset in range(0, self.sentence_span+1):\n",
    "                    if (sent_ix+offset) < len(sent_tag2spans):\n",
    "                        next_tag2spans.extend(sent_tag2spans[sent_ix+offset])\n",
    "                \n",
    "                for ltag_ix, (ltag, lstart_ix, lend_ix, lsent_ix, lcrels) in enumerate(sent_tag2spans[sent_ix]):\n",
    "                    for codes_between, (rtag, rstart_ix, rend_ix, rsent_ix, rcrels) in enumerate(next_tag2spans[ltag_ix+1:]):\n",
    "                        sent_between  = rsent_ix - lsent_ix\n",
    "\n",
    "                        ltag_words    = essay_words[lstart_ix:lend_ix+1]\n",
    "                        between_words = essay_words[lend_ix+1:rstart_ix]\n",
    "                        rtag_words    = essay_words[rstart_ix:rend_ix+1]\n",
    "                        \n",
    "                        lbls = set(lcrels).union(rcrels)\n",
    "                        x,ft_crel = self.create_features(\n",
    "                                causer_tag=ltag, result_tag=rtag, \n",
    "                                causer_words=ltag_words, between_words=between_words, result_words=rtag_words, \n",
    "                                causer_first=True, sentences_between=sent_between, codes_between=codes_between)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if ft_crel in lbls else self.negative_label)\n",
    "                        essay_sent_crel.append((essay.name, lsent_ix, rsent_ix, ft_crel))\n",
    "                        \n",
    "                        x,ft_crel = self.create_features(\n",
    "                                causer_tag=rtag, result_tag=ltag, \n",
    "                                causer_words=rtag_words, between_words=between_words, result_words=ltag_words, \n",
    "                                causer_first=False, sentences_between=sent_between, codes_between=codes_between)\n",
    "                        xs.append(x)\n",
    "                        ys.append(1 if ft_crel in lbls else self.negative_label)\n",
    "                        essay_sent_crel.append((essay.name, lsent_ix, rsent_ix, ft_crel))\n",
    "        return xs, ys, essay_sent_crel\n",
    "    \n",
    "    def train(self, train_essays, sent_span=2):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=train_essays)\n",
    "        xs_array = self.vectorizer.fit_transform(xs)\n",
    "        self.clf.fit(X=xs_array, y=ys)\n",
    "        preds = self.clf.predict(xs_array)\n",
    "        \n",
    "    def predict_probability(self, tagged_essays, min_prob=0.1):\n",
    "        # Get predicted probabilities\n",
    "        xs, _, essay_sent_crel = self.__generate_training_data__(essays=tagged_essays)\n",
    "        xs_array = self.vectorizer.transform(xs)\n",
    "        probs = self.clf.predict_proba(xs_array)[:,1]\n",
    "        \n",
    "        name2pred = defaultdict(lambda : defaultdict(list))\n",
    "        for (name, lsent_ix, rsent_ix, crel), prob in zip(essay_sent_crel, probs):\n",
    "            if prob >= min_prob:\n",
    "                name2pred[name][crel].append(prob)\n",
    "        return name2pred\n",
    "\n",
    "    def evaluate(self, tagged_essays, print_classification_report=True):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=tagged_essays)\n",
    "        xs_array = self.vectorizer.transform(xs)\n",
    "        preds = self.clf.predict(xs_array)\n",
    "        if print_classification_report:\n",
    "            print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        #TODO - This doesn't work\n",
    "        namesent2pred = defaultdict(set)\n",
    "        for (name, lsent_ix, rsent_ix, crel), pred in zip(essay_sent_crel, preds):\n",
    "            if pred == 1:\n",
    "                namesent2pred[(name, lsent_ix)].add(crel)\n",
    "                namesent2pred[(name, rsent_ix)].add(crel)\n",
    "\n",
    "        pred_ys_bytag_sent = defaultdict(list)\n",
    "        for essay in tagged_essays:\n",
    "            for sent_ix, sentence in enumerate(essay.sentences):\n",
    "                unique_cr_tags = namesent2pred[(essay.name, sent_ix)]\n",
    "                add_cr_labels(unique_cr_tags, pred_ys_bytag_sent)\n",
    "        return pred_ys_bytag_sent\n",
    "    \n",
    "    def evaluate_essay_level(self, tagged_essays, print_classification_report=True):\n",
    "        # Note that there are a small number of crels that span 2 sentences\n",
    "        xs, ys, essay_sent_crel = self.__generate_training_data__(essays=tagged_essays)\n",
    "        xs_array = self.vectorizer.transform(xs)\n",
    "        preds = self.clf.predict(xs_array)\n",
    "        if print_classification_report:\n",
    "            print(classification_report(y_true=ys, y_pred=preds))\n",
    "\n",
    "        #TODO - This doesn't work\n",
    "        namesent2pred = defaultdict(set)\n",
    "        for (name, lsent_ix, rsent_ix, crel), pred in zip(essay_sent_crel, preds):\n",
    "            if pred == 1:\n",
    "                namesent2pred[name].add(crel)\n",
    "\n",
    "        pred_ys_bytag_essay = defaultdict(list)\n",
    "        for essay in tagged_essays:\n",
    "            unique_cr_tags = namesent2pred[essay.name]\n",
    "            add_cr_labels(unique_cr_tags, pred_ys_bytag_essay)\n",
    "        return pred_ys_bytag_essay \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_accuracy(parser, essays):\n",
    "    ys_bytag_sent = get_label_data(tagged_essays=essays)\n",
    "    pred_ys_bytag_sent = parser.evaluate(tagged_essays=essays, print_classification_report=False)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_sent, pred_ys_bytag_sent)\n",
    "    return get_micro_metrics(metrics_to_df(mean_metrics))\n",
    "\n",
    "def compute_essay_accuracy(parser, essays):\n",
    "    ys_bytag_essay = get_label_data_essay_level(tagged_essays=essays)\n",
    "    pred_ys_bytag_essay = parser.evaluate_essay_level(tagged_essays=essays, print_classification_report=False)\n",
    "    mean_metrics = ResultsProcessor.compute_mean_metrics(ys_bytag_essay, pred_ys_bytag_essay)\n",
    "    return get_micro_metrics(metrics_to_df(mean_metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Level Accuracy is Equivalent to Parser Model (Or Very Close) When We Don't Look Across Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.97      9036\n",
      "          1       0.88      0.91      0.89      2916\n",
      "\n",
      "avg / total       0.95      0.95      0.95     11952\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.91      0.92      1868\n",
      "          1       0.73      0.78      0.75       586\n",
      "\n",
      "avg / total       0.88      0.88      0.88      2454\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.997886</td>\n",
       "      <td>0.691213</td>\n",
       "      <td>0.653481</td>\n",
       "      <td>0.73357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.997886  0.691213  0.653481    0.73357"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DependencyClassifier(sentence_span=0)\n",
    "parser.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent = parser.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test = parser.evaluate(pred_tagged_essays_test)\n",
    "\n",
    "compute_sentence_accuracy(parser, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.985996</td>\n",
       "      <td>0.725191</td>\n",
       "      <td>0.694698</td>\n",
       "      <td>0.758483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.985996  0.725191  0.694698   0.758483"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_essay_accuracy(parser, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(0,)\n",
      "(1,)\n",
      "(2,)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(1, 2)\n",
      "(0, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# look at the number of predicted items\n",
    "lens = []\n",
    "for ename, dct in probs.items():\n",
    "    lens.append(len(dct))\n",
    "    if len(dct) > 8:\n",
    "#         pprint(dct)\n",
    "        pass\n",
    "        \n",
    "np.mean(lens), np.median(lens), np.max(lens), np.percentile(lens, 75)\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def get_all_combos(items):\n",
    "    cbos = []\n",
    "    for i in range(1, len(items)+1):\n",
    "        cbos.extend(combinations(items,i))\n",
    "    return cbos\n",
    "\n",
    "cbos = get_all_combos(range(3))\n",
    "print(len(cbos)) # 2**len(items)-1\n",
    "if len(cbos) < 1000:\n",
    "    for cbo in sorted(cbos, key = lambda l: (len(l), l)):\n",
    "        print(cbo)\n",
    "        \n",
    "probs = parser.predict_probability(pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.97     44627\n",
      "          1       0.85      0.80      0.82      7487\n",
      "\n",
      "avg / total       0.95      0.95      0.95     52114\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94      9280\n",
      "          1       0.62      0.60      0.61      1484\n",
      "\n",
      "avg / total       0.89      0.89      0.89     10764\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.995256</td>\n",
       "      <td>0.498789</td>\n",
       "      <td>0.651899</td>\n",
       "      <td>0.403922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.995256  0.498789  0.651899   0.403922"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser2 = DependencyClassifier(sentence_span=2)\n",
    "parser2.train(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent2 = parser2.evaluate(pred_tagged_essays_train)\n",
    "pred_ys_bytag_sent_test2 = parser2.evaluate(pred_tagged_essays_test)\n",
    "\n",
    "compute_sentence_accuracy(parser2, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.981183</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>0.718464</td>\n",
       "      <td>0.627796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score    recall  precision\n",
       "95  0.981183  0.670077  0.718464   0.627796"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_essay_accuracy(parser2, pred_tagged_essays_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Structured Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Structured perceptron classifier. Implementation geared for simplicity rather than\n",
    "efficiency.\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class StructuredPerceptron(object):\n",
    "\n",
    "    '''A structured perceptron, as implemented by Matthew Honnibal.\n",
    "    See more implementation details here:\n",
    "        http://honnibal.wordpress.com/2013/09/11/a-good-part-of-speechpos-tagger-in-about-200-lines-of-python/\n",
    "    '''\n",
    "\n",
    "    def __init__(self, learning_rate):\n",
    "        # Each feature gets its own weight\n",
    "        self.weights = defaultdict(float)\n",
    "        self.learning_rate = learning_rate\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def rank(self, features_array):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores2index = {}\n",
    "        for i, feats in enumerate(features_array):\n",
    "            scores2index[i] = self.decision_function(feats)\n",
    "        # return a ranking of the scores, by best to worse\n",
    "\n",
    "        return [ ix for ix, score in sorted(scores2index.items(), key = lambda tpl: -tpl[-1]) ]\n",
    "\n",
    "    def train(self, best_feats, other_feats_array):\n",
    "        best_ix = self.rank([best_feats] + list(other_feats_array))\n",
    "        if best_ix != 0:\n",
    "            predicted_feats = other_feats_array[best_ix-1]\n",
    "            self.update(best_feats=best_feats, highest_ranked_feats=predicted_feats)\n",
    "\n",
    "    def decision_function(self, features):\n",
    "        '''Dot-product the features and current weights and return the score.'''\n",
    "        score = 0.0\n",
    "        for feat, value in features.items():\n",
    "            if feat not in self.weights or value == 0:\n",
    "                continue\n",
    "            score += self.weights[feat] * value\n",
    "        return score\n",
    "\n",
    "    def update(self, best_feats, highest_ranked_feats):\n",
    "        '''Update the feature weights.'''\n",
    "        #TODO - weight the weight update by the difference in errors\n",
    "        def upd_feat(feat, val):\n",
    "            w = self.weights[feat]\n",
    "            # update the totals by the number of timestamps the current value has survived * val\n",
    "            self._totals[feat] += (self.i - self._tstamps[feat]) * w\n",
    "            # store latest update timestamp\n",
    "            self._tstamps[feat] = self.i\n",
    "            # finally, update the current weight\n",
    "            self.weights[feat] = w + (self.learning_rate * val)\n",
    "\n",
    "        self.i += 1\n",
    "        for feat, weight in self.weights.items():\n",
    "            val = best_feats[feat] - highest_ranked_feats[feat]\n",
    "            upd_feat(feat, val)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        new_feat_weights = defaultdict(float)\n",
    "        for feat, weight in self.weights.items():\n",
    "            total = self._totals[feat]\n",
    "            total += (self.i - self._tstamps[feat]) * weight\n",
    "            averaged = round(total / float(self.i), 5)\n",
    "            if averaged != 0.0:\n",
    "                new_feat_weights[feat] = averaged\n",
    "        self.weights = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump(dict(self.weights), open(path, 'w'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:phd_py36]",
   "language": "python",
   "name": "conda-env-phd_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
